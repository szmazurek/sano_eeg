{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the network's predicitons in terms of connectivity changes\n",
    "# and feature importance. To run it, one needs to have a final model\n",
    "# checkpoints saved for every fold and the corresponding testing set.\n",
    "# Explanations are done by the model trained on a given fold and\n",
    "# on it's testing data, then merged to create average values.\n",
    "## The structure of the folder with checkpoints should be:\n",
    "checkpoints/ <br>\n",
    "  - fold_0/ <br>\n",
    "    - checkpoint_name.ckpt <br>\n",
    "  - fold_1/ <br>\n",
    "    - checkpoint_name.ckpt <br>\n",
    "  - ... <br>\n",
    "  - fold_n <br>\n",
    "    - checkpoint_name.ckpt <br>  \n",
    "## The same structure follows for the testing data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import GATv2Lightning\n",
    "from utils.dataloader_utils import GraphDataset\n",
    "from torch_geometric.nn import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from statistics import mean, stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"..data/final_kfold/checkpoints_final_folds/\"\n",
    "data_dir = \"../data/final_kfold/saved_folds/\"\n",
    "save_dir_att = \"../explainability_results/attention_connectivity\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confusion matrix\"\"\"\n",
    "try:\n",
    "    del summary_balanced_acc\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    del summary_conf_matrix\n",
    "except NameError:\n",
    "    pass\n",
    "balanced_acc_list = []\n",
    "for n, fold in enumerate(fold_list):\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[n], os.listdir(checkpoint_fold_list[n])[0])\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=False,\n",
    "        log_every_n_steps=1,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "\n",
    "    n_gat_layers = 1\n",
    "    hidden_dim = 32\n",
    "    dropout = 0.0\n",
    "    slope = 0.0025\n",
    "    pooling_method = \"mean\"\n",
    "    norm_method = \"batch\"\n",
    "    activation = \"leaky_relu\"\n",
    "    n_heads = 9\n",
    "    lr = 0.0012\n",
    "    weight_decay = 0.0078\n",
    "    dataset = GraphDataset(data_fold_list[n])\n",
    "    n_classes = 3\n",
    "    features_shape = dataset[0].x.shape[-1]\n",
    "\n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        in_features=features_shape,\n",
    "        n_classes=n_classes,\n",
    "        n_gat_layers=n_gat_layers,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_heads=n_heads,\n",
    "        slope=slope,\n",
    "        dropout=dropout,\n",
    "        pooling_method=pooling_method,\n",
    "        activation=activation,\n",
    "        norm_method=norm_method,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        map_location=torch.device('cpu')\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "    preds = trainer.predict(model,loader)\n",
    "    preds = torch.cat(preds,dim=0)\n",
    "    preds = torch.nn.functional.softmax(preds,dim=1).argmax(dim=1)\n",
    "    ground_truth= torch.tensor([data.y.int().item() for data in dataset])\n",
    "    balanced_acc = balanced_accuracy_score(ground_truth, preds)\n",
    "    balanced_acc_list.append(balanced_acc)\n",
    "    print(f\"Balanced accuracy {fold}: {balanced_acc}\")\n",
    "    metric = MulticlassConfusionMatrix(3,)\n",
    "    conf_matrix = metric(preds, ground_truth).int().numpy()\n",
    "    label_list = []\n",
    "    for row in conf_matrix:\n",
    "        row_sum = sum(row)\n",
    "        val_percentage = row / row_sum\n",
    "        val_percentage = [f\"{val:.2%}\" for val in val_percentage]\n",
    "        new_label_list = [f\"{val}\\n{perc}\" for val, perc in (zip(row, val_percentage))]\n",
    "        label_list += new_label_list\n",
    "    annots = np.asarray(label_list).reshape(3,3)\n",
    "    class_names = [\"preictal\", \"ictal\", \"interictal\"]\n",
    "    disp = sns.heatmap(conf_matrix, xticklabels=class_names,yticklabels=class_names, annot=annots,fmt='',cmap='Blues',)\n",
    "    fig = plt.gcf()\n",
    "    cbar_axes = [ax for ax in fig.get_axes() if isinstance(ax, plt.Axes)]\n",
    "    print(\"Colorbar axes:\", cbar_axes)\n",
    "    cbar = cbar_axes[0].collections[0].colorbar\n",
    "    cbar.set_ticks([])\n",
    "    if cbar_axes:\n",
    "        retrieved_colorbar = cbar_axes[0].collections[0].colorbar\n",
    "        print(\"Retrieved colorbar:\", retrieved_colorbar)\n",
    "    else:\n",
    "        print(\"No colorbar found in the plot.\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    try:\n",
    "        summary_conf_matrix += conf_matrix\n",
    "    except NameError:\n",
    "        summary_conf_matrix = conf_matrix\n",
    "    try:\n",
    "        summary_balanced_acc += balanced_acc\n",
    "    except NameError:\n",
    "        summary_balanced_acc = balanced_acc\n",
    "label_list = []\n",
    "for row in summary_conf_matrix:\n",
    "    row_sum = sum(row)\n",
    "    val_percentage = row / row_sum\n",
    "    val_percentage = [f\"{val:.2%}\" for val in val_percentage]\n",
    "    new_label_list = [f\"{val}\\n{perc}\" for val, perc in (zip(row, val_percentage))]\n",
    "    label_list += new_label_list\n",
    "annots = np.asarray(label_list).reshape(3,3)\n",
    "print(f\"Summary balanced accuracy: {mean(balanced_acc_list):.4%} +/- {stdev(balanced_acc_list):.4%}\")\n",
    "display_final = sns.heatmap(summary_conf_matrix, xticklabels=class_names,yticklabels=class_names, annot=annots,fmt='',cbar=False,cmap='Blues')\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/confusion_matrix.pdf\",dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_dict(input_dict, self_loops=False, threshold=0.0):\n",
    "    g = nx.Graph()\n",
    "    for edge, value in input_dict.items():\n",
    "        if value < threshold:\n",
    "            continue\n",
    "        if self_loops:\n",
    "            g.add_edge(*edge, strength=value)\n",
    "        else:\n",
    "            if edge[0] != edge[1]:\n",
    "                g.add_edge(*edge, strength=value)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"..data/final_kfold/checkpoints_final_folds/\"\n",
    "data_dir = \"../data/final_kfold/saved_folds/\"\n",
    "save_dir_att = \"../explainability_results/attention_connectivity\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import AttentionExplainer, Explainer, ModelConfig\n",
    "att_explainer = AttentionExplainer()\n",
    "torch_geometric.seed_everything(42)\n",
    "for i, fold in enumerate(fold_list):\n",
    "    print(f\"Fold {i}\")\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[i], os.listdir(checkpoint_fold_list[i])[0])\n",
    "    \n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    in_features=features_shape,\n",
    "    n_classes=n_classes,\n",
    "    n_gat_layers=n_gat_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    slope=slope,\n",
    "    dropout=dropout,\n",
    "    pooling_method=pooling_method,\n",
    "    activation=activation,\n",
    "    norm_method=norm_method,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    map_location=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    loader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8, prefetch_factor=20\n",
    "    )\n",
    "    config = ModelConfig(\n",
    "        \"multiclass_classification\", task_level=\"graph\", return_type=\"raw\"\n",
    "    )\n",
    "    explainer = Explainer(\n",
    "        model,\n",
    "        algorithm=att_explainer,\n",
    "        explanation_type=\"model\",\n",
    "        model_config=config,\n",
    "        edge_mask_type=\"object\",\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    edge_connection_dict_all = {}\n",
    "    edge_connection_dict_preictal = {}\n",
    "    edge_connection_dict_interictal = {}\n",
    "    edge_connection_dict_ictal = {}\n",
    "    interictal_cntr = 0\n",
    "    preictal_cntr = 0\n",
    "    ictal_cntr = 0\n",
    "    for n, batch in enumerate(loader):\n",
    "        explanation = explainer(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            target=batch.y,\n",
    "            pyg_batch=batch.batch,\n",
    "        )\n",
    "        for edge_idx in range(explanation.edge_index.size(1)):\n",
    "            edge = explanation.edge_index[:, edge_idx].tolist()\n",
    "            edge.sort()\n",
    "            edge = str(tuple(edge))\n",
    "            edge_mask = explanation.edge_mask[edge_idx].item()\n",
    "            if edge in edge_connection_dict_all.keys():\n",
    "                edge_connection_dict_all[edge] += edge_mask\n",
    "            else:\n",
    "                edge_connection_dict_all[edge] = edge_mask\n",
    "            if batch.y == 0:\n",
    "                if edge in edge_connection_dict_preictal.keys():\n",
    "                    edge_connection_dict_preictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_preictal[edge] = edge_mask\n",
    "            elif batch.y == 1:\n",
    "                if edge in edge_connection_dict_ictal.keys():\n",
    "                    edge_connection_dict_ictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_ictal[edge] = edge_mask\n",
    "            elif batch.y == 2:\n",
    "                if edge in edge_connection_dict_interictal.keys():\n",
    "                    edge_connection_dict_interictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_interictal[edge] = edge_mask\n",
    "        if batch.y == 0:\n",
    "            preictal_cntr += 1\n",
    "        elif batch.y == 1:\n",
    "            ictal_cntr += 1\n",
    "        elif batch.y == 2:\n",
    "            interictal_cntr += 1\n",
    "        \n",
    "        if n % 100 == 0:\n",
    "            print(f\"Batch {n} done\")\n",
    "    \n",
    "    edge_connection_dict_all = {key: value / (n+1) for key, value in edge_connection_dict_all.items()}\n",
    "    edge_connection_dict_interictal = {key: value / interictal_cntr for key, value in edge_connection_dict_interictal.items()}\n",
    "    edge_connection_dict_ictal = {key: value / ictal_cntr for key, value in edge_connection_dict_ictal.items()}\n",
    "    edge_connection_dict_preictal = {key: value / preictal_cntr for key, value in edge_connection_dict_preictal.items()}\n",
    "    save_path_fold = os.path.join(save_dir_att, f\"fold_{i}\")\n",
    "    if not os.path.exists(save_path_fold):\n",
    "        os.makedirs(save_path_fold)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_all.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_all, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_interictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_interictal, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_ictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_ictal, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_preictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_preictal, f)\n",
    "    print(f\"Fold {i} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names = [\n",
    "    \"Fp1\",\n",
    "    \"Fp2\",\n",
    "    \"F7\",\n",
    "    \"F3\",\n",
    "    \"Fz\",\n",
    "    \"F4\",\n",
    "    \"F8\",\n",
    "    \"T7\",\n",
    "    \"C3\",\n",
    "    \"Cz\",\n",
    "    \"C4\",\n",
    "    \"T8\",\n",
    "    \"P7\",\n",
    "    \"P3\",\n",
    "    \"P4\",\n",
    "    \"P8\",\n",
    "    \"O1\",\n",
    "    \"O2\",\n",
    "]\n",
    "custom_labels = {n : ch_names[n] for n in range(len(ch_names))}\n",
    "g = create_graph_from_dict(edge_connection_dict_preictal, threshold=0.3)\n",
    "edge_opacities = [\n",
    "    0.2 + strength * 0.8\n",
    "    for strength in nx.get_edge_attributes(g, \"strength\").values()\n",
    "]\n",
    "pos = nx.circular_layout(g)\n",
    "\n",
    "nx.draw(\n",
    "    g,\n",
    "    with_labels=True,\n",
    "    labels=custom_labels,\n",
    "    pos=pos,\n",
    "    font_weight=\"bold\",\n",
    "    edge_color=edge_opacities,\n",
    "    width=2,\n",
    "    node_size=1000,\n",
    "    node_color=\"white\",\n",
    "    edge_cmap=plt.cm.autumn,\n",
    ")\n",
    "colorbar = plt.colorbar(\n",
    "    plt.cm.ScalarMappable(cmap=plt.cm.autumn),\n",
    ")\n",
    "color = \"white\"\n",
    "colorbar.set_label(label=\"Connection Strength\", color=color)\n",
    "colorbar.ax.yaxis.set_tick_params(color=color, labelcolor=color)\n",
    "fig = plt.gcf()\n",
    "fig.set_facecolor(\"black\")  # Set the background color here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"..data/final_kfold/checkpoints_final_folds/\"\n",
    "data_dir = \"../data/final_kfold/saved_folds/\"\n",
    "save_dir = \"../explainability_results/feature_importance\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import GNNExplainer, Explainer, ModelConfig\n",
    "torch_geometric.seed_everything(42)\n",
    "\n",
    "for i, fold in enumerate(fold_list):\n",
    "    print(fold)\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[i], os.listdir(checkpoint_fold_list[i])[0])\n",
    "    \n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    in_features=features_shape,\n",
    "    n_classes=n_classes,\n",
    "    n_gat_layers=n_gat_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    slope=slope,\n",
    "    dropout=dropout,\n",
    "    pooling_method=pooling_method,\n",
    "    activation=activation,\n",
    "    norm_method=norm_method,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    map_location=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    loader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8, prefetch_factor=20\n",
    "    )\n",
    "    \n",
    "    gnn_explainer = GNNExplainer(epochs=100, lr=0.01)\n",
    "    sum_masks = torch.zeros((18,10))\n",
    "    interictal_masks = torch.zeros((18,10))\n",
    "    ictal_masks = torch.zeros((18,10))\n",
    "    preictal_masks = torch.zeros((18,10))\n",
    "    interictal_cntr = 0\n",
    "    preictal_cntr = 0\n",
    "    ictal_cntr = 0\n",
    "    config = ModelConfig(\n",
    "            \"multiclass_classification\", task_level=\"graph\", return_type=\"raw\"\n",
    "        )\n",
    "    explainer = Explainer(\n",
    "            model,\n",
    "            algorithm=gnn_explainer,\n",
    "            explanation_type=\"model\",\n",
    "            model_config=config,\n",
    "            node_mask_type=\"attributes\",\n",
    "            edge_mask_type='object'\n",
    "        )\n",
    "    for n,batch in enumerate(loader):\n",
    "        batch_unpacked = batch\n",
    "        \n",
    "\n",
    "        explanation = explainer(\n",
    "            x=batch_unpacked.x,\n",
    "            edge_index=batch_unpacked.edge_index,\n",
    "            target=batch_unpacked.y,\n",
    "            pyg_batch=batch_unpacked.batch,\n",
    "        )\n",
    "\n",
    "        sum_masks += explanation.node_mask\n",
    "\n",
    "        if  batch_unpacked.y == 0:\n",
    "            preictal_masks += explanation.node_mask\n",
    "            preictal_cntr += 1\n",
    "        elif batch_unpacked.y == 1:\n",
    "            ictal_masks += explanation.node_mask\n",
    "            ictal_cntr += 1\n",
    "        elif batch_unpacked.y == 2:\n",
    "            interictal_masks += explanation.node_mask\n",
    "            interictal_cntr += 1\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Batch {n} done\")\n",
    "    sum_masks /= n+1\n",
    "    interictal_masks /= interictal_cntr\n",
    "    ictal_masks /= ictal_cntr\n",
    "    preictal_masks /= preictal_cntr\n",
    "\n",
    "    final_explanation_sum = explanation.clone()\n",
    "    final_explanation_interictal = explanation.clone()\n",
    "    final_explanation_preictal = explanation.clone()\n",
    "    final_explanation_ictal = explanation.clone()\n",
    "    \n",
    "    final_explanation_sum.node_mask = sum_masks\n",
    "    final_explanation_interictal.node_mask = interictal_masks\n",
    "    final_explanation_preictal.node_mask = preictal_masks\n",
    "    final_explanation_ictal.node_mask = ictal_masks\n",
    "    \n",
    "   \n",
    "    save_path_fold = os.path.join(save_dir, fold)\n",
    "    if not os.path.exists(save_path_fold):\n",
    "        os.makedirs(save_path_fold)\n",
    "    torch.save(final_explanation_sum, os.path.join(save_path_fold, f\"final_explanation_sum.pt\"))\n",
    "    torch.save(final_explanation_interictal, os.path.join(save_path_fold, f\"final_explanation_interictal.pt\"))\n",
    "    torch.save(final_explanation_preictal, os.path.join(save_path_fold, f\"final_explanation_preictal.pt\"))\n",
    "    torch.save(final_explanation_ictal, os.path.join(save_path_fold, f\"final_explanation_ictal.pt\"))\n",
    "    print(f\" {fold} done\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_explanation = torch.load(os.path.join(save_path_fold, f\"final_explanation_sum.pt\"))\n",
    "feature_labels = ['variance', 'hjorth_mobility','hjorth_complexity',\n",
    "                \"line_length\", \"katz_fd\", \"higuchi_fd\", \"delta_energy\",\n",
    "                \"theta_energy\", \"alpha_energy\", \"beta_energy\"\n",
    "                ]\n",
    "loaded_explanation.visualize_feature_importance(feat_labels=feature_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
