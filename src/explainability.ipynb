{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the network's predicitons in terms of connectivity changes\n",
    "# and feature importance. To run it, one needs to have a final model\n",
    "# checkpoints saved for every fold and the corresponding testing set.\n",
    "# Explanations are done by the model trained on a given fold and\n",
    "# on it's testing data, then merged to create average values.\n",
    "## The structure of the folder with checkpoints should be:\n",
    "checkpoints/ <br>\n",
    "  - fold_0/ <br>\n",
    "    - checkpoint_name.ckpt <br>\n",
    "  - fold_1/ <br>\n",
    "    - checkpoint_name.ckpt <br>\n",
    "  - ... <br>\n",
    "  - fold_n <br>\n",
    "    - checkpoint_name.ckpt <br>  \n",
    "## The same structure follows for the testing data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import GATv2Lightning\n",
    "from utils.dataloader_utils import GraphDataset\n",
    "from torch_geometric.nn import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from statistics import mean, stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../data/thesis_kfold_results/checkpoints_timetables/\"\n",
    "data_dir = \"../data/thesis_kfold_results/final_runs_classic_kfold_timelabels/\"\n",
    "save_dir_att = \"../explainability_results/attention_connectivity_new\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confusion matrix\"\"\"\n",
    "try:\n",
    "    del summary_balanced_acc\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    del summary_conf_matrix\n",
    "except NameError:\n",
    "    pass\n",
    "balanced_acc_list = []\n",
    "for n, fold in enumerate(fold_list):\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[n], os.listdir(checkpoint_fold_list[n])[0])\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=False,\n",
    "        log_every_n_steps=1,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "\n",
    "    n_gat_layers = 1\n",
    "    hidden_dim = 32\n",
    "    dropout = 0.0\n",
    "    slope = 0.0025\n",
    "    pooling_method = \"mean\"\n",
    "    norm_method = \"batch\"\n",
    "    activation = \"leaky_relu\"\n",
    "    n_heads = 9\n",
    "    lr = 0.0012\n",
    "    weight_decay = 0.0078\n",
    "    dataset = GraphDataset(data_fold_list[n])\n",
    "    n_classes = 3\n",
    "    features_shape = dataset[0].x.shape[-1]\n",
    "\n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        in_features=features_shape,\n",
    "        n_classes=n_classes,\n",
    "        n_gat_layers=n_gat_layers,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_heads=n_heads,\n",
    "        slope=slope,\n",
    "        dropout=dropout,\n",
    "        pooling_method=pooling_method,\n",
    "        activation=activation,\n",
    "        norm_method=norm_method,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        map_location=torch.device('cpu')\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "    preds = trainer.predict(model,loader)\n",
    "    preds = torch.cat(preds,dim=0)\n",
    "    preds = torch.nn.functional.softmax(preds,dim=1).argmax(dim=1)\n",
    "    ground_truth= torch.tensor([data.y.int().item() for data in dataset])\n",
    "    balanced_acc = balanced_accuracy_score(ground_truth, preds)\n",
    "    balanced_acc_list.append(balanced_acc)\n",
    "    print(f\"Balanced accuracy {fold}: {balanced_acc}\")\n",
    "    metric = MulticlassConfusionMatrix(3,)\n",
    "    conf_matrix = metric(preds, ground_truth).int().numpy()\n",
    "    label_list = []\n",
    "    for row in conf_matrix:\n",
    "        row_sum = sum(row)\n",
    "        val_percentage = row / row_sum\n",
    "        val_percentage = [f\"{val:.2%}\" for val in val_percentage]\n",
    "        new_label_list = [f\"{val}\\n{perc}\" for val, perc in (zip(row, val_percentage))]\n",
    "        label_list += new_label_list\n",
    "    annots = np.asarray(label_list).reshape(3,3)\n",
    "    class_names = [\"preictal\", \"ictal\", \"interictal\"]\n",
    "    disp = sns.heatmap(conf_matrix, xticklabels=class_names,yticklabels=class_names, annot=annots,fmt='',cmap='Blues',)\n",
    "    fig = plt.gcf()\n",
    "    cbar_axes = [ax for ax in fig.get_axes() if isinstance(ax, plt.Axes)]\n",
    "    print(\"Colorbar axes:\", cbar_axes)\n",
    "    cbar = cbar_axes[0].collections[0].colorbar\n",
    "    cbar.set_ticks([])\n",
    "    if cbar_axes:\n",
    "        retrieved_colorbar = cbar_axes[0].collections[0].colorbar\n",
    "        print(\"Retrieved colorbar:\", retrieved_colorbar)\n",
    "    else:\n",
    "        print(\"No colorbar found in the plot.\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    try:\n",
    "        summary_conf_matrix += conf_matrix\n",
    "    except NameError:\n",
    "        summary_conf_matrix = conf_matrix\n",
    "    try:\n",
    "        summary_balanced_acc += balanced_acc\n",
    "    except NameError:\n",
    "        summary_balanced_acc = balanced_acc\n",
    "label_list = []\n",
    "for row in summary_conf_matrix:\n",
    "    row_sum = sum(row)\n",
    "    val_percentage = row / row_sum\n",
    "    val_percentage = [f\"{val:.2%}\" for val in val_percentage]\n",
    "    new_label_list = [f\"{val}\\n{perc}\" for val, perc in (zip(row, val_percentage))]\n",
    "    label_list += new_label_list\n",
    "annots = np.asarray(label_list).reshape(3,3)\n",
    "print(f\"Summary balanced accuracy: {mean(balanced_acc_list):.4%} +/- {stdev(balanced_acc_list):.4%}\")\n",
    "display_final = sns.heatmap(summary_conf_matrix, xticklabels=class_names,yticklabels=class_names, annot=annots,fmt='',cbar=False,cmap='Blues')\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../plots/confusion_matrix.pdf\",dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../data/thesis_kfold_results/checkpoints_timetables/\"\n",
    "data_dir = \"../data/thesis_kfold_results/final_runs_classic_kfold_timelabels/\"\n",
    "save_dir_att = \"../explainability_results/attention_connectivity_new\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import AttentionExplainer, Explainer, ModelConfig\n",
    "att_explainer = AttentionExplainer()\n",
    "torch_geometric.seed_everything(42)\n",
    "for i, fold in enumerate(fold_list):\n",
    "    print(f\"Fold {i}\")\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[i], os.listdir(checkpoint_fold_list[i])[0])\n",
    "    n_gat_layers = 1\n",
    "    hidden_dim = 32\n",
    "    dropout = 0.0\n",
    "    slope = 0.0025\n",
    "    pooling_method = \"mean\"\n",
    "    norm_method = \"batch\"\n",
    "    activation = \"leaky_relu\"\n",
    "    n_heads = 9\n",
    "    lr = 0.0012\n",
    "    weight_decay = 0.0078\n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    n_classes = 3\n",
    "    features_shape = dataset[0].x.shape[-1]\n",
    "    \n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    in_features=features_shape,\n",
    "    n_classes=n_classes,\n",
    "    n_gat_layers=n_gat_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    slope=slope,\n",
    "    dropout=dropout,\n",
    "    pooling_method=pooling_method,\n",
    "    activation=activation,\n",
    "    norm_method=norm_method,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    map_location=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    loader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8, prefetch_factor=20\n",
    "    )\n",
    "    config = ModelConfig(\n",
    "        \"multiclass_classification\", task_level=\"graph\", return_type=\"raw\"\n",
    "    )\n",
    "    explainer = Explainer(\n",
    "        model,\n",
    "        algorithm=att_explainer,\n",
    "        explanation_type=\"model\",\n",
    "        model_config=config,\n",
    "        edge_mask_type=\"object\",\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    edge_connection_dict_all = {}\n",
    "    edge_connection_dict_preictal = {}\n",
    "    edge_connection_dict_interictal = {}\n",
    "    edge_connection_dict_ictal = {}\n",
    "    interictal_cntr = 0\n",
    "    preictal_cntr = 0\n",
    "    ictal_cntr = 0\n",
    "    for n, batch in enumerate(loader):\n",
    "        explanation = explainer(\n",
    "            x=batch.x,\n",
    "            edge_index=batch.edge_index,\n",
    "            target=batch.y,\n",
    "            pyg_batch=batch.batch,\n",
    "        )\n",
    "        for edge_idx in range(explanation.edge_index.size(1)):\n",
    "            edge = explanation.edge_index[:, edge_idx].tolist()\n",
    "            edge.sort()\n",
    "            edge = str(tuple(edge))\n",
    "            edge_mask = explanation.edge_mask[edge_idx].item()\n",
    "            prediciton = torch.argmax(explanation.prediction)\n",
    "            if edge in edge_connection_dict_all.keys():\n",
    "                edge_connection_dict_all[edge] += edge_mask\n",
    "            else:\n",
    "                edge_connection_dict_all[edge] = edge_mask\n",
    "            if batch.y == 0 and prediciton == 0:\n",
    "                if edge in edge_connection_dict_preictal.keys():\n",
    "                    edge_connection_dict_preictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_preictal[edge] = edge_mask\n",
    "            elif batch.y == 1 and prediciton == 1:\n",
    "                if edge in edge_connection_dict_ictal.keys():\n",
    "                    edge_connection_dict_ictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_ictal[edge] = edge_mask\n",
    "            elif batch.y == 2 and prediciton == 2:\n",
    "                if edge in edge_connection_dict_interictal.keys():\n",
    "                    edge_connection_dict_interictal[edge] += edge_mask\n",
    "                else:\n",
    "                    edge_connection_dict_interictal[edge] = edge_mask\n",
    "        if batch.y == 0 and prediciton == 0:\n",
    "            preictal_cntr += 1\n",
    "        elif batch.y == 1 and prediciton == 1:\n",
    "            ictal_cntr += 1\n",
    "        elif batch.y == 2 and prediciton == 2:\n",
    "            interictal_cntr += 1\n",
    "        \n",
    "        if n % 100 == 0:\n",
    "            print(f\"Batch {n} done\")\n",
    "    \n",
    "    edge_connection_dict_all = {key: value / (n+1) for key, value in edge_connection_dict_all.items()}\n",
    "    edge_connection_dict_interictal = {key: value / interictal_cntr for key, value in edge_connection_dict_interictal.items()}\n",
    "    edge_connection_dict_ictal = {key: value / ictal_cntr for key, value in edge_connection_dict_ictal.items()}\n",
    "    edge_connection_dict_preictal = {key: value / preictal_cntr for key, value in edge_connection_dict_preictal.items()}\n",
    "    save_path_fold = os.path.join(save_dir_att, f\"fold_{i}\")\n",
    "    if not os.path.exists(save_path_fold):\n",
    "        os.makedirs(save_path_fold)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_all.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_all, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_interictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_interictal, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_ictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_ictal, f)\n",
    "    with open(os.path.join(save_path_fold, \"edge_connection_dict_preictal.json\"), \"w\") as f:\n",
    "        json.dump(edge_connection_dict_preictal, f)\n",
    "    print(f\"Fold {i} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_dict(input_dict, self_loops=False, threshold=0.0):\n",
    "    \"\"\"Helper function to create a graph from a dictionary of edges\n",
    "    and their connection strength. The graph is created using networkx.\n",
    "    Args:\n",
    "        input_dict (dict): Dictionary of edges and their connection strength.\n",
    "        self_loops (bool, optional): Whether to include self loops. Defaults to False.\n",
    "        threshold (float, optional): Threshold to apply to the connection strength. \n",
    "        Defaults to 0.0 (all edges included).\n",
    "    Returns:\n",
    "        networkx.Graph: Graph object created from the input dictionary.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()\n",
    "    nodes = np.arange(18)\n",
    "    g.add_nodes_from(nodes)\n",
    "    for edge, value in input_dict.items():\n",
    "        if value < threshold:\n",
    "            continue\n",
    "        if self_loops:\n",
    "            g.add_edge(*edge, strength=value)\n",
    "        else:\n",
    "            if edge[0] != edge[1]:\n",
    "                g.add_edge(*edge, strength=value)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dict(d):\n",
    "    \"\"\" Helper function to reverse a dictionary.\n",
    "    Args:\n",
    "        d (dict): Dictionary to reverse.\n",
    "    Returns:\n",
    "        dict: Reversed dictionary.\n",
    "    \"\"\"\n",
    "    return {v: k for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load computed attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../explainability_results/attention_connectivity_new\"\n",
    "try:\n",
    "    del sum_masks_att\n",
    "except NameError:\n",
    "    print(\"No sum masks\")\n",
    "try:\n",
    "    del ictal_masks_att\n",
    "except NameError:\n",
    "    print(\"No ictal masks\")\n",
    "try:\n",
    "    del interictal_masks_att\n",
    "except NameError:\n",
    "    print(\"No interictal masks\")\n",
    "try:\n",
    "    del preictal_masks_att\n",
    "except NameError:\n",
    "    print(\"No preictal masks\")\n",
    "for n, fold_folder in enumerate(os.listdir(result_dir)):\n",
    "    explanation_filenames_att = [os.path.join(result_dir, fold_folder, f) for f in os.listdir(os.path.join(result_dir, fold_folder)) if f.endswith(\".json\")]\n",
    "    explanation_filenames_att.sort()\n",
    "    for explanation_filename in explanation_filenames_att:\n",
    "        explanation = json.loads(open(explanation_filename).read())\n",
    "        explanation = {eval(k): v for k, v in explanation.items()}\n",
    "        keyword = os.path.basename(explanation_filename).split('_')[-1].split('.')[0]\n",
    "        if keyword == \"all\":\n",
    "            if n == 0:\n",
    "                print(\"Creating sum masks\")\n",
    "                sum_masks_att = explanation\n",
    "            else:\n",
    "                for edge in explanation.keys():\n",
    "                    if edge in sum_masks_att.keys():\n",
    "                        sum_masks_att[edge] += explanation[edge]\n",
    "                    else:\n",
    "                        sum_masks_att[edge] = explanation[edge]\n",
    "        elif keyword == \"ictal\":\n",
    "            if n == 0:\n",
    "                print(\"Creating ictal masks\")\n",
    "                ictal_masks_att = explanation\n",
    "            else:\n",
    "                for edge in explanation.keys():\n",
    "                    if edge in ictal_masks_att.keys():\n",
    "                        ictal_masks_att[edge] += explanation[edge]\n",
    "                    else:\n",
    "                        ictal_masks_att[edge] = explanation[edge]\n",
    "        elif keyword == \"interictal\":\n",
    "            if n == 0:\n",
    "                print(\"Creating interictal masks\")\n",
    "                interictal_masks_att = explanation\n",
    "            else:\n",
    "                for edge in explanation.keys():\n",
    "                    if edge in interictal_masks_att.keys():\n",
    "                        interictal_masks_att[edge] += explanation[edge]\n",
    "                    else:\n",
    "                        interictal_masks_att[edge] = explanation[edge]\n",
    "        elif keyword == \"preictal\":\n",
    "            if n == 0:\n",
    "                print(\"Creating preictal masks\")\n",
    "                preictal_masks_att = explanation\n",
    "            else:\n",
    "                for edge in explanation.keys():\n",
    "                    if edge in preictal_masks_att.keys():\n",
    "                        preictal_masks_att[edge] += explanation[edge]\n",
    "                    else:\n",
    "                        preictal_masks_att[edge] = explanation[edge]\n",
    "fold_number = len(os.listdir(result_dir))\n",
    "sum_masks_att = {k: v/fold_number for k, v in sum_masks_att.items()}\n",
    "ictal_masks_att = {k: v/fold_number for k, v in ictal_masks_att.items()}\n",
    "interictal_masks_att = {k: v/fold_number for k, v in interictal_masks_att.items()}\n",
    "preictal_masks_att = {k: v/fold_number for k, v in preictal_masks_att.items()}\n",
    "\n",
    "# masks_list = [sum_masks_att, ictal_masks_att, interictal_masks_att, preictal_masks_att]\n",
    "### ISBI version with no summed_masks ###\n",
    "masks_list = [interictal_masks_att,preictal_masks_att,ictal_masks_att]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare channel mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names_current = [\n",
    "    \"Fp1\",\n",
    "    \"Fp2\",\n",
    "    \"F7\",\n",
    "    \"F3\",\n",
    "    \"Fz\",\n",
    "    \"F4\",\n",
    "    \"F8\",\n",
    "    \"T7\",\n",
    "    \"C3\",\n",
    "    \"Cz\",\n",
    "    \"C4\",\n",
    "    \"T8\",\n",
    "    \"P7\",\n",
    "    \"P3\",\n",
    "    \"P4\",\n",
    "    \"P8\",\n",
    "    \"O1\",\n",
    "    \"O2\",\n",
    "]\n",
    "ch_dict_current = {ch_name: idx for idx, ch_name in enumerate(ch_names_current)}\n",
    "\n",
    "ch_names_target = [\n",
    "    \"T8\",\n",
    "    \"C4\",\n",
    "    \"F8\",\n",
    "    \"F4\",\n",
    "    \"Fp2\",\n",
    "    \"Fz\",\n",
    "    \"Fp1\",\n",
    "    \"F3\",\n",
    "    \"F7\",\n",
    "    \"C3\",\n",
    "    \"T7\",\n",
    "    \"P7\",\n",
    "    \"P3\",\n",
    "    \"O1\",\n",
    "    \"Cz\",\n",
    "    \"O2\",\n",
    "    \"P4\",\n",
    "    \"P8\",\n",
    "]\n",
    "## A little backflip to change the final graph electrode layout\n",
    "ch_dict_target = reverse_dict({ch_name: ch_dict_current[ch_name] for ch_name in ch_names_target})\n",
    "ch_dict_current = reverse_dict(ch_dict_current)\n",
    "channel_mapping = reverse_dict(dict(zip(ch_dict_current.keys(), ch_dict_target.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "custom_labels = {n : ch_names_current[n] for n in range(len(ch_names_current))}\n",
    "fig, axes = plt.subplots(ncols=1,nrows=3,figsize=(5, 15))\n",
    "axes = axes.ravel()\n",
    "cmap = plt.cm.hot\n",
    "for n,dict_att in enumerate(masks_list):\n",
    "    torch_geometric.seed_everything(42)\n",
    "    g = create_graph_from_dict(dict_att, threshold=0.3)\n",
    "    edge_opacities = [\n",
    "        strength\n",
    "        for strength in nx.get_edge_attributes(g, \"strength\").values()\n",
    "    ]\n",
    "    pos_circular = nx.circular_layout(g)\n",
    "    pos_custom = {n : pos_circular[channel_mapping[n]] for n in range(len(ch_names_current))}\n",
    "    nx.draw(\n",
    "        g,\n",
    "        with_labels=True,\n",
    "        labels=custom_labels,\n",
    "        pos=pos_custom,\n",
    "        font_weight=\"bold\",\n",
    "        edge_color=edge_opacities,\n",
    "        width=2,\n",
    "        node_size=1000,\n",
    "        node_color=\"white\",\n",
    "        edge_cmap=cmap,\n",
    "        ax=axes[n],\n",
    "        edge_vmin=0,\n",
    "        edge_vmax=1,\n",
    "    )\n",
    "    \n",
    "cax = fig.add_axes([1., 0.35, 0.02, 0.3])\n",
    "norm = matplotlib.colors.Normalize(vmin=0.2, vmax=1)\n",
    "colorbar = fig.colorbar(\n",
    "    plt.cm.ScalarMappable(cmap=cmap),cax=cax,\n",
    ")\n",
    "color = \"white\"\n",
    "# colorbar.set_label(label=\"Connection Strength\", color=color, size=15)\n",
    "colorbar.ax.set_ylim([0.2,1])\n",
    "colorbar.ax.yaxis.set_tick_params(color=color, labelcolor=color, size=5, labelrotation=00)\n",
    "colorbar.ax.tick_params(axis='y', which='major', pad=5, labelsize=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_facecolor(\"black\")  # Set the background color here\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/szymon/code/sano/sano_eeg/plots/conncectivity_isbi.pdf\",dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../data/thesis_kfold_results/checkpoint_dir/final_runs_lookback_1200\"\n",
    "data_dir = \"../data/thesis_kfold_results/fold_dir/final_runs_lookback_1200\"\n",
    "save_dir = \"../explainability_results/feature_importance_new\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_importances(scores, labels, ax):\n",
    "    \"\"\"Helper function to plot the feature importances.\n",
    "    Args:\n",
    "        scores (np.array): Array of feature importances.\n",
    "        labels (list): List of feature names.\n",
    "        ax (matplotlib.axes): Matplotlib axes object.\n",
    "    \"\"\"\n",
    "    # Sort the scores and labels together while keeping the coherence\n",
    "    fontsize = 12\n",
    "    sorted_indices = np.argsort(scores)\n",
    "    sorted_scores = scores[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    # Create the bar plot\n",
    "    ax.barh(range(len(sorted_scores)), sorted_scores)\n",
    "    ax.set_yticks(range(len(sorted_scores)), sorted_labels)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import GNNExplainer, Explainer, ModelConfig\n",
    "torch_geometric.seed_everything(42)\n",
    "\n",
    "for i, fold in enumerate(fold_list):\n",
    "    print(fold)\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[i], os.listdir(checkpoint_fold_list[i])[0])\n",
    "    \n",
    "    n_gat_layers = 1\n",
    "    hidden_dim = 32\n",
    "    dropout = 0.0\n",
    "    slope = 0.0025\n",
    "    pooling_method = \"mean\"\n",
    "    norm_method = \"batch\"\n",
    "    activation = \"leaky_relu\"\n",
    "    n_heads = 9\n",
    "    lr = 0.0012\n",
    "    weight_decay = 0.0078\n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    n_classes = 3\n",
    "    features_shape = dataset[0].x.shape[-1]\n",
    "\n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    in_features=features_shape,\n",
    "    n_classes=n_classes,\n",
    "    n_gat_layers=n_gat_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    slope=slope,\n",
    "    dropout=dropout,\n",
    "    pooling_method=pooling_method,\n",
    "    activation=activation,\n",
    "    norm_method=norm_method,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    map_location=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    loader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, drop_last=False, num_workers=8, prefetch_factor=20\n",
    "    )\n",
    "    \n",
    "    gnn_explainer = GNNExplainer(epochs=100, lr=0.01)\n",
    "    sum_masks = torch.zeros((18,10))\n",
    "    interictal_masks = torch.zeros((18,10))\n",
    "    ictal_masks = torch.zeros((18,10))\n",
    "    preictal_masks = torch.zeros((18,10))\n",
    "    interictal_cntr = 0\n",
    "    preictal_cntr = 0\n",
    "    ictal_cntr = 0\n",
    "    config = ModelConfig(\n",
    "            \"multiclass_classification\", task_level=\"graph\", return_type=\"raw\"\n",
    "        )\n",
    "    explainer = Explainer(\n",
    "            model,\n",
    "            algorithm=gnn_explainer,\n",
    "            explanation_type=\"model\",\n",
    "            model_config=config,\n",
    "            node_mask_type=\"attributes\",\n",
    "            edge_mask_type='object'\n",
    "        )\n",
    "    for n,batch in enumerate(loader):\n",
    "        batch_unpacked = batch\n",
    "        \n",
    "\n",
    "        explanation = explainer(\n",
    "            x=batch_unpacked.x,\n",
    "            edge_index=batch_unpacked.edge_index,\n",
    "            target=batch_unpacked.y,\n",
    "            pyg_batch=batch_unpacked.batch,\n",
    "        )\n",
    "        prediciton = torch.argmax(explanation.prediction)\n",
    "\n",
    "        sum_masks += explanation.node_mask\n",
    "\n",
    "        if  batch_unpacked.y == 0 and prediciton == 0:\n",
    "            preictal_masks += explanation.node_mask\n",
    "            preictal_cntr += 1\n",
    "        elif batch_unpacked.y == 1  and prediciton == 1:\n",
    "            ictal_masks += explanation.node_mask\n",
    "            ictal_cntr += 1\n",
    "        elif batch_unpacked.y == 2 and prediciton == 2:\n",
    "            interictal_masks += explanation.node_mask\n",
    "            interictal_cntr += 1\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Batch {n} done\")\n",
    "    sum_masks /= n+1\n",
    "    interictal_masks /= interictal_cntr\n",
    "    ictal_masks /= ictal_cntr\n",
    "    preictal_masks /= preictal_cntr\n",
    "\n",
    "    final_explanation_sum = explanation.clone()\n",
    "    final_explanation_interictal = explanation.clone()\n",
    "    final_explanation_preictal = explanation.clone()\n",
    "    final_explanation_ictal = explanation.clone()\n",
    "    \n",
    "    final_explanation_sum.node_mask = sum_masks\n",
    "    final_explanation_interictal.node_mask = interictal_masks\n",
    "    final_explanation_preictal.node_mask = preictal_masks\n",
    "    final_explanation_ictal.node_mask = ictal_masks\n",
    "    \n",
    "   \n",
    "    save_path_fold = os.path.join(save_dir, fold)\n",
    "    if not os.path.exists(save_path_fold):\n",
    "        os.makedirs(save_path_fold)\n",
    "    torch.save(final_explanation_sum, os.path.join(save_path_fold, f\"final_explanation_sum.pt\"))\n",
    "    torch.save(final_explanation_interictal, os.path.join(save_path_fold, f\"final_explanation_interictal.pt\"))\n",
    "    torch.save(final_explanation_preictal, os.path.join(save_path_fold, f\"final_explanation_preictal.pt\"))\n",
    "    torch.save(final_explanation_ictal, os.path.join(save_path_fold, f\"final_explanation_ictal.pt\"))\n",
    "    print(f\" {fold} done\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the computed feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../explainability_results/feature_importance_new/\"\n",
    "try:\n",
    "    del sum_masks\n",
    "except NameError:\n",
    "    print(\"No sum masks\")\n",
    "try:\n",
    "    del ictal_masks\n",
    "except NameError:\n",
    "    print(\"No ictal masks\")\n",
    "try:\n",
    "    del interictal_masks\n",
    "except NameError:\n",
    "    print(\"No interictal masks\")\n",
    "try:\n",
    "    del preictal_masks\n",
    "except NameError:\n",
    "    print(\"No preictal masks\")\n",
    "for n, fold_folder in enumerate(os.listdir(result_dir)):\n",
    "    explanation_filenames = [os.path.join(result_dir, fold_folder, f) for f in os.listdir(os.path.join(result_dir, fold_folder)) if f.endswith(\".pt\")]\n",
    "    explanation_filenames.sort()\n",
    " \n",
    "    for explanation_filename in explanation_filenames:\n",
    "        explanation = torch.load(explanation_filename)\n",
    "        keyword = os.path.basename(explanation_filename).split('_')[-1].split('.')[0]\n",
    "        if 'sum' == keyword:\n",
    "            try:\n",
    "                sum_masks += explanation.node_mask\n",
    "            except NameError:\n",
    "                print(\"Error\")\n",
    "                sum_masks = explanation.node_mask\n",
    "        elif 'ictal' == keyword:\n",
    "            try:\n",
    "                ictal_masks += explanation.node_mask\n",
    "            except NameError:\n",
    "                print(\"Error\")\n",
    "                ictal_masks = explanation.node_mask\n",
    "        elif 'interictal' == keyword:\n",
    "            try:\n",
    "                interictal_masks += explanation.node_mask\n",
    "            except NameError:\n",
    "                print(\"Error\")\n",
    "                interictal_masks = explanation.node_mask\n",
    "        elif 'preictal' == keyword:\n",
    "            try:\n",
    "                preictal_masks += explanation.node_mask\n",
    "            except NameError:\n",
    "                print(\"Error\")\n",
    "                preictal_masks = explanation.node_mask\n",
    "sum_masks /= len(explanation_filenames)\n",
    "ictal_masks /= len(explanation_filenames)\n",
    "interictal_masks /= len(explanation_filenames)\n",
    "preictal_masks /= len(explanation_filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = np.array(['Hjorth activity', 'Hjorth mobility','Hjorth complexity',\n",
    "                \"Line length\", \"Katz FD\", \"Higuch FD\", \"Delta band energy\",\n",
    "                \"Theta band energy\", \"Alpha band energy\", \"Beta band energy\"\n",
    "                ])\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7, 8),sharex=True)\n",
    "show_importances(interictal_masks, feature_labels, axes[0])\n",
    "show_importances(preictal_masks, feature_labels, axes[1])\n",
    "show_importances(ictal_masks, feature_labels, axes[2])\n",
    "plt.tight_layout()  # To avoid label cutoffs\n",
    "\n",
    "fig.savefig(\"/home/szymon/code/sano/sano_eeg/plots/feature_importance_isbi.pdf\",dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interictal predictions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK = 600\n",
    "checkpoint_dir = f\"../data/thesis_kfold_results/checkpoint_dir/final_runs_lookback_{LOOKBACK}/\"\n",
    "data_dir = f\"../data/thesis_kfold_results/fold_dir/final_runs_lookback_{LOOKBACK}/\"\n",
    "save_dir = \"../explainability_results/interictal_analisys_new\"\n",
    "fold_list = os.listdir(checkpoint_dir)\n",
    "checkpoint_fold_list = [os.path.join(checkpoint_dir, fold) for fold in fold_list]\n",
    "data_fold_list = [os.path.join(data_dir, fold) for fold in fold_list]\n",
    "fold_list.sort()\n",
    "data_fold_list.sort()\n",
    "checkpoint_fold_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "torch_geometric.seed_everything(42)\n",
    "time_labels_counter_dict = {}\n",
    "time_labels_correct_counter_dict = {}\n",
    "for i, fold in enumerate(fold_list):\n",
    "    print(fold)\n",
    "    checkpoint_path = os.path.join(checkpoint_fold_list[i], os.listdir(checkpoint_fold_list[i])[0])\n",
    "    \n",
    "    n_gat_layers = 1\n",
    "    hidden_dim = 32\n",
    "    dropout = 0.0\n",
    "    slope = 0.0025\n",
    "    pooling_method = \"mean\"\n",
    "    norm_method = \"batch\"\n",
    "    activation = \"leaky_relu\"\n",
    "    n_heads = 9\n",
    "    lr = 0.0012\n",
    "    weight_decay = 0.0078\n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    n_classes = 3\n",
    "    features_shape = dataset[0].x.shape[-1]\n",
    "\n",
    "    model = GATv2Lightning.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    in_features=features_shape,\n",
    "    n_classes=n_classes,\n",
    "    n_gat_layers=n_gat_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_heads=n_heads,\n",
    "    slope=slope,\n",
    "    dropout=dropout,\n",
    "    pooling_method=pooling_method,\n",
    "    activation=activation,\n",
    "    norm_method=norm_method,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    map_location=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    dataset = GraphDataset(data_fold_list[i])\n",
    "    interictal_samples_index = []\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if sample.y == 0:\n",
    "            interictal_samples_index.append(i)\n",
    "    interictal_subset = Subset(dataset, interictal_samples_index)\n",
    "    loader = DataLoader(\n",
    "    interictal_subset, batch_size=1, shuffle=False, drop_last=False, num_workers=8, prefetch_factor=20\n",
    "    )\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        model.eval()\n",
    "        features = batch.x\n",
    "        edge_index = batch.edge_index\n",
    "        label = batch.y\n",
    "        time_label = int(batch.time_labels.item())\n",
    "        batch_id = batch.batch\n",
    "        pred = model(features, edge_index, batch_id)\n",
    "        pred = pred.argmax(dim=1)\n",
    "        if time_label in time_labels_counter_dict.keys():\n",
    "            time_labels_counter_dict[time_label] += 1\n",
    "        else:\n",
    "            time_labels_counter_dict[time_label] = 1\n",
    "        if pred == label:\n",
    "            if time_label in time_labels_correct_counter_dict.keys():\n",
    "                time_labels_correct_counter_dict[time_label] += 1\n",
    "            else:\n",
    "                time_labels_correct_counter_dict[time_label] = 1\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i} done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "time_labels_correct_counter_dict_copy = deepcopy(time_labels_correct_counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in time_labels_correct_counter_dict_copy.keys():\n",
    "    time_labels_correct_counter_dict_copy[key] = time_labels_correct_counter_dict_copy[key] / time_labels_counter_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in time_labels_counter_dict.keys():\n",
    "    if key not in time_labels_correct_counter_dict_copy.keys():\n",
    "        time_labels_correct_counter_dict_copy[key] = 0\n",
    "        print(f\"Key {key} not in dict\")\n",
    "sorted_dict = dict(sorted(time_labels_correct_counter_dict_copy.items(), reverse=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_labels, time_labels_counts = zip(*sorted_dict.items())\n",
    "plt.plot(time_labels, time_labels_counts)\n",
    "plt.ylim([0.7,1])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.savefig(f\"../plots/interictal_accuracy_timestep_{LOOKBACK}.pdf\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict_save_path = f\"../explainability_results/time_labels/{LOOKBACK}/time_labels_correct_counter_dict_sorted_normalized.json\"\n",
    "total_dict_save_path = f\"../explainability_results/time_labels/{LOOKBACK}/time_labels_counter_dict.json\"\n",
    "if not os.path.exists(os.path.dirname(correct_dict_save_path)):\n",
    "    os.makedirs(os.path.dirname(correct_dict_save_path))\n",
    "\n",
    "with open(correct_dict_save_path, \"w\") as f:\n",
    "    json.dump(sorted_dict, f)\n",
    "with open(total_dict_save_path, \"w\") as f:\n",
    "    json.dump(time_labels_counter_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
