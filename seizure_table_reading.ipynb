{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from torch_geometric_temporal import  DynamicGraphTemporalSignal,StaticGraphTemporalSignal, temporal_signal_split, DynamicGraphTemporalSignalBatch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import scipy\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DynamicBatchSampler,DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN,  GConvGRU, A3TGCN, TGCN2\n",
    "from torch_geometric_temporal.nn.attention import STConv\n",
    "from torchmetrics.classification import BinaryRecall, AUROC, ROC\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import timeit\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv,BatchNorm\n",
    "from sklearn.model_selection import KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plv_connectivity(sensors,data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sensors : INT\n",
    "        DESCRIPTION. No of sensors used for capturing EEG\n",
    "    data : Array of float \n",
    "        DESCRIPTION. EEG Data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    connectivity_matrix : Matrix of float\n",
    "        DESCRIPTION. PLV connectivity matrix\n",
    "    connectivity_vector : Vector of flaot \n",
    "        DESCRIPTION. PLV connectivity vector\n",
    "    \"\"\"\n",
    "    print(\"PLV in process.....\")\n",
    "    \n",
    "    # Predefining connectivity matrix\n",
    "    connectivity_matrix = np.zeros([sensors,sensors],dtype=float)\n",
    "    \n",
    "    # Computing hilbert transform\n",
    "    data_points = data.shape[-1]\n",
    "    data_hilbert = np.imag(scipy.signal.hilbert(data))\n",
    "    phase = np.arctan(data_hilbert/data)\n",
    "    \n",
    "    # Computing connectivity matrix \n",
    "    for i in range(sensors):\n",
    "        for k in range(sensors):\n",
    "            connectivity_matrix[i,k] = np.abs(np.sum(np.exp(1j*(phase[i,:]-phase[k,:]))))/data_points\n",
    "            \n",
    "    # Computing connectivity vector\n",
    "   # connectivity_vector = connectivity_matrix[np.triu_indices(connectivity_matrix.shape[0],k=1)] \n",
    "      \n",
    "    # returning connectivity matrix and vector\n",
    "    \n",
    "    return connectivity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recordings_plv(npy_dataset_path,dst_path):\n",
    "    patient_list = os.listdir(npy_dataset_path)\n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "    for patient in patient_list: # iterate over patient names\n",
    "        patient_path = os.path.join(npy_dataset_path,patient)\n",
    "        recording_list = os.listdir(patient_path)\n",
    "        save_folder = os.path.join(dst_path,patient)\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.mkdir(save_folder)\n",
    "        for record in recording_list: # iterate over recordings for a patient\n",
    "            recording_path = os.path.join(patient_path,record)\n",
    "            data_array = np.load(recording_path) # load the recording\n",
    "            starttime = timeit.default_timer()\n",
    "            print(f'Calculating PLV for {record}')\n",
    "            plv_array = plv_connectivity(data_array.shape[0],data_array)\n",
    "            target_filename = os.path.join(save_folder,record)\n",
    "            np.save(target_filename,plv_array)\n",
    "            print(\"The time of calculation is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think about using kwargs argument here to specify args for dataloader\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path :Path\n",
    "    event_tables_path : Path\n",
    "    plv_values_path : Path\n",
    "    loso_patient : str = None\n",
    "    sampling_f : int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    self_loops : bool = True\n",
    "    balance : bool = True\n",
    "    train_test_split:  float = None\n",
    "    \n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path {Path} -- Path to folder with dataset preprocessed into .npy files.\n",
    "        event_tables_path {Path} -- Path to folder with .csv files containing seizure events information for every patient.\n",
    "        loso_patient {str} -- Name of patient to be selected for LOSO valdiation, specified in format \"chb{patient_number}\"\",\n",
    "        eg. \"chb16\". (default: {None}).\n",
    "        samplin_f {int} -- Sampling frequency of the loaded eeg data. (default: {256}).\n",
    "        seizure_lookback {int} -- Time horizon to sample pre-seizure data (length of period before seizure) in seconds. \n",
    "        (default: {600}).\n",
    "        sample_timestep {int} -- Amounts of seconds analyzed in a single sample. (default: {5}).\n",
    "        overlap {int} -- Amount of seconds overlap between samples. (default: {0}).\n",
    "        self_loops {bool} -- Wheather to add self loops to nodes of the graph. (default: {True}).\n",
    "        shuffle {bool} --  Wheather to shuffle training samples.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def _get_event_tables(self,patient_name : str) -> tuple[dict,dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [os.path.join(self.event_tables_path,ev_table)\n",
    "        for ev_table in event_table_list if patient_name in ev_table]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[0] ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict('index')\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict('index')\n",
    "        return start_events_dict,stop_events_dict\n",
    "        \n",
    "    def _get_recording_events(self,events_dict,recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict\"\"\"\n",
    "        recording_list = list(events_dict[recording+'.edf'].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "\n",
    "    def _get_graph(self,n_nodes: int) -> nx.Graph :\n",
    "        \"\"\"Creates Networx fully connected graph with self loops\"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node,node]for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "    \n",
    "    def _get_edge_weights_recording(self,plv_values: np.ndarray) ->np.ndarray:\n",
    "        \"\"\"Method that takes plv values for given recording and assigns them \n",
    "        as edge attributes to a fc graph.\"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start,e_end = edge\n",
    "            garph_dict[edge] = {'plv':plv_values[e_start,e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "    \n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(),axis=0)\n",
    "        edges_per_sample_train = np.repeat(edges,repeats =self._features.shape[0],axis=0)\n",
    "        self._edges = torch.tensor(edges_per_sample_train)\n",
    "        if self.loso_patient is not None:\n",
    "            edges_per_sample_val = np.repeat(edges,repeats =self._val_features.shape[0],axis=0)\n",
    "            self._val_edges = torch.tensor(edges_per_sample_val)\n",
    "       \n",
    "    def _array_to_tensor(self):\n",
    "        \"\"\"Method converting features, edges and weights to torch.tensors\"\"\"\n",
    "        self._features = torch.tensor(self._features)\n",
    "        self._labels = torch.tensor(self._labels)\n",
    "        self._time_labels = torch.tensor(self._time_labels)\n",
    "        self._edge_weights = torch.tensor(self._edge_weights)\n",
    "     \n",
    "    \n",
    "    def _val_array_to_tensor(self):\n",
    "        self._val_features = torch.tensor(self._val_features)\n",
    "        self._val_labels = torch.tensor(self._val_labels)\n",
    "        self._val_time_labels = torch.tensor(self._val_time_labels)\n",
    "        self._val_edge_weights = torch.tensor(self._val_edge_weights)\n",
    "     \n",
    "        \n",
    "    def _get_labels_count(self):\n",
    "        labels,counts = np.unique(self._labels,return_counts=True)\n",
    "        self._label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._label_counts[int(label)] = counts[n]\n",
    "        \n",
    "    def _get_val_labels_count(self):\n",
    "        labels,counts = np.unique(self._val_labels,return_counts=True)\n",
    "        self._val_label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._val_label_counts[int(label)] = counts[n]\n",
    "        \n",
    "    def _balance_classes(self):\n",
    "        negative_label = self._label_counts[0]\n",
    "        positive_label = self._label_counts[1]\n",
    "    \n",
    "        imbalance = negative_label - positive_label\n",
    "        negative_indices = np.where(self._labels == 0)[0]\n",
    "        indices_to_discard = np.random.choice(negative_indices,size = imbalance,replace=False)\n",
    "\n",
    "        self._features = np.delete(self._features,obj=indices_to_discard,axis=0)\n",
    "        self._labels = np.delete(self._labels,obj=indices_to_discard,axis=0)\n",
    "        self._time_labels = np.delete(self._time_labels,obj=indices_to_discard,axis=0)\n",
    "        self._edge_weights = np.delete(self._edge_weights,obj=indices_to_discard,axis=0)\n",
    "\n",
    "        \n",
    "    def _get_labels_features_edge_weights(self):\n",
    "        \"\"\"Prepare features, labels, time labels and edge wieghts for training and \n",
    "        optionally validation data.\"\"\"\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        for patient in patient_list: # iterate over patient names\n",
    "            event_tables = self._get_event_tables(patient) # extract start and stop of seizure for patient \n",
    "            patient_path = os.path.join(self.npy_dataset_path,patient)\n",
    "            recording_list = os.listdir(patient_path)\n",
    "            for record in recording_list: # iterate over recordings for a patient\n",
    "                recording_path = os.path.join(patient_path,record)\n",
    "                record_id = record.split('.npy')[0] #  get record id\n",
    "                start_event_tables = self._get_recording_events(event_tables[0],record_id) # get start events\n",
    "                stop_event_tables = self._get_recording_events(event_tables[1],record_id) # get stop events\n",
    "                data_array = np.load(recording_path) # load the recording\n",
    "\n",
    "                plv_edge_weights = np.expand_dims(\n",
    "                    self._get_edge_weights_recording(\n",
    "                        np.load(os.path.join(self.plv_values_path,patient,record))\n",
    "          \n",
    "                ),\n",
    "                axis = 0\n",
    "                )\n",
    "\n",
    "                ##TODO add a gateway to reject seizure periods shorter than lookback\n",
    "                # extract timeseries and labels from the array\n",
    "                features,labels,time_labels = utils.extract_training_data_and_labels(\n",
    "                    data_array,\n",
    "                    start_event_tables,\n",
    "                    stop_event_tables,\n",
    "                    fs = self.sampling_f,\n",
    "                    seizure_lookback = self.seizure_lookback,\n",
    "                    sample_timestep = self.sample_timestep,\n",
    "                    inter_overlap = self.inter_overlap,\n",
    "                    ictal_overlap = self.ictal_overlap\n",
    "                )\n",
    "                \n",
    "                time_labels = np.expand_dims(time_labels.astype(np.int32),1)\n",
    "                labels = labels.reshape((labels.shape[0],1)).astype(np.float32)\n",
    "            \n",
    "                if patient == self.loso_patient:\n",
    "                    try:\n",
    "                        self._val_features = np.concatenate((self._val_features, features))\n",
    "                        self._val_labels = np.concatenate((self._val_labels, labels))\n",
    "                        self._val_time_labels = np.concatenate((self._val_time_labels , time_labels))\n",
    "                        self._val_edge_weights = np.concatenate((\n",
    "                            self._val_edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                            ))\n",
    "                    except:\n",
    "                        self._val_features = features\n",
    "                        self._val_labels = labels\n",
    "                        self._val_time_labels = time_labels\n",
    "                        self._val_edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        self._features = np.concatenate((self._features, features))\n",
    "                        self._labels = np.concatenate((self._labels, labels))\n",
    "                        self._time_labels = np.concatenate((self._time_labels , time_labels))\n",
    "                        self._edge_weights = np.concatenate((\n",
    "                            self._edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                        ))\n",
    "                       \n",
    "                    except:\n",
    "                        print(\"Creating initial attributes\")\n",
    "                        self._features = features\n",
    "                        self._labels = labels\n",
    "                        self._time_labels = time_labels\n",
    "                        self._edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                \n",
    "        \n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self) -> DynamicGraphTemporalSignal:\n",
    "\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes \n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of \n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        \n",
    "        self._get_labels_features_edge_weights()\n",
    "        if self.balance:\n",
    "            self._get_labels_count()\n",
    "            self._balance_classes()\n",
    "        self._get_edges()\n",
    "        self._get_labels_count()\n",
    "        \n",
    "        self._array_to_tensor()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "        self._features, self._edges, self._edge_weights, self._labels,  self._time_labels\n",
    "        )\n",
    "        if self.train_test_split is not None:\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                train_dataset,[1-self.train_test_split,self.train_test_split]\n",
    "            )\n",
    "            \n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            \n",
    "            val_dataloader = torch.utils.data.DataLoader(\n",
    "                val_dataset, batch_size = 16,shuffle = False,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            loaders = [train_dataloader,val_dataloader]\n",
    "        else:\n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            loaders = [train_dataloader]\n",
    "        if self.loso_patient:\n",
    "            self._get_val_labels_count()\n",
    "            self._val_array_to_tensor()\n",
    "            loso_dataset = torch.utils.data.TensorDataset(\n",
    "                self._val_features, self._val_edges, self._val_edge_weights, self._val_labels, self._val_time_labels\n",
    "            )\n",
    "            loso_dataloader = torch.utils.data.DataLoader(\n",
    "                loso_dataset, batch_size = 16,shuffle = False,pin_memory = True,num_workers=2, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            return (*loaders,loso_dataloader)\n",
    "\n",
    "        return (*loaders,)\n",
    "        \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = TGCN2(timestep*sfreq,32,batch_size = batch_size, add_self_loops=True,improved=True)\n",
    "        self.recurrent_2 = TGCN2(32,64,batch_size = batch_size,add_self_loops=True,improved=True)\n",
    "        self.recurrent_3 = TGCN2(64,128,batch_size = batch_size,add_self_loops=True,improved=True)\n",
    "        self.fc1 = torch.nn.Linear(n_nodes*128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.squeeze(x)\n",
    "  \n",
    "       # x = F.normalize(x,dim=1)\n",
    "        h = self.recurrent_1(x, edge_index=edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_2(h, edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_3(h, edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.flatten(h)\n",
    "      \n",
    "        #h = global_mean_pool(h,torch.zeros(self.n_nodes,dtype=torch.int64)).squeeze()\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial attributes\n"
     ]
    }
   ],
   "source": [
    "TIMESTEP = 5\n",
    "INTER_OVERLAP = 0\n",
    "ICTAL_OVERLAP = 0\n",
    "SFREQ = 256\n",
    "\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('data/npy_data'),\n",
    "    event_tables_path=Path('data/event_tables'),\n",
    "    plv_values_path=Path('data/plv_arrays'),\n",
    "    loso_patient='chb16',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    inter_overlap=INTER_OVERLAP,\n",
    "    ictal_overlap=ICTAL_OVERLAP,\n",
    "    self_loops=False,\n",
    "    balance=False,\n",
    "    train_test_split=0.2\n",
    "    \n",
    "    )\n",
    "train_loader,val_loader,loso_dataloader=dataloader.get_dataset()\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_accuracy(y_true, y_prob):\n",
    "    \"\"\"Binary accuracy calculation\"\"\"\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_prob = np.where(y_prob <= 0.0, 0, y_prob)\n",
    "    y_prob = np.where(y_prob > 0.0, 1, y_prob)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_prob)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "## 1. Implement kfold cross validation .get_dataset() will return also a kfold cross validation split?\n",
    "## 2. Think of time series augmentation techniques\n",
    "## 3. Try to add a feature balancing between ictal and iterictal periods.\n",
    "## 4.Try to run the algorithm on list of patients shown in the articles.\n",
    "## 5. !Why there is no patient 15 and 20 in the dataset? !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kfold loop\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "# mean = dataloader._features.squeeze().mean(dim=0)\n",
    "# std = dataloader._features.squeeze().std(dim=0)\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_loader)))):\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "        #pos_weight=torch.full([1], 1.1\n",
    "        loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], 8.64))\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "        recall = BinaryRecall(threshold=0.5)\n",
    "        auroc = AUROC(task=\"binary\")\n",
    "        roc = ROC('binary')\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=train_sampler,drop_last = True)\n",
    "        test_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=test_sampler,drop_last = True)\n",
    "        print(f'Fold {fold+1}')\n",
    "        for epoch in tqdm(range(30)):\n",
    "\n",
    "                preds = []\n",
    "                ground_truth = []\n",
    "                epoch_loss = 0.0\n",
    "                epoch_loss_valid = 0.0\n",
    "                preds_valid = []\n",
    "                ground_truth_valid = []\n",
    "                model.train()\n",
    "                sample_counter = 0\n",
    "                batch_counter = 0\n",
    "                for time, batch in enumerate(train_loader_fold):\n",
    "                        x, edge_index, edge_attr,y = batch[0:4]\n",
    "                        x = x.squeeze()\n",
    "                        #mean = torch.mean(x,dim=0)\n",
    "                        #std = torch.std(x,dim=0)\n",
    "                        x = (x-mean)/std\n",
    "                        y_hat = model(x=x.float(), edge_index=edge_index[0])\n",
    "                        ##loss\n",
    "                        loss = loss_fn(y_hat,y)\n",
    "                        \n",
    "                        epoch_loss += loss\n",
    "                        ## get preds & gorund truth\n",
    "                        preds.append(y_hat.detach().numpy())\n",
    "                        ground_truth.append(y.numpy())\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                ## calculate acc\n",
    "\n",
    "                train_auroc = auroc(torch.FloatTensor(preds),torch.FloatTensor(ground_truth))\n",
    "                sensitivity = recall(torch.FloatTensor(preds),torch.FloatTensor(ground_truth))\n",
    "                print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "                print(f'Epoch AUROC: {train_auroc} ')\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                        for time_valid, batch_valid in enumerate(test_loader_fold):\n",
    "                                x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                                #print(y_val)\n",
    "                                x = x.squeeze()\n",
    "                                #mean = torch.mean(x,dim=0)\n",
    "                                #std = torch.std(x,dim=0)\n",
    "                                x = (x-mean)/std\n",
    "                                y_hat_val = model(x=x.float(), edge_index=edge_index[0])\n",
    "                                loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                                #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                                epoch_loss_valid += loss_valid\n",
    "                                preds_valid.append(y_hat_val.detach().numpy())\n",
    "                                ground_truth_valid.append(y_val.numpy())\n",
    "\n",
    "                val_auroc = auroc(torch.FloatTensor(preds_valid),torch.FloatTensor(ground_truth_valid))\n",
    "                val_sensitivity = recall(torch.FloatTensor(preds_valid),torch.FloatTensor(ground_truth_valid))\n",
    "                print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "                print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Epoch: 0 Epoch sensitivity: 0.49509522318840027 Epoch loss: 2.14620374402842\n",
      "Epoch AUROC: 0.700488269329071 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:42<13:20, 42.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Epoch val_sensitivity: 0.7694235444068909 Epoch val_loss: 2.035345196723938\n",
      "Epoch val AUROC: 0.7854689359664917 \n",
      "0.0001\n",
      "Epoch: 1 Epoch sensitivity: 0.6172055602073669 Epoch loss: 2.042982062439143\n",
      "Epoch AUROC: 0.7831432819366455 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:24<12:41, 42.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Epoch val_sensitivity: 0.6140350699424744 Epoch val_loss: 1.9316887855529785\n",
      "Epoch val AUROC: 0.8258071541786194 \n",
      "0.0001\n",
      "Epoch: 2 Epoch sensitivity: 0.682631254196167 Epoch loss: 1.9782968017815847\n",
      "Epoch AUROC: 0.8171514272689819 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:05<11:50, 41.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Epoch val_sensitivity: 0.7969924807548523 Epoch val_loss: 1.9048713445663452\n",
      "Epoch val AUROC: 0.8432170152664185 \n",
      "0.0001\n",
      "Epoch: 3 Epoch sensitivity: 0.7091748118400574 Epoch loss: 1.9740253511631267\n",
      "Epoch AUROC: 0.8230531215667725 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:49<11:18, 42.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Epoch val_sensitivity: 0.7719298005104065 Epoch val_loss: 1.821280598640442\n",
      "Epoch val AUROC: 0.8638901114463806 \n",
      "0.0001\n",
      "Epoch: 4 Epoch sensitivity: 0.7166762948036194 Epoch loss: 1.9365580855000912\n",
      "Epoch AUROC: 0.8375841975212097 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:35<10:56, 43.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Epoch val_sensitivity: 0.7769423723220825 Epoch val_loss: 1.812062382698059\n",
      "Epoch val AUROC: 0.8596127033233643 \n",
      "0.0001\n",
      "Epoch: 5 Epoch sensitivity: 0.7218695878982544 Epoch loss: 1.9299124445631999\n",
      "Epoch AUROC: 0.8394788503646851 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [04:24<10:37, 45.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Epoch val_sensitivity: 0.7844611406326294 Epoch val_loss: 1.8126726746559143\n",
      "Epoch val AUROC: 0.8692125678062439 \n",
      "0.0001\n",
      "Epoch: 6 Epoch sensitivity: 0.7305250763893127 Epoch loss: 1.9140356373995862\n",
      "Epoch AUROC: 0.8487619757652283 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [05:12<10:04, 46.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Epoch val_sensitivity: 0.7819548845291138 Epoch val_loss: 1.7989073991775513\n",
      "Epoch val AUROC: 0.8685665130615234 \n",
      "0.0001\n",
      "Epoch: 7 Epoch sensitivity: 0.7488452792167664 Epoch loss: 1.8785778963113133\n",
      "Epoch AUROC: 0.8598389029502869 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [06:01<09:27, 47.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Epoch val_sensitivity: 0.6290726661682129 Epoch val_loss: 1.7996689677238464\n",
      "Epoch val AUROC: 0.8813637495040894 \n",
      "0.0001\n",
      "Epoch: 8 Epoch sensitivity: 0.7530294060707092 Epoch loss: 1.871244703550998\n",
      "Epoch AUROC: 0.8633301258087158 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [06:51<08:50, 48.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Epoch val_sensitivity: 0.786967396736145 Epoch val_loss: 1.7597084045410156\n",
      "Epoch val AUROC: 0.880255401134491 \n",
      "0.0001\n",
      "Epoch: 9 Epoch sensitivity: 0.7547605037689209 Epoch loss: 1.8522512998645935\n",
      "Epoch AUROC: 0.8704930543899536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [07:40<08:02, 48.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Epoch val_sensitivity: 0.8220551609992981 Epoch val_loss: 1.832556664943695\n",
      "Epoch val AUROC: 0.8718766570091248 \n",
      "0.0001\n",
      "Epoch: 10 Epoch sensitivity: 0.7703404426574707 Epoch loss: 1.8372132357975293\n",
      "Epoch AUROC: 0.8732407689094543 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [08:26<07:08, 47.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Epoch val_sensitivity: 0.6867167949676514 Epoch val_loss: 1.7835447788238525\n",
      "Epoch val AUROC: 0.8755629062652588 \n",
      "0.0001\n",
      "Epoch: 11 Epoch sensitivity: 0.7512983083724976 Epoch loss: 1.8391802675419913\n",
      "Epoch AUROC: 0.8734744787216187 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [09:12<06:15, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Epoch val_sensitivity: 0.7593985199928284 Epoch val_loss: 1.7156906127929688\n",
      "Epoch val AUROC: 0.8926018476486206 \n",
      "0.0001\n",
      "Epoch: 12 Epoch sensitivity: 0.7772648334503174 Epoch loss: 1.8215274383634068\n",
      "Epoch AUROC: 0.8797026872634888 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [09:59<05:29, 47.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Epoch val_sensitivity: 0.6867167949676514 Epoch val_loss: 1.820328176021576\n",
      "Epoch val AUROC: 0.8591915369033813 \n",
      "0.0001\n",
      "Epoch: 13 Epoch sensitivity: 0.7599538564682007 Epoch loss: 1.826936329931688\n",
      "Epoch AUROC: 0.8774716258049011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [10:46<04:42, 47.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Epoch val_sensitivity: 0.8220551609992981 Epoch val_loss: 1.7766404151916504\n",
      "Epoch val AUROC: 0.8859429955482483 \n",
      "0.0001\n",
      "Epoch: 14 Epoch sensitivity: 0.78534334897995 Epoch loss: 1.811464066398643\n",
      "Epoch AUROC: 0.8824381232261658 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [11:28<03:47, 45.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Epoch val_sensitivity: 0.7493734359741211 Epoch val_loss: 1.8012176752090454\n",
      "Epoch val AUROC: 0.8666597008705139 \n",
      "1e-05\n",
      "Epoch: 15 Epoch sensitivity: 0.7830352187156677 Epoch loss: 1.7820880299301667\n",
      "Epoch AUROC: 0.8917785286903381 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [12:14<03:03, 45.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Epoch val_sensitivity: 0.7894737124443054 Epoch val_loss: 1.701718807220459\n",
      "Epoch val AUROC: 0.9001726508140564 \n",
      "1e-05\n",
      "Epoch: 16 Epoch sensitivity: 0.781304121017456 Epoch loss: 1.7577379741130112\n",
      "Epoch AUROC: 0.8998035788536072 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [12:59<02:16, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Epoch val_sensitivity: 0.7543859481811523 Epoch val_loss: 1.6857519745826721\n",
      "Epoch val AUROC: 0.9035030603408813 \n",
      "1e-05\n",
      "Epoch: 17 Epoch sensitivity: 0.7778418660163879 Epoch loss: 1.7525027979894414\n",
      "Epoch AUROC: 0.9026682376861572 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [13:41<01:28, 44.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Epoch val_sensitivity: 0.7894737124443054 Epoch val_loss: 1.683185875415802\n",
      "Epoch val AUROC: 0.904228687286377 \n",
      "1e-05\n",
      "Epoch: 18 Epoch sensitivity: 0.7743796706199646 Epoch loss: 1.7618597786521726\n",
      "Epoch AUROC: 0.9000216722488403 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [14:24<00:43, 44.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Epoch val_sensitivity: 0.7669172883033752 Epoch val_loss: 1.6799248456954956\n",
      "Epoch val AUROC: 0.9052553772926331 \n",
      "1e-05\n",
      "Epoch: 19 Epoch sensitivity: 0.789260983467102 Epoch loss: 1.7401209221146239\n",
      "Epoch AUROC: 0.9043723344802856 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [15:06<00:00, 45.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Epoch val_sensitivity: 0.8020049929618835 Epoch val_loss: 1.6773536205291748\n",
      "Epoch val AUROC: 0.9062967300415039 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## normal loop\n",
    "device = torch.device(\"cpu\")\n",
    "model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "#pos_weight=torch.full([1], 1.1\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], 8.65))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "recall = BinaryRecall(threshold=0.5)\n",
    "auroc = AUROC(task=\"binary\")\n",
    "roc = ROC('binary')\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "\n",
    "        preds = []\n",
    "        ground_truth = []\n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        preds_valid = []\n",
    "        ground_truth_valid = []\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "        \n",
    "                x, edge_index, edge_attr,y = batch[0:4]\n",
    "                \n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                #x = (x-mean)/std\n",
    "                y_hat = model(x=x.float(), edge_index=edge_index[0])\n",
    "                ##loss\n",
    "                loss = loss_fn(y_hat,y)\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                ## get preds & gorund truth\n",
    "                preds.append(y_hat.detach())\n",
    "                ground_truth.append(y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "\n",
    "        train_auroc = auroc(torch.stack(preds),torch.stack(ground_truth))\n",
    "        sensitivity = recall(torch.stack(preds),torch.stack(ground_truth))\n",
    "        print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                for time_valid, batch_valid in enumerate(val_loader):\n",
    "                        x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                        signal_samples = x.shape[3]\n",
    "                        x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                        x = x.squeeze()\n",
    "                        # mean = torch.mean(x,dim=0)\n",
    "                        # std = torch.std(x,dim=0)\n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                        #x = (x-mean)/std\n",
    "                        y_hat_val = model(x=x.float(), edge_index=edge_index[0])\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                        #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        preds_valid.append(y_hat_val.detach())\n",
    "                        ground_truth_valid.append(y_val)\n",
    "        scheduler.step(epoch_loss_valid)\n",
    "        val_auroc = auroc(torch.stack(preds_valid),torch.stack(ground_truth_valid))\n",
    "        val_sensitivity = recall(torch.stack(preds_valid),torch.stack(ground_truth_valid))\n",
    "        print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "        print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sensitivity: 0.75 Test loss: 1.0219301727582824\n",
      "Epoch val AUROC: 0.875 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds_test = []\n",
    "ground_truth_test = []\n",
    "loss_test = 0.0\n",
    "with torch.no_grad():\n",
    "        for time_test, batch_test in enumerate(loso_dataloader):\n",
    "                x, edge_index, edge_attr,y_test = batch_test[0:4]\n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                x = x.squeeze()\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                y_hat_test = model(x=x.float(), edge_index=edge_index[0])\n",
    "                loss_test = loss_fn(y_hat_test,y_test)\n",
    "\n",
    "                #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                loss_test+= loss_test\n",
    "                preds_test.append(y_hat_val.detach())\n",
    "                ground_truth_test.append(y_val)\n",
    "\n",
    "test_auroc = auroc(torch.stack(preds_test),torch.stack(ground_truth_test))\n",
    "test_sensitivity = recall(torch.stack(preds_test),torch.stack(ground_truth_test))\n",
    "print(f'Test sensitivity: {test_sensitivity}', f'Test loss: {loss_test.detach().numpy()/time_test+1}')\n",
    "print(f'Epoch val AUROC: {test_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc(torch.stack(preds_valid),torch.stack(ground_truth_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f505d06d450>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9011/1180606619.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )\n",
      "/tmp/ipykernel_9011/1180606619.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )\n",
      "/tmp/ipykernel_9011/1180606619.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39mequal(np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39marray(ground_truth) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m],np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39;49marray(preds) \u001b[39m>\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m] )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cc438b9c90976695678f0d6c20e4c06983b5710e6855b5b4390f60ecf93fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
