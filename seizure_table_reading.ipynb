{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from torch_geometric_temporal import  DynamicGraphTemporalSignal,StaticGraphTemporalSignal, temporal_signal_split, DynamicGraphTemporalSignalBatch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import scipy\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DynamicBatchSampler,DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN,  GConvGRU, A3TGCN, TGCN2, TGCN\n",
    "from torch_geometric_temporal.nn.attention import STConv\n",
    "from torchmetrics.classification import BinaryRecall, AUROC, ROC\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import timeit\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv,BatchNorm\n",
    "from sklearn.model_selection import KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think about using kwargs argument here to specify args for dataloader\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path :Path\n",
    "    event_tables_path : Path\n",
    "    plv_values_path : Path\n",
    "    loso_patient : str = None\n",
    "    sampling_f : int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    self_loops : bool = True\n",
    "    balance : bool = True\n",
    "    train_test_split:  float = None\n",
    "    \n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path {Path} -- Path to folder with dataset preprocessed into .npy files.\n",
    "        event_tables_path {Path} -- Path to folder with .csv files containing seizure events information for every patient.\n",
    "        loso_patient {str} -- Name of patient to be selected for LOSO valdiation, specified in format \"chb{patient_number}\"\",\n",
    "        eg. \"chb16\". (default: {None}).\n",
    "        samplin_f {int} -- Sampling frequency of the loaded eeg data. (default: {256}).\n",
    "        seizure_lookback {int} -- Time horizon to sample pre-seizure data (length of period before seizure) in seconds. \n",
    "        (default: {600}).\n",
    "        sample_timestep {int} -- Amounts of seconds analyzed in a single sample. (default: {5}).\n",
    "        overlap {int} -- Amount of seconds overlap between samples. (default: {0}).\n",
    "        self_loops {bool} -- Wheather to add self loops to nodes of the graph. (default: {True}).\n",
    "        shuffle {bool} --  Wheather to shuffle training samples.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def _get_event_tables(self,patient_name : str) -> tuple[dict,dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [os.path.join(self.event_tables_path,ev_table)\n",
    "        for ev_table in event_table_list if patient_name in ev_table]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[0] ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict('index')\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict('index')\n",
    "        return start_events_dict,stop_events_dict\n",
    "        \n",
    "    def _get_recording_events(self,events_dict,recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict\"\"\"\n",
    "        recording_list = list(events_dict[recording+'.edf'].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "\n",
    "    def _get_graph(self,n_nodes: int) -> nx.Graph :\n",
    "        \"\"\"Creates Networx fully connected graph with self loops\"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node,node]for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "    \n",
    "    def _get_edge_weights_recording(self,plv_values: np.ndarray) ->np.ndarray:\n",
    "        \"\"\"Method that takes plv values for given recording and assigns them \n",
    "        as edge attributes to a fc graph.\"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start,e_end = edge\n",
    "            garph_dict[edge] = {'plv':plv_values[e_start,e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "    \n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(),axis=0)\n",
    "        edges_per_sample_train = np.repeat(edges,repeats =self._features.shape[0],axis=0)\n",
    "        self._edges = torch.tensor(edges_per_sample_train)\n",
    "        if self.loso_patient is not None:\n",
    "            edges_per_sample_val = np.repeat(edges,repeats =self._val_features.shape[0],axis=0)\n",
    "            self._val_edges = torch.tensor(edges_per_sample_val)\n",
    "       \n",
    "    def _array_to_tensor(self):\n",
    "        \"\"\"Method converting features, edges and weights to torch.tensors\"\"\"\n",
    "        self._features = torch.tensor(self._features)\n",
    "        self._labels = torch.tensor(self._labels)\n",
    "        self._time_labels = torch.tensor(self._time_labels)\n",
    "        self._edge_weights = torch.tensor(self._edge_weights)\n",
    "     \n",
    "    \n",
    "    def _val_array_to_tensor(self):\n",
    "        self._val_features = torch.tensor(self._val_features)\n",
    "        self._val_labels = torch.tensor(self._val_labels)\n",
    "        self._val_time_labels = torch.tensor(self._val_time_labels)\n",
    "        self._val_edge_weights = torch.tensor(self._val_edge_weights)\n",
    "     \n",
    "        \n",
    "    def _get_labels_count(self):\n",
    "        labels,counts = np.unique(self._labels,return_counts=True)\n",
    "        self._label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._label_counts[int(label)] = counts[n]\n",
    "        \n",
    "    def _get_val_labels_count(self):\n",
    "        labels,counts = np.unique(self._val_labels,return_counts=True)\n",
    "        self._val_label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._val_label_counts[int(label)] = counts[n]\n",
    "        \n",
    "    def _balance_classes(self):\n",
    "        negative_label = self._label_counts[0]\n",
    "        positive_label = self._label_counts[1]\n",
    "    \n",
    "        imbalance = negative_label - positive_label\n",
    "        negative_indices = np.where(self._labels == 0)[0]\n",
    "        indices_to_discard = np.random.choice(negative_indices,size = imbalance,replace=False)\n",
    "\n",
    "        self._features = np.delete(self._features,obj=indices_to_discard,axis=0)\n",
    "        self._labels = np.delete(self._labels,obj=indices_to_discard,axis=0)\n",
    "        self._time_labels = np.delete(self._time_labels,obj=indices_to_discard,axis=0)\n",
    "        self._edge_weights = np.delete(self._edge_weights,obj=indices_to_discard,axis=0)\n",
    "\n",
    "        \n",
    "    def _get_labels_features_edge_weights(self):\n",
    "        \"\"\"Prepare features, labels, time labels and edge wieghts for training and \n",
    "        optionally validation data.\"\"\"\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        for patient in patient_list: # iterate over patient names\n",
    "            event_tables = self._get_event_tables(patient) # extract start and stop of seizure for patient \n",
    "            patient_path = os.path.join(self.npy_dataset_path,patient)\n",
    "            recording_list = os.listdir(patient_path)\n",
    "            for record in recording_list: # iterate over recordings for a patient\n",
    "                recording_path = os.path.join(patient_path,record)\n",
    "                record_id = record.split('.npy')[0] #  get record id\n",
    "                start_event_tables = self._get_recording_events(event_tables[0],record_id) # get start events\n",
    "                stop_event_tables = self._get_recording_events(event_tables[1],record_id) # get stop events\n",
    "                data_array = np.load(recording_path) # load the recording\n",
    "\n",
    "                plv_edge_weights = np.expand_dims(\n",
    "                    self._get_edge_weights_recording(\n",
    "                        np.load(os.path.join(self.plv_values_path,patient,record))\n",
    "          \n",
    "                ),\n",
    "                axis = 0\n",
    "                )\n",
    "\n",
    "                ##TODO add a gateway to reject seizure periods shorter than lookback\n",
    "                # extract timeseries and labels from the array\n",
    "                features,labels,time_labels = utils.extract_training_data_and_labels(\n",
    "                    data_array,\n",
    "                    start_event_tables,\n",
    "                    stop_event_tables,\n",
    "                    fs = self.sampling_f,\n",
    "                    seizure_lookback = self.seizure_lookback,\n",
    "                    sample_timestep = self.sample_timestep,\n",
    "                    inter_overlap = self.inter_overlap,\n",
    "                    ictal_overlap = self.ictal_overlap\n",
    "                )\n",
    "                \n",
    "                if  features is None:\n",
    "                    continue\n",
    "                time_labels = np.expand_dims(time_labels.astype(np.int32),1)\n",
    "                labels = labels.reshape((labels.shape[0],1)).astype(np.float32)\n",
    "            \n",
    "                if patient == self.loso_patient:\n",
    "                    try:\n",
    "                        self._val_features = np.concatenate((self._val_features, features))\n",
    "                        self._val_labels = np.concatenate((self._val_labels, labels))\n",
    "                        self._val_time_labels = np.concatenate((self._val_time_labels , time_labels))\n",
    "                        self._val_edge_weights = np.concatenate((\n",
    "                            self._val_edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                            ))\n",
    "                    except:\n",
    "                        self._val_features = features\n",
    "                        self._val_labels = labels\n",
    "                        self._val_time_labels = time_labels\n",
    "                        self._val_edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        self._features = np.concatenate((self._features, features))\n",
    "                        self._labels = np.concatenate((self._labels, labels))\n",
    "                        self._time_labels = np.concatenate((self._time_labels , time_labels))\n",
    "                        self._edge_weights = np.concatenate((\n",
    "                            self._edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                        ))\n",
    "                       \n",
    "                    except:\n",
    "                        print(\"Creating initial attributes\")\n",
    "                        self._features = features\n",
    "                        self._labels = labels\n",
    "                        self._time_labels = time_labels\n",
    "                        self._edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                \n",
    "        \n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self) -> DynamicGraphTemporalSignal:\n",
    "\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes \n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of \n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        \n",
    "        self._get_labels_features_edge_weights()\n",
    "        if self.balance:\n",
    "            self._get_labels_count()\n",
    "            self._balance_classes()\n",
    "        self._get_edges()\n",
    "        self._get_labels_count()\n",
    "        \n",
    "        self._array_to_tensor()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "        self._features, self._edges, self._edge_weights, self._labels,  self._time_labels\n",
    "        )\n",
    "        if self.train_test_split is not None:\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                train_dataset,[1-self.train_test_split,self.train_test_split]\n",
    "            )\n",
    "            \n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            \n",
    "            val_dataloader = torch.utils.data.DataLoader(\n",
    "                val_dataset, batch_size = 16,shuffle = False,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            loaders = [train_dataloader,val_dataloader]\n",
    "        else:\n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            loaders = [train_dataloader]\n",
    "        if self.loso_patient:\n",
    "            self._get_val_labels_count()\n",
    "            self._val_array_to_tensor()\n",
    "            loso_dataset = torch.utils.data.TensorDataset(\n",
    "                self._val_features, self._val_edges, self._val_edge_weights, self._val_labels, self._val_time_labels\n",
    "            )\n",
    "            loso_dataloader = torch.utils.data.DataLoader(\n",
    "                loso_dataset, batch_size = 16,shuffle = False,pin_memory = True,num_workers=2, prefetch_factor = 30,\n",
    "                drop_last=True\n",
    "            )\n",
    "            return (*loaders,loso_dataloader)\n",
    "\n",
    "        return (*loaders,)\n",
    "        \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = TGCN(timestep*sfreq,32, add_self_loops=True,improved=True)\n",
    "        self.recurrent_2 = TGCN(32,64,add_self_loops=True,improved=True)\n",
    "        self.recurrent_3 = TGCN(64,128,add_self_loops=True,improved=True)\n",
    "        self.fc1 = torch.nn.Linear(n_nodes*128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index,edge_weight):\n",
    "        x = torch.squeeze(x)\n",
    "  \n",
    "       # x = F.normalize(x,dim=1)\n",
    "        h = self.recurrent_1(x, edge_index=edge_index, edge_weight = edge_weight)\n",
    "        \n",
    "      #  h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_2(h, edge_index,edge_weight)\n",
    "      #  h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_3(h, edge_index,edge_weight)\n",
    "      #  h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.flatten(h)\n",
    "        #print(h.shape)\n",
    "        #h = global_mean_pool(h,torch.zeros(self.n_nodes,dtype=torch.int64)).squeeze()\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Creating initial attributes\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 1\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'NoneType'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 1\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'NoneType'>\n",
      "Len Seizure features: 1\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 1\n",
      "Len Seizure features: 1\n",
      "Len Seizure features: 1\n",
      "<class 'NoneType'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 1\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'NoneType'>\n",
      "Len Seizure features: 1\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'NoneType'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "Len Seizure features: 4\n",
      "Entering here\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n",
      "Len Seizure features: 4\n",
      "Creating initial features for this recording at iteration 0\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "TIMESTEP = 10\n",
    "INTER_OVERLAP = 0\n",
    "ICTAL_OVERLAP = 0\n",
    "SFREQ = 256\n",
    "\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('data/npy_data'),\n",
    "    event_tables_path=Path('data/event_tables'),\n",
    "    plv_values_path=Path('data/plv_arrays'),\n",
    "    loso_patient='chb16',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    inter_overlap=INTER_OVERLAP,\n",
    "    ictal_overlap=ICTAL_OVERLAP,\n",
    "    self_loops=False,\n",
    "    balance=False,\n",
    "    train_test_split=0.2\n",
    "    \n",
    "    )\n",
    "train_loader,val_loader,loso_dataloader=dataloader.get_dataset()\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_accuracy(y_true, y_prob):\n",
    "    \"\"\"Binary accuracy calculation\"\"\"\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_prob = np.where(y_prob <= 0.0, 0, y_prob)\n",
    "    y_prob = np.where(y_prob > 0.0, 1, y_prob)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_prob)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "## 1. Implement kfold cross validation .get_dataset() will return also a kfold cross validation split?\n",
    "## 2. Think of time series augmentation techniques\n",
    "## 3. Try to add a feature balancing between ictal and iterictal periods.\n",
    "## 4.Try to run the algorithm on list of patients shown in the articles.\n",
    "## 5. !Why there is no patient 15 and 20 in the dataset? !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kfold loop\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "# mean = dataloader._features.squeeze().mean(dim=0)\n",
    "# std = dataloader._features.squeeze().std(dim=0)\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_loader)))):\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "        #pos_weight=torch.full([1], 1.1\n",
    "        loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], 8.64))\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "        recall = BinaryRecall(threshold=0.5)\n",
    "        auroc = AUROC(task=\"binary\")\n",
    "        roc = ROC('binary')\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=train_sampler,drop_last = True)\n",
    "        test_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=test_sampler,drop_last = True)\n",
    "        print(f'Fold {fold+1}')\n",
    "        for epoch in tqdm(range(30)):\n",
    "\n",
    "                preds = []\n",
    "                ground_truth = []\n",
    "                epoch_loss = 0.0\n",
    "                epoch_loss_valid = 0.0\n",
    "                preds_valid = []\n",
    "                ground_truth_valid = []\n",
    "                model.train()\n",
    "                sample_counter = 0\n",
    "                batch_counter = 0\n",
    "                for time, batch in enumerate(train_loader_fold):\n",
    "                        x, edge_index, edge_attr,y = batch[0:4]\n",
    "                        x = x.squeeze()\n",
    "                        #mean = torch.mean(x,dim=0)\n",
    "                        #std = torch.std(x,dim=0)\n",
    "                        x = (x-mean)/std\n",
    "                        y_hat = model(x=x.float(), edge_index=edge_index[0])\n",
    "                        \n",
    "                        ##loss\n",
    "                        loss = loss_fn(y_hat,y)\n",
    "                        \n",
    "                        epoch_loss += loss\n",
    "                        ## get preds & gorund truth\n",
    "                        preds.append(y_hat.detach().numpy())\n",
    "                        ground_truth.append(y.numpy())\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                ## calculate acc\n",
    "\n",
    "                train_auroc = auroc(torch.FloatTensor(preds),torch.FloatTensor(ground_truth))\n",
    "                sensitivity = recall(torch.FloatTensor(preds),torch.FloatTensor(ground_truth))\n",
    "                print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "                print(f'Epoch AUROC: {train_auroc} ')\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                        for time_valid, batch_valid in enumerate(test_loader_fold):\n",
    "                                x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                                #print(y_val)\n",
    "                                x = x.squeeze()\n",
    "                                #mean = torch.mean(x,dim=0)\n",
    "                                #std = torch.std(x,dim=0)\n",
    "                                x = (x-mean)/std\n",
    "                                y_hat_val = model(x=x.float(), edge_index=edge_index[0])\n",
    "                                loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                                #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                                epoch_loss_valid += loss_valid\n",
    "                                preds_valid.append(y_hat_val.detach().numpy())\n",
    "                                ground_truth_valid.append(y_val.numpy())\n",
    "\n",
    "                val_auroc = auroc(torch.FloatTensor(preds_valid),torch.FloatTensor(ground_truth_valid))\n",
    "                val_sensitivity = recall(torch.FloatTensor(preds_valid),torch.FloatTensor(ground_truth_valid))\n",
    "                print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "                print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Epoch: 0 Epoch sensitivity: 0.07099143415689468 Epoch loss: 2.203608827007828\n",
      "Epoch AUROC: 0.6201717853546143 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:14<23:35, 74.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Epoch val_sensitivity: 0.6634615659713745 Epoch val_loss: 2.149944850376674\n",
      "Epoch val AUROC: 0.7853262424468994 \n",
      "0.0001\n",
      "Epoch: 1 Epoch sensitivity: 0.5544675588607788 Epoch loss: 2.1143140322593075\n",
      "Epoch AUROC: 0.720425546169281 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [02:33<23:11, 77.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Epoch val_sensitivity: 0.7115384340286255 Epoch val_loss: 2.0353512234157987\n",
      "Epoch val AUROC: 0.8009394407272339 \n",
      "0.0001\n",
      "Epoch: 2 Epoch sensitivity: 0.665036678314209 Epoch loss: 2.0462087898329635\n",
      "Epoch AUROC: 0.7677388787269592 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [03:52<22:08, 78.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Epoch val_sensitivity: 0.754807710647583 Epoch val_loss: 1.9467063782707092\n",
      "Epoch val AUROC: 0.8257949352264404 \n",
      "0.0001\n",
      "Epoch: 3 Epoch sensitivity: 0.6719706058502197 Epoch loss: 2.007756220048231\n",
      "Epoch AUROC: 0.7935322523117065 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [05:12<20:57, 78.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Epoch val_sensitivity: 0.8990384340286255 Epoch val_loss: 1.9654719034830728\n",
      "Epoch val AUROC: 0.8395722508430481 \n",
      "0.0001\n",
      "Epoch: 4 Epoch sensitivity: 0.702570378780365 Epoch loss: 1.990576508952786\n",
      "Epoch AUROC: 0.8030022978782654 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [06:32<19:48, 79.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Epoch val_sensitivity: 0.8653846383094788 Epoch val_loss: 2.0219748360770087\n",
      "Epoch val AUROC: 0.8186941146850586 \n",
      "0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [07:19<21:58, 87.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m         ground_truth\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m     50\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     52\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[39m## calculate acc\u001b[39;00m\n",
      "File \u001b[0;32m~/code/sano/sano_eeg/sano_env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/sano/sano_eeg/sano_env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## normal loop\n",
    "device = torch.device(\"cpu\")\n",
    "model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "#pos_weight=torch.full([1], 1.1\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], 8.65))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "recall = BinaryRecall(threshold=0.5)\n",
    "auroc = AUROC(task=\"binary\")\n",
    "roc = ROC('binary')\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "\n",
    "        preds = []\n",
    "        ground_truth = []\n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        preds_valid = []\n",
    "        ground_truth_valid = []\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "        \n",
    "                x, edge_index, edge_attr,y = batch[0:4]\n",
    "                \n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                #x = (x-mean)/std\n",
    "           \n",
    "                y_hat =torch.stack(\n",
    "                        [model(x=x[n].float(), edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                         for n in range(x.shape[0])])\n",
    "                ##loss\n",
    "        \n",
    "                loss = loss_fn(y_hat,y)\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                ## get preds & gorund truth\n",
    "                preds.append(y_hat.detach())\n",
    "                ground_truth.append(y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "\n",
    "        train_auroc = auroc(torch.stack(preds),torch.stack(ground_truth))\n",
    "        sensitivity = recall(torch.stack(preds),torch.stack(ground_truth))\n",
    "        print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                for time_valid, batch_valid in enumerate(val_loader):\n",
    "                        x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                        signal_samples = x.shape[3]\n",
    "                        x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                        x = x.squeeze()\n",
    "                        # mean = torch.mean(x,dim=0)\n",
    "                        # std = torch.std(x,dim=0)\n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                        #x = (x-mean)/std\n",
    "                        y_hat_val = torch.stack(\n",
    "                                [model(x=x[n].float(), edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                                 for n in range(x.shape[0])])\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                        #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        preds_valid.append(y_hat_val.detach())\n",
    "                        ground_truth_valid.append(y_val)\n",
    "        scheduler.step(epoch_loss_valid)\n",
    "        val_auroc = auroc(torch.stack(preds_valid),torch.stack(ground_truth_valid))\n",
    "        val_sensitivity = recall(torch.stack(preds_valid),torch.stack(ground_truth_valid))\n",
    "        print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "        print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds_test = []\n",
    "ground_truth_test = []\n",
    "loss_test = 0.0\n",
    "with torch.no_grad():\n",
    "        for time_test, batch_test in enumerate(loso_dataloader):\n",
    "                x, edge_index, edge_attr,y_test = batch_test[0:4]\n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(torch.fft.fft(x))\n",
    "                x = x.squeeze()\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                y_hat_test = torch.stack(\n",
    "                                [model(x=x[n].float(), edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                                 for n in range(x.shape[0])])\n",
    "                loss_test = loss_fn(y_hat_test,y_test)\n",
    "\n",
    "                #loss_valid = sigmoid_focal_loss(y_hat,snapshot.y,alpha=0.8,gamma=1).squeeze()\n",
    "\n",
    "                loss_test+= loss_test\n",
    "                preds_test.append(y_hat_val.detach())\n",
    "                ground_truth_test.append(y_val)\n",
    "\n",
    "test_auroc = auroc(torch.stack(preds_test),torch.stack(ground_truth_test))\n",
    "test_sensitivity = recall(torch.stack(preds_test),torch.stack(ground_truth_test))\n",
    "print(f'Test sensitivity: {test_sensitivity}', f'Test loss: {loss_test.detach().numpy()/time_test+1}')\n",
    "print(f'Epoch val AUROC: {test_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc(torch.stack(preds_valid),torch.stack(ground_truth_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cc438b9c90976695678f0d6c20e4c06983b5710e6855b5b4390f60ecf93fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
