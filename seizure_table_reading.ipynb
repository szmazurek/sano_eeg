{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from torch_geometric_temporal import  DynamicGraphTemporalSignal,StaticGraphTemporalSignal, temporal_signal_split, DynamicGraphTemporalSignalBatch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import scipy\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DynamicBatchSampler,DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN,  GConvGRU, A3TGCN, TGCN2, TGCN, A3TGCN2\n",
    "from torch_geometric_temporal.nn.attention import STConv\n",
    "from torchmetrics.classification import BinaryRecall,BinarySpecificity, AUROC, ROC\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import timeit\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv,BatchNorm,GATv2Conv\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import mne_features\n",
    "import torchaudio\n",
    "import random\n",
    "from mne_features.univariate import compute_kurtosis, compute_hjorth_complexity, compute_hjorth_mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think about using kwargs argument here to specify args for dataloader\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path :Path\n",
    "    event_tables_path : Path\n",
    "    plv_values_path : Path\n",
    "    loso_patient : str = None\n",
    "    sampling_f : int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    self_loops : bool = True\n",
    "    balance : bool = True\n",
    "    train_test_split:  float = None\n",
    "    fft : bool = False\n",
    "    hjorth : bool = False\n",
    "    downsample : int = None\n",
    "    buffer_time : int = 15\n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path {Path} -- Path to folder with dataset preprocessed into .npy files.\n",
    "        event_tables_path {Path} -- Path to folder with .csv files containing seizure events information for every patient.\n",
    "        loso_patient {str} -- Name of patient to be selected for LOSO valdiation, specified in format \"chb{patient_number}\"\",\n",
    "        eg. \"chb16\". (default: {None}).\n",
    "        samplin_f {int} -- Sampling frequency of the loaded eeg data. (default: {256}).\n",
    "        seizure_lookback {int} -- Time horizon to sample pre-seizure data (length of period before seizure) in seconds. \n",
    "        (default: {600}).\n",
    "        sample_timestep {int} -- Amounts of seconds analyzed in a single sample. (default: {5}).\n",
    "        overlap {int} -- Amount of seconds overlap between samples. (default: {0}).\n",
    "        self_loops {bool} -- Wheather to add self loops to nodes of the graph. (default: {True}).\n",
    "        shuffle {bool} --  Wheather to shuffle training samples.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def _get_event_tables(self,patient_name : str) -> tuple[dict,dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [os.path.join(self.event_tables_path,ev_table)\n",
    "        for ev_table in event_table_list if patient_name in ev_table]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[0] ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict('index')\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict('index')\n",
    "        return start_events_dict,stop_events_dict\n",
    "        \n",
    "    def _get_recording_events(self,events_dict,recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict\"\"\"\n",
    "        recording_list = list(events_dict[recording+'.edf'].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "\n",
    "    def _get_graph(self,n_nodes: int) -> nx.Graph :\n",
    "        \"\"\"Creates Networx fully connected graph with self loops\"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node,node]for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "    \n",
    "    def _get_edge_weights_recording(self,plv_values: np.ndarray) ->np.ndarray:\n",
    "        \"\"\"Method that takes plv values for given recording and assigns them \n",
    "        as edge attributes to a fc graph.\"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start,e_end = edge\n",
    "            garph_dict[edge] = {'plv':plv_values[e_start,e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "    \n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(),axis=0)\n",
    "        edges_per_sample_train = np.repeat(edges,repeats =self._features.shape[0],axis=0)\n",
    "        self._edges = torch.tensor(edges_per_sample_train)\n",
    "        if self.loso_patient is not None:\n",
    "            edges_per_sample_val = np.repeat(edges,repeats =self._val_features.shape[0],axis=0)\n",
    "            self._val_edges = torch.tensor(edges_per_sample_val)\n",
    "       \n",
    "    def _array_to_tensor(self):\n",
    "        \"\"\"Method converting features, edges and weights to torch.tensors\"\"\"\n",
    "        self._features = torch.tensor(self._features,dtype=torch.float32)\n",
    "        self._labels = torch.tensor(self._labels)\n",
    "        self._time_labels = torch.tensor(self._time_labels)\n",
    "        self._edge_weights = torch.tensor(self._edge_weights)\n",
    "     \n",
    "    \n",
    "    def _val_array_to_tensor(self):\n",
    "        self._val_features = torch.tensor(self._val_features,dtype=torch.float32)\n",
    "        self._val_labels = torch.tensor(self._val_labels)\n",
    "        self._val_time_labels = torch.tensor(self._val_time_labels)\n",
    "        self._val_edge_weights = torch.tensor(self._val_edge_weights)\n",
    "     \n",
    "        \n",
    "    def _get_labels_count(self):\n",
    "        labels,counts = np.unique(self._labels,return_counts=True)\n",
    "        self._label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._label_counts[int(label)] = counts[n]\n",
    "        \n",
    "    def _get_val_labels_count(self):\n",
    "        labels,counts = np.unique(self._val_labels,return_counts=True)\n",
    "        self._val_label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._val_label_counts[int(label)] = counts[n]\n",
    "    \n",
    "    def _perform_features_train_fft(self):\n",
    "        self._features = torch.fft.fft(self._features)\n",
    "    def _perform_features_val_fft(self):  \n",
    "        self._val_features = torch.fft.fft(self._val_features)\n",
    "    \n",
    "    def _downsample_features_train(self):\n",
    "        resampler = torchaudio.transforms.Resample(self.sampling_f,self.downsample)\n",
    "        self._features = resampler(self._features)\n",
    "        \n",
    "    def _downsample_features_val(self):\n",
    "        resampler = torchaudio.transforms.Resample(self.sampling_f,self.downsample)\n",
    "        self._val_features = resampler(self._val_features)\n",
    "    \n",
    "    def _calculate_hjorth_features_train(self):\n",
    "        \n",
    "        new_features = [np.concatenate(\n",
    "            [\n",
    "                compute_hjorth_mobility(feature),\n",
    "                compute_hjorth_complexity(feature),\n",
    "            ], axis=1\n",
    "        ) for feature in self._features\n",
    "        ]\n",
    "        self._features = np.array(new_features)\n",
    "        \n",
    "    def _calculate_hjorth_features_val(self):\n",
    "        \n",
    "        new_features = [np.concatenate(\n",
    "            [\n",
    "                compute_hjorth_mobility(feature),\n",
    "                compute_hjorth_complexity(feature),\n",
    "            ], axis=1\n",
    "        ) for feature in self._val_features\n",
    "        ]\n",
    "        self._val_features = np.array(new_features)\n",
    "        \n",
    "    def _balance_classes(self):\n",
    "        negative_label = self._label_counts[0]\n",
    "        positive_label = self._label_counts[1]\n",
    "    \n",
    "        imbalance = negative_label - positive_label\n",
    "        negative_indices = np.where(self._labels == 0)[0]\n",
    "        indices_to_discard = np.random.choice(negative_indices,size = imbalance,replace=False)\n",
    "\n",
    "        self._features = np.delete(self._features,obj=indices_to_discard,axis=0)\n",
    "        self._labels = np.delete(self._labels,obj=indices_to_discard,axis=0)\n",
    "        self._time_labels = np.delete(self._time_labels,obj=indices_to_discard,axis=0)\n",
    "        self._edge_weights = np.delete(self._edge_weights,obj=indices_to_discard,axis=0)\n",
    "\n",
    "        \n",
    "    def _get_labels_features_edge_weights(self):\n",
    "        \"\"\"Prepare features, labels, time labels and edge wieghts for training and \n",
    "        optionally validation data.\"\"\"\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        for patient in patient_list: # iterate over patient names\n",
    "            event_tables = self._get_event_tables(patient) # extract start and stop of seizure for patient \n",
    "            patient_path = os.path.join(self.npy_dataset_path,patient)\n",
    "            recording_list = os.listdir(patient_path)\n",
    "            for record in recording_list: # iterate over recordings for a patient\n",
    "                recording_path = os.path.join(patient_path,record)\n",
    "                record_id = record.split('.npy')[0] #  get record id\n",
    "                start_event_tables = self._get_recording_events(event_tables[0],record_id) # get start events\n",
    "                stop_event_tables = self._get_recording_events(event_tables[1],record_id) # get stop events\n",
    "                data_array = np.load(recording_path) # load the recording\n",
    "\n",
    "                plv_edge_weights = np.expand_dims(\n",
    "                    self._get_edge_weights_recording(\n",
    "                        np.load(os.path.join(self.plv_values_path,patient,record))\n",
    "          \n",
    "                ),\n",
    "                axis = 0\n",
    "                )\n",
    "\n",
    "                ##TODO add a gateway to reject seizure periods shorter than lookback\n",
    "                # extract timeseries and labels from the array\n",
    "                features,labels,time_labels = utils.extract_training_data_and_labels(\n",
    "                    data_array,\n",
    "                    start_event_tables,\n",
    "                    stop_event_tables,\n",
    "                    fs = self.sampling_f,\n",
    "                    seizure_lookback = self.seizure_lookback,\n",
    "                    sample_timestep = self.sample_timestep,\n",
    "                    inter_overlap = self.inter_overlap,\n",
    "                    ictal_overlap = self.ictal_overlap,\n",
    "                    buffer_time=self.buffer_time\n",
    "                )\n",
    "                \n",
    "                if  features is None:\n",
    "                    continue\n",
    "                time_labels = np.expand_dims(time_labels.astype(np.int32),1)\n",
    "                labels = labels.reshape((labels.shape[0],1)).astype(np.float32)\n",
    "                \"\"\"SCALING FEATURES INTO uV!!!\"\"\"\n",
    "               # features = features*(10**6)\n",
    "                \"\"\"SCALING FEATURES INTO uV!!!\"\"\"\n",
    "                if patient == self.loso_patient:\n",
    "                    try:\n",
    "                        self._val_features = np.concatenate((self._val_features, features))\n",
    "                        self._val_labels = np.concatenate((self._val_labels, labels))\n",
    "                        self._val_time_labels = np.concatenate((self._val_time_labels , time_labels))\n",
    "                        self._val_edge_weights = np.concatenate((\n",
    "                            self._val_edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                            ))\n",
    "                    except:\n",
    "                        self._val_features = features\n",
    "                        self._val_labels = labels\n",
    "                        self._val_time_labels = time_labels\n",
    "                        self._val_edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        self._features = np.concatenate((self._features, features))\n",
    "                        self._labels = np.concatenate((self._labels, labels))\n",
    "                        self._time_labels = np.concatenate((self._time_labels , time_labels))\n",
    "                        self._edge_weights = np.concatenate((\n",
    "                            self._edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                        ))\n",
    "                       \n",
    "                    except:\n",
    "                        print(\"Creating initial attributes\")\n",
    "                        self._features = features\n",
    "                        self._labels = labels\n",
    "                        self._time_labels = time_labels\n",
    "                        self._edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                \n",
    "        \n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self) -> DynamicGraphTemporalSignal:\n",
    "\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes \n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of \n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        \n",
    "        self._get_labels_features_edge_weights()\n",
    "        self.train_features_min_max = [self._features.min(),self._features.max()]\n",
    "        if self.balance:\n",
    "            self._get_labels_count()\n",
    "            self._balance_classes()\n",
    "        self._get_edges()\n",
    "        self._get_labels_count()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.hjorth:\n",
    "            self._calculate_hjorth_features_train()\n",
    "        self._array_to_tensor()\n",
    "        if self.downsample:\n",
    "            self._downsample_features_train()\n",
    "        if self.fft:\n",
    "            self._perform_features_train_fft()\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "        self._features, self._edges, self._edge_weights, self._labels,  self._time_labels\n",
    "        )\n",
    "        if self.train_test_split is not None:\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                train_dataset,[1-self.train_test_split,self.train_test_split],\n",
    "                generator=torch.Generator().manual_seed(42)\n",
    "            )\n",
    "            \n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 4,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "            val_dataloader = torch.utils.data.DataLoader(\n",
    "                val_dataset, batch_size = 16,shuffle = False,num_workers=2,pin_memory = True, prefetch_factor = 4,\n",
    "                drop_last=False\n",
    "            )\n",
    "            loaders = [train_dataloader,val_dataloader]\n",
    "        else:\n",
    "            train_dataloader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size = 16,shuffle = True,num_workers=2,pin_memory = True, prefetch_factor = 4,\n",
    "                drop_last=False\n",
    "            )\n",
    "            loaders = [train_dataloader]\n",
    "        if self.loso_patient:\n",
    "            self.val_features_min_max = [self._val_features.min(),self._val_features.max()]\n",
    "            self._get_val_labels_count()\n",
    "            self._calculate_hjorth_features_val()\n",
    "            self._val_array_to_tensor()\n",
    "            if self.downsample:\n",
    "                self._downsample_features_val()\n",
    "            if self.fft:\n",
    "                self._perform_features_val_fft()\n",
    "            loso_dataset = torch.utils.data.TensorDataset(\n",
    "                self._val_features, self._val_edges, self._val_edge_weights, self._val_labels, self._val_time_labels\n",
    "            )\n",
    "            loso_dataloader = torch.utils.data.DataLoader(\n",
    "                loso_dataset, batch_size = 16,shuffle = False,pin_memory = True,num_workers=2, prefetch_factor = 4,\n",
    "                drop_last=False\n",
    "            )\n",
    "            return (*loaders,loso_dataloader)\n",
    "\n",
    "        return (*loaders,)\n",
    "        \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = GATv2Conv(sfreq*timestep,32,heads=6, add_self_loops=True,improved=False,edge_dim=1)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(3456, 1024)\n",
    "        self.fc2 = torch.nn.Linear(1024, 512)\n",
    "        self.fc3 = torch.nn.Linear(512, 128)\n",
    "        self.fc4 = torch.nn.Linear(128, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index,edge_weight):\n",
    "        x = torch.squeeze(x)\n",
    "        h = self.recurrent_1(x, edge_index=edge_index, edge_attr = edge_weight)\n",
    "     \n",
    "        h = torch.nn.BatchNorm1d(192)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.flatten(h)\n",
    "      \n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = GCNConv(sfreq*timestep,32, add_self_loops=True,improved=False)\n",
    "        self.recurrent_2 = GCNConv(32,64,add_self_loops=True,improved=False)\n",
    "        self.recurrent_3 = GCNConv(64,128,add_self_loops=True,improved=False)\n",
    "        self.fc1 = torch.nn.Linear(n_nodes*128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index,edge_weight):\n",
    "        x = torch.squeeze(x)\n",
    "        h = self.recurrent_1(x, edge_index=edge_index, edge_weight = edge_weight)\n",
    "        h = torch.nn.BatchNorm1d(32)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_2(h, edge_index,edge_weight)\n",
    "        h = torch.nn.BatchNorm1d(64)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_3(h, edge_index,edge_weight)\n",
    "        h = torch.nn.BatchNorm1d(128)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.flatten(h)\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial attributes\n"
     ]
    }
   ],
   "source": [
    "TIMESTEP = 10\n",
    "INTER_OVERLAP = 0\n",
    "ICTAL_OVERLAP = 5\n",
    "SFREQ = 256\n",
    "\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('data/npy_data'),\n",
    "    event_tables_path=Path('data/event_tables'),\n",
    "    plv_values_path=Path('data/plv_arrays'),\n",
    "    loso_patient='chb16',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    inter_overlap=INTER_OVERLAP,\n",
    "    ictal_overlap=ICTAL_OVERLAP,\n",
    "    self_loops=False,\n",
    "    balance=False,\n",
    "    train_test_split=0.2,\n",
    "    fft=True,\n",
    "    hjorth=False,\n",
    "    downsample=60\n",
    "    \n",
    "    )\n",
    "train_loader,val_dataloader,loso_loader=dataloader.get_dataset()\n",
    "alpha = list(dataloader._label_counts.values())[0]/list(dataloader._label_counts.values())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10043, 18, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader._features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDumbNLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDumbNLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(252,128)\n",
    "        self.fc2 = torch.nn.Linear(128,1)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self,x):\n",
    "        h = torch.tensor(mne_features.univariate.compute_teager_kaiser_energy(x)).float()\n",
    "     \n",
    "        h = self.fc1(h)\n",
    "        h = F.relu(h,inplace=True)\n",
    "        h = self.dropout(h)\n",
    "        out = self.fc2(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "## 2. Think of time series augmentation techniques\n",
    "\n",
    "## 4.Try to run the algorithm on list of patients shown in the articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kfold loop\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "# mean = dataloader._features.squeeze().mean(dim=0)\n",
    "# std = dataloader._features.squeeze().std(dim=0)\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_loader)))):\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "        #pos_weight=torch.full([1], 1.1\n",
    "        loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], alpha))\n",
    "       \n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "        recall = BinaryRecall(threshold=0.5)\n",
    "        auroc = AUROC(task=\"binary\")\n",
    "        \n",
    "        roc = ROC('binary')\n",
    "        model.train()\n",
    "       # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=train_sampler,drop_last = False)\n",
    "        test_loader_fold = DataLoader(train_loader.dataset, batch_size=16, sampler=test_sampler,drop_last = False)\n",
    "        print(f'Fold {fold+1}')\n",
    "        for epoch in tqdm(range(20)):\n",
    "\n",
    "        \n",
    "                epoch_loss = 0.0\n",
    "                epoch_loss_valid = 0.0\n",
    "                model.train()\n",
    "                sample_counter = 0\n",
    "                batch_counter = 0\n",
    "                print(get_lr(optimizer))\n",
    "                for time, batch in enumerate(train_loader_fold): \n",
    "                \n",
    "                        x, edge_index, edge_attr,y = batch[0:4]\n",
    "                        \n",
    "                        signal_samples = x.shape[3]\n",
    "                        x = 2 / signal_samples * torch.abs(x)\n",
    "                        \n",
    "                        x = x.squeeze()\n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                        # mean = torch.mean(x,dim=0)\n",
    "                        # std = torch.std(x,dim=0)\n",
    "                        #x = (x-mean)/std\n",
    "                \n",
    "                        y_hat =torch.stack(\n",
    "                                [model(x=x[n], edge_index=edge_index[n], edge_weight=edge_attr[n]) \n",
    "                                for n in range(x.shape[0])])\n",
    "                        ##loss\n",
    "                \n",
    "                        loss = loss_fn(y_hat,y)\n",
    "                        \n",
    "                        epoch_loss += loss\n",
    "                        ## get preds & gorund truth\n",
    "                        try:\n",
    "                                preds = torch.cat([preds,y_hat],dim=0)\n",
    "                                ground_truth = torch.cat([ground_truth,y],dim=0)\n",
    "                        \n",
    "                        except:\n",
    "                                preds= y_hat\n",
    "                                ground_truth = y\n",
    "                                optimizer.zero_grad()\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "\n",
    "                ## calculate acc\n",
    "\n",
    "                train_auroc = auroc(preds,ground_truth)\n",
    "                sensitivity = recall(preds,ground_truth)\n",
    "                del preds, ground_truth\n",
    "                print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "                print(f'Epoch AUROC: {train_auroc} ')\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                        for time_valid, batch_valid in enumerate(test_loader_fold):\n",
    "                                x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                                signal_samples = x.shape[3]\n",
    "                                x = 2 / signal_samples * torch.abs(x)\n",
    "                                x = x.squeeze()\n",
    "                                # mean = torch.mean(x,dim=0)\n",
    "                                # std = torch.std(x,dim=0)\n",
    "                                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                                #x = (x-mean)/std\n",
    "                                y_hat_val = torch.stack(\n",
    "                                        [model(x=x[n], edge_index=edge_index[n], edge_weight=edge_attr[n]) \n",
    "                                        for n in range(x.shape[0])])\n",
    "                                loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                                epoch_loss_valid += loss_valid\n",
    "                                try:\n",
    "                                        preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                                        ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                                except:\n",
    "                                        preds_valid= y_hat_val\n",
    "                                        ground_truth_valid = y_val\n",
    "               # scheduler.step(epoch_loss_valid)\n",
    "                val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "                val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "                del preds_valid, ground_truth_valid\n",
    "                print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "                print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dumb nlp loop\n",
    "device = torch.device(\"cpu\")\n",
    "#model = RecurrentGCN(TIMESTEP,SFREQ,batch_size=16).to(device)\n",
    "model = MyDumbNLP()\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], alpha))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "recall = BinaryRecall(threshold=0.5,requires_grad=True)\n",
    "auroc = AUROC(task=\"binary\")\n",
    "roc = ROC('binary')\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "\n",
    "    \n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "        \n",
    "                x, edge_index, edge_attr,y = batch[0:4]\n",
    "                \n",
    "                # signal_samples = x.shape[3]\n",
    "                # x = 2 / signal_samples * torch.abs(x)\n",
    "                \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                #x = (x-mean)/std\n",
    "           \n",
    "                y_hat =torch.stack(\n",
    "                        [model(x=x[n].float()) \n",
    "                         for n in range(x.shape[0])])\n",
    "                ##loss\n",
    "        \n",
    "                loss = loss_fn(y_hat,y)\n",
    "                \n",
    "                ## get preds & gorund truth\n",
    "                try:\n",
    "                 preds = torch.cat([preds,y_hat],dim=0)\n",
    "                 ground_truth = torch.cat([ground_truth,y],dim=0)\n",
    "            \n",
    "                except:\n",
    "                 preds= y_hat\n",
    "                 ground_truth = y\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "\n",
    "        train_auroc = auroc(preds,ground_truth)\n",
    "        sensitivity = recall(preds,ground_truth)\n",
    "        del preds, ground_truth\n",
    "        print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                for time_valid, batch_valid in enumerate(val_dataloader):\n",
    "                        x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                        #signal_samples = x.shape[3]\n",
    "                        # x = 2 / signal_samples * torch.abs(x)\n",
    "                        x = x.squeeze()\n",
    "                        # mean = torch.mean(x,dim=0)\n",
    "                        # std = torch.std(x,dim=0)\n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                        #x = (x-mean)/std\n",
    "                        y_hat_val = torch.stack(\n",
    "                                [model(x=x[n].float()) \n",
    "                                 for n in range(x.shape[0])])\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        try:\n",
    "                         preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                         ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                        except:\n",
    "                         preds_valid= y_hat_val\n",
    "                         ground_truth_valid = y_val\n",
    "        scheduler.step()\n",
    "        val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "        val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "        del preds_valid, ground_truth_valid\n",
    "        print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "        print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss_valid = 0.0\n",
    "with torch.no_grad():\n",
    "        for time_valid, batch_valid in enumerate(loso_dataloader):\n",
    "                x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                #signal_samples = x.shape[3]\n",
    "                # x = 2 / signal_samples * torch.abs(x)\n",
    "                x = x.squeeze()\n",
    "                # mean = torch.mean(x,dim=0)\n",
    "                # std = torch.std(x,dim=0)\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                #x = (x-mean)/std\n",
    "                y_hat_val = torch.stack(\n",
    "                        [model(x=x[n].float()) \n",
    "                                for n in range(x.shape[0])])\n",
    "                loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                epoch_loss_valid += loss_valid\n",
    "                try:\n",
    "                        preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                        ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                except:\n",
    "                        preds_valid= y_hat_val\n",
    "                        ground_truth_valid = y_val\n",
    "\n",
    "val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "del preds_valid, ground_truth_valid\n",
    "print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## normal loop\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m GATv2(TIMESTEP,\u001b[39m60\u001b[39m,batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[39m#pos_weight=torch.full([1], 1.1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "## normal loop\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = GATv2(TIMESTEP,60,batch_size=16).to(device)\n",
    "#pos_weight=torch.full([1], 1.1\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], alpha))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "resampler = torchaudio.transforms.Resample(256,60)\n",
    "recall = BinaryRecall(threshold=0.5)\n",
    "specificity = BinarySpecificity(threshold=0.6)\n",
    "auroc = AUROC(task=\"binary\")\n",
    "roc = ROC('binary')\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "\n",
    "for epoch in tqdm(range(13)):\n",
    "\n",
    "    \n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "        \n",
    "                x, edge_index, edge_attr,y = batch[0:4]\n",
    "               \n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(x)\n",
    "            \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "           \n",
    "                y_hat =torch.stack(\n",
    "                        [model(x=x[n], edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                         for n in range(x.shape[0])])\n",
    "                ##loss\n",
    "        \n",
    "                loss = loss_fn(y_hat,y)\n",
    "                #loss = torchvision.ops.sigmoid_focal_loss(y_hat,y,alpha=alpha*0.1,gamma=2,reduction='mean')\n",
    "                epoch_loss += loss\n",
    "                ## get preds & gorund truth\n",
    "                try:\n",
    "                 preds = torch.cat([preds,y_hat],dim=0)\n",
    "                 ground_truth = torch.cat([ground_truth,y],dim=0)\n",
    "            \n",
    "                except:\n",
    "                 preds= y_hat.detach()\n",
    "                 ground_truth = y\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "\n",
    "        train_auroc = auroc(preds,ground_truth)\n",
    "        sensitivity = recall(preds,ground_truth)\n",
    "        train_specificity = specificity(preds,ground_truth)\n",
    "        del preds, ground_truth\n",
    "        print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "        print(f'Epoch specificity: {train_specificity}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                for time_valid, batch_valid in enumerate(val_dataloader):\n",
    "                        x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                    \n",
    "                        signal_samples = x.shape[3]\n",
    "                        x = 2 / signal_samples * torch.abs(x)\n",
    "              \n",
    "                        x = x.squeeze()\n",
    "                      \n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                        \n",
    "                        y_hat_val = torch.stack(\n",
    "                                [model(x=x[n], edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                                 for n in range(x.shape[0])])\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "                        #loss_valid = torchvision.ops.sigmoid_focal_loss(y_hat,y,alpha=alpha*0.1,gamma=2,reduction='mean')\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        try:\n",
    "                         preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                         ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                        except:\n",
    "                         preds_valid= y_hat_val\n",
    "                         ground_truth_valid = y_val\n",
    "        scheduler.step(epoch_loss_valid)\n",
    "        val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "        val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "        val_specificity = specificity(preds_valid,ground_truth_valid)\n",
    "        del preds_valid, ground_truth_valid\n",
    "        print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "        print(f'Epoch val specificity: {train_specificity}')\n",
    "        print(f'Epoch val AUROC: {val_specificity} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m time_test, batch_test \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loso_loader):\n\u001b[1;32m     10\u001b[0m         x, edge_index, edge_attr,y_test \u001b[39m=\u001b[39m batch_test[\u001b[39m0\u001b[39m:\u001b[39m4\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m         signal_samples \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mshape[\u001b[39m3\u001b[39;49m]\n\u001b[1;32m     12\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m/\u001b[39m signal_samples \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mabs(x)\n\u001b[1;32m     14\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "try:\n",
    "        del preds_test,ground_truth_test\n",
    "except:\n",
    "        pass\n",
    "loss_test_total = 0.0\n",
    "with torch.no_grad():\n",
    "        for time_test, batch_test in enumerate(loso_loader):\n",
    "                x, edge_index, edge_attr,y_test = batch_test[0:4]\n",
    "                signal_samples = x.shape[3]\n",
    "                x = 2 / signal_samples * torch.abs(x)\n",
    "                \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                y_hat_test = torch.stack(\n",
    "                                [model(x=x[n].float(), edge_index=edge_index[n], edge_weight=edge_attr[n].float()) \n",
    "                                        for n in range(x.shape[0])])\n",
    "                loss_test = loss_fn(y_hat_test,y_test)\n",
    "                loss_test_total+= loss_test\n",
    "                try:\n",
    "                 preds_test  = torch.cat([preds_test,y_hat_test],dim=0)\n",
    "                 ground_truth_test = torch.cat([ground_truth_test,y_test],dim=0)\n",
    "                except:\n",
    "                 preds_test = y_hat_test\n",
    "                 ground_truth_test = y_test\n",
    "test_auroc = auroc(preds_test,ground_truth_test)\n",
    "test_sensitivity = recall(preds_test,ground_truth_test)\n",
    "print(f'Test sensitivity: {test_sensitivity}', f'Test loss: {loss_test_total.numpy()/time_test+1}')\n",
    "print(f'Test AUROC: {test_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 600])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1737e-02],\n",
       "        [2.8777e-03],\n",
       "        [2.3858e-01],\n",
       "        [3.5076e-03],\n",
       "        [1.0364e-02],\n",
       "        [4.7990e-01],\n",
       "        [2.4076e-01],\n",
       "        [6.4314e-01],\n",
       "        [1.1202e-04],\n",
       "        [1.9877e-02],\n",
       "        [6.6054e-03],\n",
       "        [7.3178e-03],\n",
       "        [2.3780e-04],\n",
       "        [1.9216e-02],\n",
       "        [1.4167e-03],\n",
       "        [4.1621e-02],\n",
       "        [7.1588e-02],\n",
       "        [1.2937e-01],\n",
       "        [1.8439e-02],\n",
       "        [4.4233e-05],\n",
       "        [2.3641e-03],\n",
       "        [3.6222e-02],\n",
       "        [2.2595e-03],\n",
       "        [1.5049e-07],\n",
       "        [2.0636e-01],\n",
       "        [5.0370e-04],\n",
       "        [1.5404e-01],\n",
       "        [9.3539e-01],\n",
       "        [8.2671e-01],\n",
       "        [7.4010e-01],\n",
       "        [1.6278e-02],\n",
       "        [4.0697e-01],\n",
       "        [3.0493e-02],\n",
       "        [1.4850e-04],\n",
       "        [1.3387e-01],\n",
       "        [4.2417e-01],\n",
       "        [3.8312e-02],\n",
       "        [1.6792e-04],\n",
       "        [5.2351e-04],\n",
       "        [8.8920e-02],\n",
       "        [5.6934e-02],\n",
       "        [6.3590e-02],\n",
       "        [1.6345e-02],\n",
       "        [1.5481e-01],\n",
       "        [4.0328e-04],\n",
       "        [1.6424e-05],\n",
       "        [7.2855e-02],\n",
       "        [9.6236e-01],\n",
       "        [4.1784e-01],\n",
       "        [3.7670e-01],\n",
       "        [2.6433e-01],\n",
       "        [2.8007e-02],\n",
       "        [2.0284e-02],\n",
       "        [2.7073e-03],\n",
       "        [9.0782e-01],\n",
       "        [3.8966e-01],\n",
       "        [9.1548e-01],\n",
       "        [1.4646e-02],\n",
       "        [8.7929e-02],\n",
       "        [2.3420e-04],\n",
       "        [4.7124e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sigmoid()(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc(preds_test,ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8e083fd630>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNBatched(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(TGCNBatched, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = TGCN2(sfreq*timestep,32, add_self_loops=True,improved=False, batch_size=batch_size)\n",
    "        self.recurrent_2 = TGCN2(32,64,add_self_loops=True,improved=False, batch_size=batch_size)\n",
    "        self.recurrent_3 = TGCN2(64,128,add_self_loops=True,improved=False, batch_size=batch_size)\n",
    "        self.fc1 = torch.nn.Linear(n_nodes*128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.squeeze(x)\n",
    "        h = self.recurrent_1(x, edge_index=edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_2(h, edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_3(h, edge_index)\n",
    "        h = torch.nn.BatchNorm1d(18)(h)\n",
    "      \n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.flatten(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "Epoch: 0 Epoch sensitivity: 0.6780905723571777 Epoch loss: 2.2557904368852713\n",
      "Epoch AUROC: 0.5408072471618652 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:15<02:15, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Epoch val_sensitivity: 0.6763284802436829 Epoch val_loss: 2.2348775634765623\n",
      "Epoch val AUROC: 0.6916943192481995 \n",
      "0.001\n",
      "Epoch: 1 Epoch sensitivity: 0.6070991158485413 Epoch loss: 2.1710371648172933\n",
      "Epoch AUROC: 0.665611743927002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:31<02:08, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Epoch val_sensitivity: 0.4057970941066742 Epoch val_loss: 2.160444580078125\n",
      "Epoch val AUROC: 0.6969209909439087 \n",
      "0.001\n",
      "Epoch: 2 Epoch sensitivity: 0.6083231568336487 Epoch loss: 2.1272257162755226\n",
      "Epoch AUROC: 0.7110415697097778 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:50<01:59, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Epoch val_sensitivity: 0.4589371979236603 Epoch val_loss: 2.140898681640625\n",
      "Epoch val AUROC: 0.6977068185806274 \n",
      "0.001\n",
      "Epoch: 3 Epoch sensitivity: 0.5813953280448914 Epoch loss: 2.0961814363639193\n",
      "Epoch AUROC: 0.7263129949569702 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:09<01:49, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Epoch val_sensitivity: 0.4975845515727997 Epoch val_loss: 2.11329443359375\n",
      "Epoch val AUROC: 0.7286343574523926 \n",
      "0.001\n",
      "Epoch: 4 Epoch sensitivity: 0.509179949760437 Epoch loss: 2.071815277950697\n",
      "Epoch AUROC: 0.7231451272964478 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:28<01:32, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Epoch val_sensitivity: 0.3864734172821045 Epoch val_loss: 2.104787963867188\n",
      "Epoch val AUROC: 0.7072305679321289 \n",
      "0.001\n",
      "Epoch: 5 Epoch sensitivity: 0.4773561954498291 Epoch loss: 2.046755847702938\n",
      "Epoch AUROC: 0.7299982905387878 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:47<01:14, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Epoch val_sensitivity: 0.3478260934352875 Epoch val_loss: 2.124690185546875\n",
      "Epoch val AUROC: 0.7074652910232544 \n",
      "0.001\n",
      "Epoch: 6 Epoch sensitivity: 0.4993880093097687 Epoch loss: 2.0450905119755354\n",
      "Epoch AUROC: 0.7378701567649841 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:06<00:55, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Epoch val_sensitivity: 0.30917873978614807 Epoch val_loss: 2.1198828125\n",
      "Epoch val AUROC: 0.7151045799255371 \n",
      "0.001\n",
      "Epoch: 7 Epoch sensitivity: 0.5140758752822876 Epoch loss: 2.0455662700759465\n",
      "Epoch AUROC: 0.7451328635215759 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:25<00:37, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Epoch val_sensitivity: 0.3913043439388275 Epoch val_loss: 2.1271722412109373\n",
      "Epoch val AUROC: 0.6949011087417603 \n",
      "0.0001\n",
      "Epoch: 8 Epoch sensitivity: 0.5299877524375916 Epoch loss: 2.009807449887948\n",
      "Epoch AUROC: 0.7493698596954346 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:44<00:18, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Epoch val_sensitivity: 0.40096619725227356 Epoch val_loss: 2.1264189453125\n",
      "Epoch val AUROC: 0.7010490894317627 \n",
      "0.0001\n",
      "Epoch: 9 Epoch sensitivity: 0.5495715737342834 Epoch loss: 1.9862154166536978\n",
      "Epoch AUROC: 0.764013946056366 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:04<00:00, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Epoch val_sensitivity: 0.4057970941066742 Epoch val_loss: 2.1267490234375\n",
      "Epoch val AUROC: 0.704300045967102 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## normal loop\n",
    "device = torch.device(\"cpu\")\n",
    "model = TGCNBatched(TIMESTEP,60,batch_size=16).to(device)\n",
    "#pos_weight=torch.full([1], 1.1\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], alpha))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "recall = BinaryRecall(threshold=0.5)\n",
    "auroc = AUROC(task=\"binary\")\n",
    "roc = ROC('binary')\n",
    "resampler = torchaudio.transforms.Resample(256,60)\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "\n",
    "    \n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "        \n",
    "                x, edge_index, edge_attr,y = batch[0:4]\n",
    "                \n",
    "                # signal_samples = x.shape[3]\n",
    "                # x = 2 / signal_samples * torch.abs(x)\n",
    "                \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "           \n",
    "                y_hat = model(x,edge_index[0])\n",
    "                ##loss\n",
    "        \n",
    "                loss = loss_fn(y_hat,y)\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                ## get preds & gorund truth\n",
    "                try:\n",
    "                 preds = torch.cat([preds,y_hat],dim=0)\n",
    "                 ground_truth = torch.cat([ground_truth,y],dim=0)\n",
    "            \n",
    "                except:\n",
    "                 preds= y_hat\n",
    "                 ground_truth = y\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "\n",
    "        train_auroc = auroc(preds,ground_truth)\n",
    "        sensitivity = recall(preds,ground_truth)\n",
    "        del preds, ground_truth\n",
    "        print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()/time+1}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                for time_valid, batch_valid in enumerate(val_dataloader):\n",
    "                        x, edge_index, edge_attr,y_val = batch_valid[0:4]\n",
    "                        \n",
    "                        # signal_samples = x.shape[3]\n",
    "                        # x = 2 / signal_samples * torch.abs(x)\n",
    "                        x = x.squeeze()\n",
    "                      \n",
    "                        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                     \n",
    "                        y_hat_val = model(x.float(),edge_index[0])\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        try:\n",
    "                         preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                         ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                        except:\n",
    "                         preds_valid= y_hat_val\n",
    "                         ground_truth_valid = y_val\n",
    "        scheduler.step(epoch_loss_valid)\n",
    "        val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "        val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "        del preds_valid, ground_truth_valid\n",
    "        print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/time_valid+1}')\n",
    "        print(f'Epoch val AUROC: {val_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylayer = A3TGCN2(10*60,32,1,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylayer(x,edge_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sensitivity: 0.4057970941066742 Test loss: 2.1267490234375\n",
      "Epoch val AUROC: 0.704300045967102 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "try:\n",
    "        del preds_test,ground_truth_test\n",
    "except:\n",
    "        pass\n",
    "loss_test_total = 0.0\n",
    "with torch.no_grad():\n",
    "        for time_test, batch_test in enumerate(val_dataloader):\n",
    "                x, edge_index, edge_attr,y_test = batch_test[0:4]\n",
    "                # signal_samples = x.shape[3]\n",
    "                # x = 2 / signal_samples * torch.abs(x)\n",
    "               \n",
    "                x = x.squeeze()\n",
    "                x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "                y_hat_test = model(x.float(),edge_index[0])\n",
    "                loss_test = loss_fn(y_hat_test,y_test)\n",
    "                loss_test_total+= loss_test\n",
    "                try:\n",
    "                 preds_test  = torch.cat([preds_test,y_hat_test],dim=0)\n",
    "                 ground_truth_test = torch.cat([ground_truth_test,y_test],dim=0)\n",
    "                except:\n",
    "                 preds_test = y_hat_test\n",
    "                 ground_truth_test = y_test\n",
    "test_auroc = auroc(preds_test,ground_truth_test)\n",
    "test_sensitivity = recall(preds_test,ground_truth_test)\n",
    "print(f'Test sensitivity: {test_sensitivity}', f'Test loss: {loss_test_total.numpy()/time_test+1}')\n",
    "print(f'Epoch val AUROC: {test_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 60, 1: 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader._val_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-14.716414, -14.44849 , -14.086151, ...,  11.199499,  11.632316,\n",
       "         12.473233], dtype=float32),\n",
       " array([1, 1, 1, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(preds_test.numpy(),return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cc438b9c90976695678f0d6c20e4c06983b5710e6855b5b4390f60ecf93fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
