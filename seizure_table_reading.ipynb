{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Users\\Szymon Mazurek\\anaconda3\\envs\\sano_eeg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from torch_geometric_temporal import  DynamicGraphTemporalSignal,StaticGraphTemporalSignal\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import scipy\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torchmetrics.classification import BinaryRecall\n",
    "from torch_geometric.nn import global_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plv_connectivity(sensors,data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sensors : INT\n",
    "        DESCRIPTION. No of sensors used for capturing EEG\n",
    "    data : Array of float \n",
    "        DESCRIPTION. EEG Data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    connectivity_matrix : Matrix of float\n",
    "        DESCRIPTION. PLV connectivity matrix\n",
    "    connectivity_vector : Vector of flaot \n",
    "        DESCRIPTION. PLV connectivity vector\n",
    "    \"\"\"\n",
    "    print(\"PLV in process.....\")\n",
    "    \n",
    "    # Predefining connectivity matrix\n",
    "    connectivity_matrix = np.zeros([sensors,sensors],dtype=float)\n",
    "    \n",
    "    # Computing hilbert transform\n",
    "    data_points = data.shape[-1]\n",
    "    data_hilbert = np.imag(scipy.signal.hilbert(data))\n",
    "    phase = np.arctan(data_hilbert/data)\n",
    "    \n",
    "    # Computing connectivity matrix \n",
    "    for i in range(sensors):\n",
    "        for k in range(sensors):\n",
    "            connectivity_matrix[i,k] = np.abs(np.sum(np.exp(1j*(phase[i,:]-phase[k,:]))))/data_points\n",
    "            \n",
    "    # Computing connectivity vector\n",
    "   # connectivity_vector = connectivity_matrix[np.triu_indices(connectivity_matrix.shape[0],k=1)] \n",
    "      \n",
    "    # returning connectivity matrix and vector\n",
    "    print(\"PLV done!\")\n",
    "    return connectivity_matrix#, connectivity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path :Path\n",
    "    event_tables_path : Path\n",
    "    loso_patient : str = None\n",
    "    sampling_f : int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    overlap: int = 0\n",
    "    self_loops : bool = True,\n",
    "    shuffle=True\n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path {Path} -- Path to folder with dataset preprocessed into .npy files.\n",
    "        event_tables_path {Path} -- Path to folder with .csv files containing seizure events information for every patient.\n",
    "        loso_patient {str} -- Name of patient to be selected for LOSO valdiation, specified in format \"chb{patient_number}\"\",\n",
    "        eg. \"chb16\". (default: {None}).\n",
    "        samplin_f {int} -- Sampling frequency of the loaded eeg data. (default: {256}).\n",
    "        seizure_lookback {int} -- Time horizon to sample pre-seizure data (length of period before seizure) in seconds. \n",
    "        (default: {600}).\n",
    "        sample_timestep {int} -- Amounts of seconds analyzed in a single sample. (default: {5}).\n",
    "        overlap {int} -- Amount of seconds overlap between samples. (default: {0}).\n",
    "        self_loops {bool} -- Wheather to add self loops to nodes of the graph. (default: {True}).\n",
    "        shuffle {bool} --  Wheather to shuffle training samples.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def _get_event_tables(self,patient_name : str) -> tuple[dict,dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_start_table, patient_stop_table = [os.path.join(self.event_tables_path,ev_table)\n",
    "        for ev_table in event_table_list if patient_name in ev_table]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict('index')\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict('index')\n",
    "        return start_events_dict,stop_events_dict\n",
    "        \n",
    "    def _get_recording_events(self,events_dict,recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict\"\"\"\n",
    "        recording_list = list(events_dict[recording+'.edf'].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "\n",
    "    def _get_graph(self,n_nodes: int) -> nx.Graph :\n",
    "        \"\"\"Creates Networx fully connected graph with self loops\"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node,node]for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "    \n",
    "    def _get_edge_weights_recording(self,plv_values: np.ndarray) ->np.ndarray:\n",
    "        \"\"\"Method that takes plv values for given recording and assigns them \n",
    "        as edge attributes to a fc graph.\"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start,e_end = edge\n",
    "            garph_dict[edge] = {'plv':plv_values[e_start,e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "    \n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(),axis=0)\n",
    "        edges_per_sample_train = np.repeat(edges,repeats =self._features.shape[0],axis=0)\n",
    "        edges_per_sample_val = np.repeat(edges,repeats =self._val_features.shape[0],axis=0)\n",
    "        self._edges = edges_per_sample_train\n",
    "        self._val_edges = edges_per_sample_val\n",
    "       \n",
    "        \n",
    "    def _get_labels_features_edge_weights(self):\n",
    "        \"\"\"Prepare features, labels, time labels and edge wieghts for training and \n",
    "        optionally validation data.\"\"\"\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        for patient in patient_list: # iterate over patient names\n",
    "            event_tables = self._get_event_tables(patient) # extract start and stop of seizure for patient \n",
    "            patient_path = os.path.join(self.npy_dataset_path,patient)\n",
    "            recording_list = os.listdir(patient_path)\n",
    "            for record in recording_list: # iterate over recordings for a patient\n",
    "                recording_path = os.path.join(patient_path,record)\n",
    "                record_id = record.split('.npy')[0] #  get record id\n",
    "                start_event_tables = self._get_recording_events(event_tables[0],record_id) # get start events\n",
    "                stop_event_tables = self._get_recording_events(event_tables[1],record_id) # get stop events\n",
    "                data_array = np.load(recording_path) # load the recording\n",
    "                \n",
    "                plv_edge_weights = np.expand_dims(\n",
    "                    self._get_edge_weights_recording(\n",
    "                        np.random.uniform(size=(18,18))\n",
    "                    #plv_connectivity(data_array.shape[0],data_array)\n",
    "                ),\n",
    "                axis = 0\n",
    "                )\n",
    "                \n",
    "\n",
    "                ##TODO add a gateway to reject seizure periods shorter than lookback\n",
    "                # extract timeseries and labels from the array\n",
    "                features,labels,time_labels = utils.extract_training_data_and_labels(\n",
    "                    data_array,\n",
    "                    start_event_tables,\n",
    "                    stop_event_tables,\n",
    "                    fs = self.sampling_f,\n",
    "                    seizure_lookback = self.seizure_lookback,\n",
    "                    sample_timestep = self.sample_timestep,\n",
    "                    overlap = self.overlap,\n",
    "                )\n",
    "\n",
    "                time_labels = time_labels.astype(np.int32)\n",
    "                labels = labels.reshape((labels.shape[0],1)).astype(np.float32)\n",
    "                if patient == self.loso_patient:\n",
    "                    try:\n",
    "                        self._val_features = np.concatenate((self._val_features, features))\n",
    "                        self._val_labels = np.concatenate((self._val_labels, labels))\n",
    "                        self._val_time_labels = np.concatenate((self._val_time_labels , time_labels))\n",
    "                        self._val_edge_weights = np.concatenate((\n",
    "                            self._val_edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                            ))\n",
    "                    except:\n",
    "                        self._val_features = features\n",
    "                        self._val_labels = labels\n",
    "                        self._val_time_labels = time_labels\n",
    "                        self._val_edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        self._features = np.concatenate((self._features, features))\n",
    "                        self._labels = np.concatenate((self._labels, labels))\n",
    "                        self._time_labels = np.concatenate((self._time_labels , time_labels))\n",
    "                        self._edge_weights = np.concatenate((\n",
    "                            self._edge_weights,\n",
    "                            np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                        ))\n",
    "                       \n",
    "                    except:\n",
    "                        print(\"Creating initial attributes\")\n",
    "                        self._features = features\n",
    "                        self._labels = labels\n",
    "                        self._time_labels = time_labels\n",
    "                        self._edge_weights = np.repeat(plv_edge_weights,features.shape[0],axis=0)\n",
    "                \n",
    "        if self.shuffle is True:\n",
    "            shuffled_features, shuffled_labels, shuffled_time_labels, shuffled_edge_weights = sklearn.utils.shuffle(self._features,self._labels,self._time_labels,self._edge_weights)\n",
    "            self._features = shuffled_features\n",
    "            self._labels = shuffled_labels\n",
    "            self._time_labels = shuffled_time_labels\n",
    "            self._edge_weights = shuffled_edge_weights\n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self) -> DynamicGraphTemporalSignal:\n",
    "\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes \n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of \n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        \n",
    "        self._get_labels_features_edge_weights()\n",
    "        self._get_edges()\n",
    "        \n",
    "        train_dataset = DynamicGraphTemporalSignal(\n",
    "        self._edges, self._edge_weights, self._features, self._labels, time_labels = self._time_labels\n",
    "        )\n",
    "        if self.loso_patient:\n",
    "            val_dataset = DynamicGraphTemporalSignal(\n",
    "            self._val_edges, self._val_edge_weights, self._val_features, self._val_labels, time_labels = self._val_time_labels\n",
    "            )\n",
    "            return train_dataset, val_dataset\n",
    "\n",
    "        return train_dataset\n",
    "        \n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEP = 5\n",
    "OVERLAP = 0\n",
    "SFREQ = 256\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('npy_data'),\n",
    "    event_tables_path=Path('event_tables'),\n",
    "    loso_patient='chb16',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    overlap=OVERLAP,\n",
    "    self_loops=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial attributes\n"
     ]
    }
   ],
   "source": [
    "train_loader,val_loader=dataloader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        out_features = 64\n",
    "        self.recurrent_1 = DCRNN(timestep*sfreq, out_features, 1)\n",
    "        self.fc1 = torch.nn.Linear(out_features*n_nodes, 1)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=0)\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = torch.squeeze(x)\n",
    "        x = F.normalize(x,dim=1)\n",
    "        h = self.recurrent_1(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        print(global_max_pool(torch.reshape(h,(1,h.shape[0],h.shape[1])),size=1))\n",
    "        h = self.flatten(h)\n",
    "        h = self.fc1(h)\n",
    "        #h = F.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_accuracy(y_true, y_prob):\n",
    "    \"\"\"Binary accuracy calculation\"\"\"\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_prob = np.where(y_prob <= 0.0, 0, y_prob)\n",
    "    y_prob = np.where(y_prob > 0.0, 1, y_prob)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_prob)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "global_max_pool() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m time, snapshot \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 21\u001b[0m         y_hat \u001b[39m=\u001b[39m model(snapshot\u001b[39m.\u001b[39;49mx, snapshot\u001b[39m.\u001b[39;49medge_index, snapshot\u001b[39m.\u001b[39;49medge_attr)\n\u001b[0;32m     22\u001b[0m         \u001b[39m##loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(y_hat,snapshot\u001b[39m.\u001b[39my)\n",
      "File \u001b[1;32ma:\\Users\\Szymon Mazurek\\anaconda3\\envs\\sano_eeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [10], line 13\u001b[0m, in \u001b[0;36mRecurrentGCN.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     11\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurrent_1(x, edge_index, edge_weight)\n\u001b[0;32m     12\u001b[0m h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(h)\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(global_max_pool(torch\u001b[39m.\u001b[39;49mreshape(h,(\u001b[39m1\u001b[39;49m,h\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],h\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])),size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     14\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(h)\n\u001b[0;32m     15\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(h)\n",
      "\u001b[1;31mTypeError\u001b[0m: global_max_pool() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device(\"cpu\")\n",
    "model = RecurrentGCN(TIMESTEP,SFREQ).to(device)\n",
    "\n",
    "loss_fn =  nn.BCEWithLogitsLoss(pos_weight=torch.full([1], 15))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "recall = BinaryRecall(threshold=0.5)\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    \n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "    epoch_loss = 0.0\n",
    "    epoch_loss_valid = 0.0\n",
    "    preds_valid = []\n",
    "    ground_truth_valid = []\n",
    "    model.train()\n",
    "    for time, snapshot in enumerate(train_loader):\n",
    "        \n",
    "            y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "            ##loss\n",
    "    \n",
    "            loss = loss_fn(y_hat,snapshot.y)\n",
    "            epoch_loss += loss\n",
    "            ## get preds & gorund truth\n",
    "            preds.append(y_hat.detach().numpy()[0])\n",
    "            ground_truth.append(snapshot.y.numpy()[0])\n",
    "            ##backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    ## calculate acc\n",
    " \n",
    "    sensitivity = recall(torch.FloatTensor(preds),torch.FloatTensor(ground_truth))\n",
    "    print(f'Epoch: {epoch}',f'Epoch sensitivity: {sensitivity}', f'Epoch loss: {epoch_loss.detach().numpy()}')\n",
    "    model.eval()\n",
    "    for time_valid, snapshot_valid in enumerate(val_loader):\n",
    "\n",
    "       # if time_valid == 171 or time_valid == 169:\n",
    "            y_hat_val = model(snapshot_valid.x,snapshot_valid.edge_index, snapshot_valid.edge_attr)\n",
    "            loss_valid = loss_fn(y_hat_val,snapshot_valid.y)\n",
    "            epoch_loss_valid += loss_valid\n",
    "            preds_valid.append(y_hat_val.detach().numpy()[0])\n",
    "            ground_truth_valid.append(snapshot_valid.y.numpy()[0])\n",
    "            \n",
    "    \n",
    "\n",
    "    val_sensitivity = recall(torch.FloatTensor(preds_valid),torch.FloatTensor(ground_truth_valid))\n",
    "    print(f'Epoch: {epoch}',f'Epoch val_sensitivity: {val_sensitivity}', f'Epoch val_loss: {epoch_loss_valid.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(ground_truth) == 1)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(np.where(np.array(ground_truth) == 1)[0],np.where(np.array(preds) >0)[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(preds) >0)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sano_eeg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5382ef638fa5c12c498aced3863f52a5a1fec44f4008cfb813246c6500772437"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
