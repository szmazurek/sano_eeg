{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import time\n",
    "import os\n",
    "import utils\n",
    "import warnings\n",
    "import scipy\n",
    "import sklearn\n",
    "import torch\n",
    "import torch_geometric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import lightning.pytorch as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from tsfresh import extract_features\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchmetrics.classification import BinaryRecall, BinarySpecificity, AUROC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from mne_features.univariate import (\n",
    "    compute_variance,\n",
    "    compute_hjorth_complexity,\n",
    "    compute_hjorth_mobility,\n",
    "    compute_line_length,\n",
    "    compute_higuchi_fd,\n",
    "    compute_katz_fd,\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import Parallel, delayed\n",
    "from models import ClassicGCN, GATv2, GATv2Lightning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*does not have many workers.*\"\n",
    ")  ## DISABLED ON PURPOSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think about using kwargs argument here to specify args for dataloader\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path: Path\n",
    "    event_tables_path: Path\n",
    "    plv_values_path: Path\n",
    "    loso_patient: str = None\n",
    "    sampling_f: int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    preictal_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    self_loops: bool = True\n",
    "    balance: bool = True\n",
    "    train_test_split: float = None\n",
    "    fft: bool = False\n",
    "    hjorth: bool = False\n",
    "    downsample: int = None\n",
    "    buffer_time: int = 15\n",
    "    batch_size: int = 32\n",
    "    smote: bool = False\n",
    "    tsfresh: bool = False\n",
    "    rescale: bool = False\n",
    "    used_classes_dict: dict[str] = field(\n",
    "        default_factory=lambda: {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    )\n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path: (Path) Path to directory with .npy files\n",
    "        event_tables_path: (Path) Path to directory with .csv files\n",
    "        plv_values_path: (Path) Path to directory with .npy files\n",
    "        loso_patient: (str) Patient name to be left out of training set.\n",
    "    If None, no patient is left out for testing. (default: None)\n",
    "        sampling_f: (int) Sampling frequency of the recordings. (default: 256)\n",
    "        seizure_lookback: (int) Time in seconds to look back from seizure onset. (default: 600)\n",
    "        sample_timestep: (int) Time in seconds between samples. (default: 5)\n",
    "        inter_overlap: (int) Time in seconds to overlap between interictal samples. (default: 0)\n",
    "        preictal_overlap: (int) Time in seconds to overlap between preictal samples. (default: 0)\n",
    "        ictal_overlap: (int) Time in seconds to overlap between ictal samples. (default: 0)\n",
    "        self_loops: (bool) Whether to add self loops to the graph. (default: True)\n",
    "        balance: (bool) Whether to balance the classes. (default: True)\n",
    "        train_test_split: (float) Percentage of data to be used for testing. (default: None)\n",
    "        fft: (bool) Whether to use fft features. (default: False)\n",
    "        hjorth: (bool) Whether to use hjorth features. (default: False)\n",
    "        downsample: (int) Factor by which to downsample the data. (default: None)\n",
    "        buffer_time: (int) Time in seconds to skip before and after every sample from seizure period. \n",
    "    (default: 15)\n",
    "        batch_size: (int) Batch size for dataloaders. (default: 32)\n",
    "        smote: (bool) Whether to use smote to balance the classes. (default: False)\n",
    "        used_classes_ditct: (dict) Dictionary with classes to be used. \n",
    "    (default: {'interictal': True, 'preictal': True, 'ictal': True})\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # if used_classes_dict is None:\n",
    "    #     used_classes_dict = {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    assert (fft and hjorth) == False, \"When fft is True, hjorth should be False\"\n",
    "    assert (downsample is None) or (\n",
    "        downsample > 0\n",
    "    ), \"Downsample should be None or positive integer\"\n",
    "    assert (train_test_split is None) or (\n",
    "        train_test_split > 0 and train_test_split < 1\n",
    "    ), \"Train test split should be None or float between 0 and 1\"\n",
    "\n",
    "    assert not (smote and balance), \"Cannot use smote and balance at the same time\"\n",
    "    assert not (\n",
    "        (fft or hjorth) and tsfresh\n",
    "    ), \"Cannot use fft or hjorth and tsfresh at the same time\"\n",
    "\n",
    "    def _get_event_tables(self, patient_name: str) -> tuple[dict, dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\n",
    "        Args:\n",
    "            patient_name: (str) Name of the patient to get events for.\n",
    "        Returns:\n",
    "            start_events_dict: (dict) Dictionary with start events for given patient.\n",
    "            stop_events_dict: (dict) Dictionary with stop events for given patient.\n",
    "        \"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [\n",
    "            os.path.join(self.event_tables_path, ev_table)\n",
    "            for ev_table in event_table_list\n",
    "            if patient_name in ev_table\n",
    "        ]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[\n",
    "            0\n",
    "        ]  ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict(\"index\")\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict(\"index\")\n",
    "        return start_events_dict, stop_events_dict\n",
    "\n",
    "    def _get_recording_events(self, events_dict, recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict.\n",
    "        Args:\n",
    "            events_dict: (dict) Dictionary with events for given patient.\n",
    "            recording: (str) Name of the recording to get events for.\n",
    "        Returns:\n",
    "            recording_events: (list) List of seizure event start and stop time for given recording.\n",
    "        \"\"\"\n",
    "        recording_list = list(events_dict[recording + \".edf\"].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "    def _get_graph(self, n_nodes: int) -> nx.Graph:\n",
    "        \"\"\"Creates Networx complete graph with self loops\n",
    "        for given number of nodes.\n",
    "        Args:\n",
    "            n_nodes: (int) Number of nodes in the graph.\n",
    "        Returns:\n",
    "            graph: (nx.Graph) Complete graph with self loops.\n",
    "        \"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node, node] for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "\n",
    "    def _get_edge_weights_recording(self, plv_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Method for extracting PLV values associated with given edges for a recording.\n",
    "        The PLV was computed for the entire recroding for all channels when the recording was\n",
    "        processed.\n",
    "        Args:\n",
    "            plv_values: (np.ndarray) Array with PLV values for given recording.\n",
    "        Returns:\n",
    "            edge_weights: (np.ndarray) Array with PLV values for given edges.\n",
    "        \"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start, e_end = edge\n",
    "            garph_dict[edge] = {\"plv\": plv_values[e_start, e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "    def _create_edge_idx_and_attributes(self,connectivity_matrix : np.ndarray , threshold : int =0.0) -> tuple[np.ndarray,np.ndarray]:\n",
    "        \"\"\"Create adjacency matrix from connectivity matrix. Edges are created for values above threshold.\n",
    "        If the edge is created, it has an attribute \"weight\" with the value of the connectivity measure associated.\n",
    "        Args:\n",
    "            connectivity_matrix: (np.ndarray) Array with connectivity values.\n",
    "            threshold: (float) Threshold for creating edges. (default: 0.0)\n",
    "        Returns:\n",
    "            edge_index: (np.ndarray) Array with edge indices.\n",
    "            edge_weights: (np.ndarray) Array with edge weights.\n",
    "        \"\"\"\n",
    "        result_graph = nx.graph.Graph()\n",
    "        n_nodes = connectivity_matrix.shape[0]\n",
    "        result_graph.add_nodes_from(range(n_nodes))\n",
    "        edge_tuples = [(i,j,{\"weight\": connectivity_matrix[i,j]}) for i in range(n_nodes) for j in range(n_nodes) if connectivity_matrix[i,j]>threshold]\n",
    "        result_graph.add_edges_from(edge_tuples)\n",
    "        pyg_graph = from_networkx(result_graph)\n",
    "        edge_index = pyg_graph.edge_index\n",
    "        edge_weights = pyg_graph.weight\n",
    "        return edge_index, edge_weights\n",
    "    \n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(), axis=0)\n",
    "        edges_per_sample_train = np.repeat(\n",
    "            edges, repeats=self._features.shape[0], axis=0\n",
    "        )\n",
    "        self._edges = torch.tensor(edges_per_sample_train)\n",
    "        if self.loso_patient is not None:\n",
    "            edges_per_sample_val = np.repeat(\n",
    "                edges, repeats=self._loso_features.shape[0], axis=0\n",
    "            )\n",
    "            self._loso_edges = torch.tensor(edges_per_sample_val)\n",
    "\n",
    "    def _array_to_tensor(self):\n",
    "        \"\"\"Method converting features, edges and weights to torch.tensors\"\"\"\n",
    "\n",
    "        self._features = torch.from_numpy(self._features)\n",
    "        self._labels = torch.from_numpy(self._labels)\n",
    "        # self._time_labels = torch.from_numpy(self._time_labels)\n",
    "        # self._edge_weights = torch.from_numpy(self._edge_weights)\n",
    "        if self.loso_patient is not None:\n",
    "            self._loso_features = torch.from_numpy(self._loso_features)\n",
    "            self._loso_labels = torch.from_numpy(self._loso_labels)\n",
    "            # self._loso_time_labels = torch.from_numpy(self._loso_time_labels)\n",
    "            # self._loso_edge_weights = torch.from_numpy(self._loso_edge_weights)\n",
    "\n",
    "    def _get_labels_count(self):\n",
    "        \"\"\"Convenience method to get counts of labels in the dataset.\"\"\"\n",
    "        labels, counts = np.unique(self._labels, return_counts=True)\n",
    "        self._label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._label_counts[int(label)] = counts[n]\n",
    "        if self.loso_patient is not None:\n",
    "            labels, counts = np.unique(self._loso_labels, return_counts=True)\n",
    "            self._val_label_counts = {}\n",
    "            for n, label in enumerate(labels):\n",
    "                self._val_label_counts[int(label)] = counts[n]\n",
    "\n",
    "    def _calculate_hjorth_features(self, features):\n",
    "        \"\"\"Converting features to Hjorth features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features to be converted.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with Hjorth features.\n",
    "        \"\"\"\n",
    "        new_features = np.array(\n",
    "            [\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        np.expand_dims(compute_variance(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_mobility(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_complexity(feature), 1),\n",
    "                        np.expand_dims(compute_line_length(feature), 1),\n",
    "                        np.expand_dims(compute_katz_fd(feature), 1),\n",
    "                        np.expand_dims(compute_higuchi_fd(feature), 1),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for feature in features\n",
    "            ]\n",
    "        )\n",
    "        # new_mean = new_features.mean(axis=0)\n",
    "        # new_std = new_features.std(axis=0)\n",
    "        # new_features = (new_features - new_mean) / new_std\n",
    "        return new_features\n",
    "\n",
    "    def _features_to_data_list(self, features, edges, labels, edge_weights=None):\n",
    "        \"\"\"Converts features, edges and labels to list of torch_geometric.data.Data objects.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            edges: (np.ndarray) Array with edges.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        \"\"\"\n",
    "        data_list = [\n",
    "            Data(\n",
    "                x=features[i],\n",
    "                edge_index=edges[i],\n",
    "                edge_attr=edge_weights[i],\n",
    "                y=labels[i],\n",
    "                # time=time_label[i],\n",
    "            )\n",
    "            for i in range(len(features))\n",
    "        ]\n",
    "        return data_list\n",
    "\n",
    "    def _split_data_list(self, data_list):\n",
    "        \"\"\"Methods for splitting list of torch_geometric.data.Data objects into train and validation sets.\n",
    "        Uses StratifiedShuffleSplit to ensure that the classes are balanced in both sets.\n",
    "        Args:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        Returns:\n",
    "            data_list_train: (list) List of torch_geometric.data.Data objects for training.\n",
    "            dataset_list_val: (list) List of torch_geometric.data.Data objects for validation.\n",
    "        \"\"\"\n",
    "        class_labels = torch.tensor(\n",
    "            [data.y.item() for data in data_list], dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "        patient_labels = torch.tensor(\n",
    "            np.expand_dims(self._patient_number, 1), dtype=torch.float32\n",
    "        )\n",
    "        class_labels_patient_labels = torch.cat([class_labels, patient_labels], dim=1)\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=self.train_test_split, random_state=42\n",
    "        )\n",
    "        train_indices, val_indices = next(\n",
    "            splitter.split(data_list, class_labels_patient_labels)\n",
    "        )\n",
    "        self._indexes_to_later_delete = {\"train\": train_indices, \"val\": val_indices}\n",
    "        data_list_train = [data_list[i] for i in train_indices]\n",
    "        dataset_list_val = [data_list[i] for i in val_indices]\n",
    "        return data_list_train, dataset_list_val\n",
    "\n",
    "    def _initialize_dicts(self):\n",
    "        \"\"\"Temporary method to initialize dictionaries for storing features, labels, etc.\n",
    "        Looks terrible, but convenient so far.\n",
    "        \"\"\"\n",
    "        self._features_dict = {}\n",
    "        self._labels_dict = {}\n",
    "        self._time_labels_dict = {}\n",
    "        self._edges_dict = {}\n",
    "        self._edge_weights_dict = {}\n",
    "        self._patient_number_dict = {}\n",
    "        if self.loso_patient:\n",
    "            self._loso_features_dict = {}\n",
    "            self._loso_labels_dict = {}\n",
    "            self._loso_time_labels_dict = {}\n",
    "            self._loso_edges_dict = {}\n",
    "            self._loso_edge_weights_dict = {}\n",
    "            self._loso_patient_number_dict = {}\n",
    "\n",
    "    def _convert_dict_to_array(self):\n",
    "        \"\"\"A method to convert dictionaries to numpy arrays. This approach with dicts is redundant,\n",
    "        but allows for joblib parallelization for data loading by not using concatenation in the loading loop.\n",
    "        \"\"\"\n",
    "        self._features = np.concatenate(\n",
    "            [self._features_dict[key] for key in self._features_dict.keys()]\n",
    "        )\n",
    "        del self._features_dict\n",
    "\n",
    "        self._labels = np.concatenate(\n",
    "            [self._labels_dict[key] for key in self._labels_dict.keys()]\n",
    "        )\n",
    "        del self._labels_dict\n",
    "        # self._time_labels = np.concatenate(\n",
    "        #     [self._time_labels_dict[key] for key in self._time_labels_dict.keys()]\n",
    "        # )\n",
    "        # del self._time_labels_dict\n",
    "        # self._edge_weights = np.concatenate(\n",
    "        #     [self._edge_weights_dict[key] for key in self._edge_weights_dict.keys()]\n",
    "        # )\n",
    "        self._edge_weights = [item for sublist in [self._edge_weights_dict[patient] for patient in self._edge_weights_dict.keys()] for item in sublist]\n",
    "        del self._edge_weights_dict\n",
    "        self._edges = [item for sublist in [self._edges_dict[patient] for patient in self._edges_dict.keys()] for item in sublist]\n",
    "        del self._edges_dict\n",
    "        self._patient_number = np.concatenate(\n",
    "            [self._patient_number_dict[key] for key in self._patient_number_dict.keys()]\n",
    "        )\n",
    "        del self._patient_number_dict\n",
    "        if self.loso_patient:\n",
    "            self._loso_features = np.concatenate(\n",
    "                [self._loso_features_dict[key] for key in self._loso_features_dict.keys()]\n",
    "            )\n",
    "            del self._loso_features_dict\n",
    "            self._loso_labels = np.concatenate(\n",
    "                [self._loso_labels_dict[key] for key in self._loso_labels_dict.keys()]\n",
    "            )\n",
    "            del self._loso_labels_dict\n",
    "            # self._loso_time_labels = np.concatenate(\n",
    "            #     [\n",
    "            #         self._loso_time_labels_dict[key]\n",
    "            #         for key in self._loso_time_labels_dict.keys()\n",
    "            #     ]\n",
    "            # )\n",
    "            # del self._loso_time_labels_dict\n",
    "            # self._loso_edge_weights = np.concatenate(\n",
    "            #     [\n",
    "            #         self._loso_edge_weights_dict[key]\n",
    "            #         for key in self._loso_edge_weights_dict.keys()\n",
    "            #     ]\n",
    "            # )\n",
    "            self._loso_edge_weights = [item for sublist in [self._loso_edge_weights_dict[patient] for patient in self._loso_edge_weights_dict.keys()] for item in sublist]\n",
    "            print(f\"Edges loso weights shape: {len(self._loso_edge_weights)}\")\n",
    "            del self._loso_edge_weights_dict\n",
    "            \n",
    "            self._loso_edges = [item for sublist in [self._loso_edges_dict[patient] for patient in self._loso_edges_dict.keys()] for item in sublist]\n",
    "            del self._loso_edges_dict\n",
    "            print(f\"Edges loso shape: {len(self._loso_edges)}\")\n",
    "            self._loso_patient_number = np.concatenate(\n",
    "                [\n",
    "                    self._loso_patient_number_dict[key]\n",
    "                    for key in self._loso_patient_number_dict.keys()\n",
    "                ]\n",
    "            )\n",
    "            del self._loso_patient_number_dict\n",
    "\n",
    "    def _balance_classes(self):\n",
    "        \"\"\"Method to balance classes in the dataset by removing samples from the majority class.\n",
    "        Currently works only for interictal and ictal classes.\"\"\"\n",
    "        negative_label = self._label_counts[0]\n",
    "        positive_label = self._label_counts[1]\n",
    "\n",
    "        print(f\"Number of negative samples pre removal {negative_label}\")\n",
    "        print(f\"Number of positive samples pre removal {positive_label}\")\n",
    "        imbalance = negative_label - positive_label\n",
    "        print(f\"imbalance {imbalance}\")\n",
    "        negative_indices = np.where(self._labels == 0)[0]\n",
    "        indices_to_discard = np.random.choice(\n",
    "            negative_indices, size=imbalance, replace=False\n",
    "        )\n",
    "\n",
    "        self._features = np.delete(self._features, obj=indices_to_discard, axis=0)\n",
    "        self._labels = np.delete(self._labels, obj=indices_to_discard, axis=0)\n",
    "        self._time_labels = np.delete(self._time_labels, obj=indices_to_discard, axis=0)\n",
    "        self._edge_weights = np.delete(\n",
    "            self._edge_weights, obj=indices_to_discard, axis=0\n",
    "        )\n",
    "        self._patient_number = np.delete(\n",
    "            self._patient_number, obj=indices_to_discard, axis=0\n",
    "        )\n",
    "\n",
    "    def _standardize_data(self, features, labels, loso_features=None):\n",
    "        \"\"\"Standardize features by subtracting mean and dividing by standard deviation.\n",
    "        The mean and std are computed from the interictal class. The same values are used for loso_features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "            loso_features (optional): (np.ndarray) Array with features for LOSO patient.\n",
    "        \"\"\"\n",
    "        indexes = np.where(labels == 0)[0]\n",
    "        features_negative = features[indexes]\n",
    "        channel_mean = features_negative.mean()\n",
    "        channel_std = features_negative.std()\n",
    "        for i in range(features.shape[0]):\n",
    "            for n in range(features.shape[1]):\n",
    "                features[i, n, :] = (features[i, n, :] - channel_mean) / channel_std\n",
    "        if (\n",
    "            loso_features is not None\n",
    "        ):  ## standardize loso features with the same values as for training data\n",
    "            for i in range(loso_features.shape[0]):\n",
    "                for n in range(loso_features.shape[1]):\n",
    "                    loso_features[i, n, :] = (\n",
    "                        loso_features[i, n, :] - channel_mean\n",
    "                    ) / channel_std\n",
    "\n",
    "    def _min_max_scale(self, features, labels, loso_features=None):\n",
    "        \"\"\"Min max scale features to range [0,1]. The min and max values are computed from the interictal class.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        \"\"\"\n",
    "        indexes = np.where(labels == 0)[0]\n",
    "        features_negative = features[indexes]\n",
    "\n",
    "        channel_min = features_negative.min()\n",
    "        channel_max = features_negative.max()\n",
    "        for i in range(features.shape[0]):\n",
    "            for n in range(features.shape[1]):\n",
    "                features[i, n, :] = (features[i, n, :] - channel_min) / (\n",
    "                    channel_max - channel_min\n",
    "                )\n",
    "        if loso_features is not None:\n",
    "            for i in range(loso_features.shape[0]):\n",
    "                for n in range(loso_features.shape[1]):\n",
    "                    loso_features[i, n, :] = (loso_features[i, n, :] - channel_min) / (\n",
    "                        channel_max - channel_min\n",
    "                    )\n",
    "\n",
    "    def _apply_smote(self, features, labels):\n",
    "        \"\"\"Performs SMOTE oversampling on the dataset. Implemented for preictal vs ictal scenarion only.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            x_train_smote: (np.ndarray) Array with SMOTE oversampled features.\n",
    "            y_train_smote: (np.ndarray) Array with SMOTE oversampled labels.\n",
    "        \"\"\"\n",
    "        dim_1, dim_2, dim_3 = features.shape\n",
    "\n",
    "        new_dim = dim_1 * dim_2\n",
    "        new_x_train = features.reshape(new_dim, dim_3)\n",
    "        new_y_train = []\n",
    "        for i in range(len(labels)):\n",
    "            new_y_train.extend([labels[i]] * dim_2)\n",
    "\n",
    "        new_y_train = np.array(new_y_train)\n",
    "\n",
    "        # transform the dataset\n",
    "        oversample = SMOTE(random_state=42)\n",
    "        x_train, y_train = oversample.fit_resample(new_x_train, new_y_train)\n",
    "        x_train_smote = x_train.reshape(int(x_train.shape[0] / dim_2), dim_2, dim_3)\n",
    "        y_train_smote = []\n",
    "        for i in range(int(x_train.shape[0] / dim_2)):\n",
    "            # print(i)\n",
    "            value_list = list(y_train.reshape(int(x_train.shape[0] / dim_2), dim_2)[i])\n",
    "            # print(list(set(value_list)))\n",
    "            y_train_smote.extend(list(set(value_list)))\n",
    "            ## Check: if there is any different value in a list\n",
    "            if len(set(value_list)) != 1:\n",
    "                print(\n",
    "                    \"\\n\\n********* STOP: THERE IS SOMETHING WRONG IN TRAIN ******\\n\\n\"\n",
    "                )\n",
    "        y_train_smote = np.array(y_train_smote)\n",
    "        # print(np.unique(y_train_smote,return_counts=True))\n",
    "        return x_train_smote, y_train_smote\n",
    "\n",
    "    def _compute_tsfresh_features(self, features):\n",
    "        \"\"\"Compute tsfresh features for given features. The features are computed for each channel in every sample.\n",
    "        Currently the extracted features are hardcoded into the method.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with tsfresh features.\n",
    "        \"\"\"\n",
    "        fc_parameters = {\n",
    "            \"abs_energy\": None,\n",
    "            \"absolute_sum_of_changes\": None,\n",
    "            \"agg_autocorrelation\": [{\"f_agg\": \"mean\", \"maxlag\": 10}],\n",
    "            \"autocorrelation\": [{\"lag\": 10}],\n",
    "            \"number_peaks\": [{\"n\": 5}],\n",
    "            \"c3\": [{\"lag\": 10}],\n",
    "            \"cid_ce\": [{\"normalize\": True}],\n",
    "            \"longest_strike_below_mean\": None,\n",
    "            \"longest_strike_above_mean\": None,\n",
    "            \"fourier_entropy\": [{\"bins\": 10}],\n",
    "            \"mean_change\": None,\n",
    "            \"number_crossing_m\": [{\"m\": 0}],\n",
    "            \"sample_entropy\": None,\n",
    "            \"variance\": None,\n",
    "            \"variation_coefficient\": None,\n",
    "        }\n",
    "        n_variables = len(fc_parameters.keys())\n",
    "        n_nodes = features[0].shape[0]\n",
    "        for n, case in enumerate(features):\n",
    "            new_dataframe = pd.DataFrame(columns=[\"id\", \"time\", \"channel\", \"value\"])\n",
    "            new_dataframe[\"id\"] = np.repeat(n, case.shape[0] * case.shape[1])\n",
    "            new_dataframe[\"time\"] = np.tile(np.arange(case.shape[1]), case.shape[0])\n",
    "            new_dataframe[\"channel\"] = np.stack(\n",
    "                [torch.full([case.shape[1]], i) for i in range(case.shape[0])]\n",
    "            ).flatten()\n",
    "            new_dataframe[\"value\"] = abs(case.flatten())\n",
    "            extracted_features = extract_features(\n",
    "                new_dataframe,\n",
    "                column_id=\"id\",\n",
    "                column_sort=\"time\",\n",
    "                column_kind=\"channel\",\n",
    "                column_value=\"value\",\n",
    "                default_fc_parameters=fc_parameters,\n",
    "                n_jobs=6,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                final_df = pd.concat([final_df, extracted_features], axis=0)\n",
    "            except:\n",
    "                final_df = extracted_features\n",
    "        new_features = final_df.to_numpy().reshape(-1, n_nodes, n_variables)\n",
    "        return new_features\n",
    "\n",
    "    def _extend_data(\n",
    "        self,\n",
    "        patient,\n",
    "        patient_number,\n",
    "        features,\n",
    "        labels,\n",
    "        edges,\n",
    "        time_labels=None,\n",
    "        edge_weights=None,\n",
    "    ):\n",
    "        \"\"\"Convenience method to extend the dictionaries with features, labels, time labels and edge weights.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to extend the dictionaries for.\n",
    "            patient_number: (int) Patient number to extend the dictionaries for.\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "            time_labels (optional): (np.ndarray) Array with time labels.\n",
    "            edge_weights (optional): (np.ndarray) Array with edge weights.\n",
    "        \"\"\"\n",
    "        if patient == self.loso_patient:\n",
    "            # logging.info(f\"Adding recording {record} of patient {patient}\")\n",
    "            try:\n",
    "                self._loso_features_dict[patient] = np.concatenate(\n",
    "                    (self._loso_features_dict[patient], features), axis=0\n",
    "                )\n",
    "                self._loso_labels_dict[patient] = np.concatenate(\n",
    "                    (self._loso_labels_dict[patient], labels), axis=0\n",
    "                )\n",
    "                # self._loso_time_labels_dict[patient] = np.concatenate(\n",
    "                #     (self._loso_time_labels_dict[patient], time_labels), axis=0\n",
    "                # )\n",
    "                self._loso_edge_weights_dict[patient].extend(edge_weights)\n",
    "                # self._loso_edge_weights_dict[patient] = np.concatenate(\n",
    "                #     (\n",
    "                #         self._loso_edge_weights_dict[patient],\n",
    "                #         np.repeat(edge_weights, features.shape[0], axis=0),\n",
    "                #     )\n",
    "                # )\n",
    "                self._loso_edges_dict[patient].extend(edges)\n",
    "                self._loso_patient_number_dict[patient] = np.concatenate(\n",
    "                    (self._loso_patient_number_dict[patient], patient_number)\n",
    "                )\n",
    "            except:\n",
    "                self._loso_features_dict[patient] = features\n",
    "                self._loso_labels_dict[patient] = labels\n",
    "                # self._loso_time_labels_dict[patient] = time_labels\n",
    "                self._loso_edge_weights_dict[patient] = edge_weights\n",
    "                self._loso_edges_dict[patient] = edges\n",
    "                # self._loso_edge_weights_dict[patient] = np.repeat(\n",
    "                #     edge_weights, features.shape[0], axis=0\n",
    "                # )\n",
    "                self._loso_patient_number_dict[patient] = patient_number\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                self._features_dict[patient] = np.concatenate(\n",
    "                    (self._features_dict[patient], features), axis=0\n",
    "                )\n",
    "                self._labels_dict[patient] = np.concatenate(\n",
    "                    (self._labels_dict[patient], labels), axis=0\n",
    "                )\n",
    "                # self._time_labels_dict[patient] = np.concatenate(\n",
    "                #     (self._time_labels_dict[patient], time_labels), axis=0\n",
    "                # )\n",
    "                self._edge_weights_dict[patient].extend(edge_weights)\n",
    "                # self._edge_weights_dict[patient] = np.concatenate(\n",
    "                #     (\n",
    "                #         self._edge_weights_dict[patient],\n",
    "                #         np.repeat(edge_weights, features.shape[0], axis=0),\n",
    "                #     )\n",
    "                # )\n",
    "                self._edges_dict[patient].extend(edges)\n",
    "                self._patient_number_dict[patient] = np.concatenate(\n",
    "                    (self._patient_number_dict[patient], patient_number)\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                self._features_dict[patient] = features\n",
    "                self._labels_dict[patient] = labels\n",
    "                # self._time_labels_dict[patient] = time_labels\n",
    "                self._edge_weights_dict[patient] = edge_weights\n",
    "                # self._edge_weights_dict[patient] = np.repeat(\n",
    "                #     edge_weights, features.shape[0], axis=0\n",
    "                # )\n",
    "                self._edges_dict[patient] = edges\n",
    "                self._patient_number_dict[patient] = patient_number\n",
    "       \n",
    "    def _get_labels_features_edge_weights_seizure(self, patient):\n",
    "        \"\"\"Method to extract features, labels and edge weights for seizure and interictal samples.\"\"\"\n",
    "\n",
    "        event_tables = self._get_event_tables(\n",
    "            patient\n",
    "        )  # extract start and stop of seizure for patient\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if \"seizures\" in recording\n",
    "        ]\n",
    "        for record in recording_list:  # iterate over recordings for a patient\n",
    "            recording_path = os.path.join(patient_path, record)\n",
    "            record = record.replace(\n",
    "                \"seizures_\", \"\"\n",
    "            )  ## some magic to get it properly working with event tables\n",
    "            record_id = record.split(\".npy\")[0]  #  get record id\n",
    "            start_event_tables = self._get_recording_events(\n",
    "                event_tables[0], record_id\n",
    "            )  # get start events\n",
    "            stop_event_tables = self._get_recording_events(\n",
    "                event_tables[1], record_id\n",
    "            )  # get stop events\n",
    "            data_array = np.load(recording_path)  # load the recording\n",
    "\n",
    "            # plv_edge_weights = np.expand_dims(\n",
    "            #     self._get_edge_weights_recording(\n",
    "            #         np.load(os.path.join(self.plv_values_path, patient, record))\n",
    "            #     ),\n",
    "            #     axis=0,\n",
    "            # )\n",
    "\n",
    "            features, labels, time_labels = utils.extract_training_data_and_labels(\n",
    "                data_array,\n",
    "                start_event_tables,\n",
    "                stop_event_tables,\n",
    "                fs=self.sampling_f,\n",
    "                seizure_lookback=self.seizure_lookback,\n",
    "                sample_timestep=self.sample_timestep,\n",
    "                preictal_overlap=self.preictal_overlap,\n",
    "                ictal_overlap=self.ictal_overlap,\n",
    "                buffer_time=self.buffer_time,\n",
    "            )\n",
    "\n",
    "            if features is None:\n",
    "                print(\n",
    "                    f\"Skipping the recording {record} patients {patient} cuz features are none\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            features = features.squeeze(2)\n",
    "            conn_matrix_list = [utils.compute_spect_corr_matrix(feature,self.sampling_f)for feature in features]\n",
    "            edges_and_weights = [self._create_edge_idx_and_attributes(conn_matrix) for conn_matrix in conn_matrix_list]\n",
    "            edge_idx, edge_weights = zip(*edges_and_weights)\n",
    "            edge_idx = list(edge_idx)\n",
    "            edge_weights = list(edge_weights)\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = scipy.signal.resample(features, new_sample_count, axis=2)\n",
    "            if self.fft:\n",
    "                features = np.fft.rfft(features, axis=2)\n",
    "            if self.smote:\n",
    "                features, labels = self._apply_smote(features, labels)\n",
    "           # time_labels = np.expand_dims(time_labels.astype(np.int32), 1)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "            patient_number = torch.full(\n",
    "                [labels.shape[0]],\n",
    "                int(\"\".join(x for x in patient if x.isdigit())),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            self._extend_data(patient, patient_number, features, labels, edges=edge_idx , edge_weights=edge_weights)\n",
    "\n",
    "            \n",
    "    def _get_labels_features_edge_weights_interictal(\n",
    "        self, patient, samples_patient: int = None\n",
    "    ):\n",
    "        \"\"\"Method to extract features, labels and edge weights for interictal samples.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to extract the data for.\n",
    "            samples_patient (optional): (int) Number of samples to extract for a patient.\n",
    "        Samples are extracted from non-seizure recordings for a patient, starting from random time point.\n",
    "        If not specified, the number of samples is calculated as the number of interictal samples for a patient\n",
    "        divided by the number of recordings for a patient.\n",
    "\n",
    "        \"\"\"\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        ## get all non-seizure recordings for a patient\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if not \"seizures_\" in recording\n",
    "        ]\n",
    "        if samples_patient is None:\n",
    "            patient_num = int(\"\".join(filter(str.isdigit, patient)))\n",
    "            if patient == self.loso_patient:\n",
    "                patient_negatives = np.unique(\n",
    "                    self._loso_labels_dict[patient], return_counts=True\n",
    "                )[1][0]\n",
    "                samples_per_recording = int(patient_negatives / len(recording_list))\n",
    "            else:\n",
    "                patient_negatives = np.unique(\n",
    "                    self._labels_dict[patient], return_counts=True\n",
    "                )[1][0]\n",
    "                samples_per_recording = int(patient_negatives / len(recording_list))\n",
    "        else:\n",
    "            samples_per_recording = int(samples_patient / len(recording_list))\n",
    "        for recording in recording_list:\n",
    "            recording_path = os.path.join(patient_path, recording)\n",
    "            data_array = np.expand_dims(np.load(recording_path), 1)\n",
    "            try:\n",
    "                features, labels = utils.extract_training_data_and_labels_interictal(\n",
    "                    input_array=data_array,\n",
    "                    samples_per_recording=samples_per_recording,\n",
    "                    fs=self.sampling_f,\n",
    "                    timestep=self.sample_timestep,\n",
    "                    overlap=self.inter_overlap,\n",
    "                )\n",
    "            except:\n",
    "                print(f\"Skipping recording {recording} for patient due to the error\")\n",
    "                continue\n",
    "            idx_to_delete = np.where(\n",
    "                np.array([np.diff(feature, axis=-1).mean() for feature in features])\n",
    "                == 0\n",
    "            )[0]\n",
    "            if len(idx_to_delete) > 0:\n",
    "                features = np.delete(features, obj=idx_to_delete, axis=0)\n",
    "                labels = np.delete(labels, obj=idx_to_delete, axis=0)\n",
    "                print(\n",
    "                    f\"Deleted {len(idx_to_delete)} samples from patient {patient} \\n recording {recording} due to zero variance\"\n",
    "                )\n",
    "            patient_number = torch.full(\n",
    "                [labels.shape[0]],\n",
    "                patient_num,\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            features = features.squeeze(2)\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = scipy.signal.resample(features, new_sample_count, axis=2)\n",
    "            if self.fft:\n",
    "                features = np.fft.rfft(features, axis=2)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "            self._extend_data(patient, patient_number, features, labels)\n",
    "\n",
    "    def _update_classes(self):\n",
    "        \"\"\"Method to remove samples of period that we do not want to load, as specified in used_classes_dict.\n",
    "        If it is possible, the method aims set the interictal period as class 0 to be used for extracting normalization parameters.\n",
    "        If it is not possible, preictal period remains chosen as class 0.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            not self.used_classes_dict[\"ictal\"]\n",
    "            or not self.used_classes_dict[\"preictal\"]\n",
    "        ):\n",
    "            label_to_delete = 0 if self.used_classes_dict[\"ictal\"] else 1\n",
    "            idx_to_delete = np.where(self._labels == label_to_delete)[0]\n",
    "            self._features = np.delete(self._features, obj=idx_to_delete, axis=0)\n",
    "            self._labels = np.delete(self._labels, obj=idx_to_delete, axis=0)\n",
    "            self._patient_number = np.delete(\n",
    "                self._patient_number, obj=idx_to_delete, axis=0\n",
    "            )\n",
    "            ## change labels of remaining classes\n",
    "            if label_to_delete == 0:\n",
    "                self._labels[self._labels == 2] = 0\n",
    "                print(\n",
    "                    \"Deleted preictal samples, changed interictal label to 0,  ictal remains 1 \"\n",
    "                )\n",
    "            else:\n",
    "                self._labels[self._labels == 0] = 1\n",
    "                self._labels[self._labels == 2] = 0\n",
    "                print(\n",
    "                    \"Deleted ictal samples, changed interictal label to 0, preictal to 1\"\n",
    "                )\n",
    "            if self.loso_patient is not None:\n",
    "                idx_to_delete = np.where(self._loso_labels == label_to_delete)[0]\n",
    "                self._loso_features = np.delete(\n",
    "                    self._loso_features, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                self._loso_labels = np.delete(\n",
    "                    self._loso_labels, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                self._loso_patient_number = np.delete(\n",
    "                    self._loso_patient_number, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                if label_to_delete == 0:\n",
    "                    self._loso_labels[self._loso_labels == 2] = 0\n",
    "                    print(\n",
    "                        \"Deleted preictal samples from LOSO patient, changed interictal label to 0, ictal remains 1 \"\n",
    "                    )\n",
    "                else:\n",
    "                    self._loso_labels[self._loso_labels == 0] = 1\n",
    "                    self._loso_labels[self._loso_labels == 2] = 0\n",
    "                    print(\n",
    "                        \"Deleted ictal from LOSO patient, changed interictal label to 0, preictal to 1\"\n",
    "                    )\n",
    "        elif (\n",
    "            sum(self.used_classes_dict.values()) == 3\n",
    "        ):  ## case when all three classes are used - just flipping labels\n",
    "            self._labels[\n",
    "                self._labels == 2\n",
    "            ] = 4  ## change interictal to 4 from 2 temporarily\n",
    "            self._labels[self._labels == 0] = 2  ## change preictal to 2 from 0\n",
    "            self._labels[self._labels == 4] = 0  ## change interictal to 0 from 4\n",
    "            if self.loso_patient is not None:\n",
    "                self._loso_labels[self._loso_labels == 2] = 4\n",
    "                self._loso_labels[self._loso_labels == 0] = 2\n",
    "                self._loso_labels[self._loso_labels == 4] = 0\n",
    "\n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes\n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of\n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        assert (\n",
    "            \"interictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for interictal class in used_classes_dict\"\n",
    "        assert (\n",
    "            \"preictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for preictal class in used_classes_dict\"\n",
    "        assert (\n",
    "            \"ictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for ictal class in used_classes_dict\"\n",
    "\n",
    "        assert (\n",
    "            sum(self.used_classes_dict.values()) > 1\n",
    "        ), \"Please define at least two classes to use in used_classes_dict\"\n",
    "\n",
    "        self._initialize_dicts()\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        start_time = time.time()\n",
    "        if self.smote:\n",
    "            for patient in patient_list:\n",
    "                self._get_labels_features_edge_weights_seizure(patient)\n",
    "        else:\n",
    "            Parallel(n_jobs=6, require=\"sharedmem\")(\n",
    "                delayed(self._get_labels_features_edge_weights_seizure)(patient)\n",
    "                for patient in patient_list\n",
    "            )\n",
    "        print(\n",
    "            f\"Finished reading in {time.time() - start_time} seconds for seizure data\"\n",
    "        )\n",
    "        if self.used_classes_dict[\"interictal\"]:\n",
    "            Parallel(n_jobs=6, require=\"sharedmem\")(\n",
    "                delayed(self._get_labels_features_edge_weights_interictal)(patient)\n",
    "                for patient in patient_list\n",
    "            )\n",
    "        self._convert_dict_to_array()\n",
    "        self._update_classes()\n",
    "        self._get_labels_count()\n",
    "\n",
    "        if self.balance:\n",
    "            self._balance_classes()\n",
    "        print(\n",
    "            f\"Finished reading in {time.time() - start_time} seconds for non seizure data\"\n",
    "        )\n",
    "        start_time_preprocessing = time.time()\n",
    "        if self.rescale:\n",
    "            self._features *= (\n",
    "                self._features * 1e6\n",
    "            )  ## rescale to back to volts, numeric stability problems\n",
    "            if self.loso_patient is not None:\n",
    "                self._loso_features *= self._loso_features * 1e6\n",
    "        self._standardize_data(self._features, self._labels, self._loso_features)\n",
    "        if self.tsfresh:\n",
    "            self._features = self._compute_tsfresh_features(self._features)\n",
    "            if self.loso_patient is not None:\n",
    "                self._loso_features = self._compute_tsfresh_features(self._loso_features)\n",
    "        if self.hjorth:\n",
    "            self._features = self._calculate_hjorth_features(self._features)\n",
    "            if self.loso_patient is not None:\n",
    "                self._loso_features = self._calculate_hjorth_features(self._loso_features)\n",
    "        #self._get_edges()\n",
    "        self._array_to_tensor()\n",
    "\n",
    "        if self.train_test_split is not None:\n",
    "            if self.fft or self.hjorth:\n",
    "                data_list = self._features_to_data_list(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    self._labels,\n",
    "                    self._edge_weights,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                train_data_list, val_data_list = self._split_data_list(data_list)\n",
    "                label_count = np.unique(\n",
    "                    [data.y.item() for data in train_data_list], return_counts=True\n",
    "                )[1]\n",
    "                self.alpha = label_count[0] / label_count[1]\n",
    "                loaders = [\n",
    "                    DataLoader(\n",
    "                        train_data_list,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                    DataLoader(\n",
    "                        val_data_list,\n",
    "                        batch_size=len(val_data_list),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                train_dataset = torch.utils.data.TensorDataset(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                    train_dataset,\n",
    "                    [1 - self.train_test_split, self.train_test_split],\n",
    "                    generator=torch.Generator().manual_seed(42),\n",
    "                )\n",
    "\n",
    "                train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "\n",
    "                val_dataloader = torch.utils.data.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=False,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "                loaders = [train_dataloader, val_dataloader]\n",
    "        else:\n",
    "            if self.fft or self.hjorth:\n",
    "                train_data_list = self._features_to_data_list(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    self._labels,\n",
    "                    self._edge_weights,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                loaders = [\n",
    "                    DataLoader(\n",
    "                        train_data_list,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                train_dataset = torch.utils.data.TensorDataset(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "                loaders = [train_dataloader]\n",
    "        if self.loso_patient:\n",
    "            if self.fft or self.hjorth:\n",
    "                loso_data_list = self._features_to_data_list(\n",
    "                    self._loso_features,\n",
    "                    self._loso_edges,\n",
    "                    self._loso_labels,\n",
    "                    self._loso_edge_weights,\n",
    "                    # self._loso_time_labels,\n",
    "                )\n",
    "                print(\"Preprocessing time: \", time.time() - start_time_preprocessing)\n",
    "                return (\n",
    "                    *loaders,\n",
    "                    DataLoader(\n",
    "                        loso_data_list,\n",
    "                        batch_size=len(loso_data_list),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                )\n",
    "            loso_dataset = torch.utils.data.TensorDataset(\n",
    "                self._loso_features,\n",
    "                self._loso_edges,\n",
    "                self._loso_labels,\n",
    "                self._loso_edge_weights,\n",
    "                #  self._loso_time_labels,\n",
    "            )\n",
    "            loso_dataloader = torch.utils.data.DataLoader(\n",
    "                loso_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            return (*loaders, loso_dataloader)\n",
    "\n",
    "        return (*loaders,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "TIMESTEP = 6\n",
    "PREICTAL_OVERLAP = 0\n",
    "ICTAL_OVERLAP = 0\n",
    "INTER_OVERLAP = 0\n",
    "SFREQ = 256\n",
    "torch_geometric.seed_everything(42)\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('data/npy_data_full'),\n",
    "    event_tables_path=Path('data/event_tables'),\n",
    "    plv_values_path=Path('data/plv_arrays'),\n",
    "    loso_patient='chb06',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    inter_overlap=INTER_OVERLAP,\n",
    "    preictal_overlap=PREICTAL_OVERLAP,\n",
    "    ictal_overlap=ICTAL_OVERLAP,\n",
    "    self_loops=False,\n",
    "    balance=False,\n",
    "    train_test_split=0.05,\n",
    "    fft=False,\n",
    "    hjorth=True,\n",
    "    downsample=60,\n",
    "    batch_size=64,\n",
    "    buffer_time=60,\n",
    "    smote=False,\n",
    "    tsfresh=False,\n",
    "    rescale=False,\n",
    "    used_classes_dict={\"ictal\": True, \"interictal\": False, \"preictal\": True}\n",
    "    )\n",
    "train_loader,valid_loader, loso_loader =dataloader.get_dataset() \n",
    "alpha = list(dataloader._label_counts.values())[0]/list(dataloader._label_counts.values())[1]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.unique(dataloader._labels.int().squeeze()),\n",
    "    y=dataloader._labels.int().squeeze().numpy(),\n",
    ")\n",
    "class_weights = torch.from_numpy(class_weights).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, GINConv, GINEConv\n",
    "from torchmetrics import (\n",
    "    Specificity,\n",
    "    Recall,\n",
    "    F1Score,\n",
    "    AUROC,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_device = torch.device(device)\n",
    "precision = \"bf16-mixed\" if device == \"cpu\" else \"16-mixed\"\n",
    "strategy = pl.strategies.SingleDeviceStrategy(device=torch_device)\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=6, verbose=False, mode=\"min\"\n",
    ")\n",
    "epochs = 10\n",
    "callbacks = [early_stopping]\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    precision=precision,\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    enable_progress_bar=True,\n",
    "    strategy=strategy,\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=1,\n",
    "    enable_model_summary=False,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "n_features = dataloader._features.shape[2]\n",
    "model = GATv2Lightning(n_features,n_classes=2)\n",
    "trainer.fit(model, train_loader, valid_loader)\n",
    "trainer.test(model,loso_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicitons for confusion matrix\"\"\"\n",
    "preds_loso = torch.cat(trainer.predict(model, dataloaders=loso_loader)).float()\n",
    "preds_loso = torch.nn.functional.softmax(preds_loso).cpu().numpy()\n",
    "ground_truth_loso = np.array([data.y.item() for data in loso_loader.dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "classes = ('Preictal', 'Ictal', 'Healthy')\n",
    "loso_preds_cm = np.argmax(preds_loso,axis=1)\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(ground_truth_loso, loso_preds_cm)\n",
    "df_cm = pd.DataFrame(cf_matrix , index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization tool, for experiments with different preprocessing methods\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "features = dataloader._features\n",
    "labels = dataloader._labels\n",
    "indexes_interictal = np.where(dataloader._labels == 0)[0]\n",
    "features_interictal = features[indexes_interictal]\n",
    "indexes_ictal = np.where(dataloader._labels == 1)[0]\n",
    "features_ictal = features[indexes_ictal]\n",
    "indexes_preictal = np.where(dataloader._labels == 2)[0]\n",
    "features_preictal = features[indexes_preictal]\n",
    "fig,ax = plt.subplots(18,1,figsize=(10,10))\n",
    "\n",
    "for i in range(18):\n",
    "    \n",
    "    ax[i].plot(features_interictal[12,i,:])\n",
    "    ax[i].plot(features_ictal[15,i,:])\n",
    "    ax[i].plot(features_preictal[5,i,:])\n",
    "    ax[i].set_title(f'Channel {i+1}')\n",
    "plt.legend(['interictal','ictal', 'preictal'])\n",
    "plt.show()\n",
    "\n",
    "features_loso = dataloader._loso_features\n",
    "labels_loso = dataloader._loso_labels\n",
    "indexes_interictal_loso = np.where(dataloader._loso_labels == 0)[0]\n",
    "features_interictal_loso = features_loso[indexes_interictal_loso]\n",
    "indexes_ictal_loso = np.where(dataloader._loso_labels == 1)[0]\n",
    "features_ictal_loso = features_loso[indexes_ictal_loso]\n",
    "indexes_preictal_loso = np.where(dataloader._loso_labels == 2)[0]\n",
    "features_preictal_loso = features_loso[indexes_preictal_loso]\n",
    "from matplotlib import pyplot as plt\n",
    "fig,ax = plt.subplots(18,1,figsize=(10,10))\n",
    "for i in range(18):\n",
    "    \n",
    "    ax[i].plot(features_interictal_loso[20,i,:])\n",
    "    ax[i].plot(features_ictal_loso[2,i,:])\n",
    "    ax[i].plot(features_preictal_loso[15,i,:])\n",
    "    ax[i].set_title(f'Channel {i+1}')\n",
    "plt.legend(['interictal', 'ictal','preictal'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cc438b9c90976695678f0d6c20e4c06983b5710e6855b5b4390f60ecf93fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
