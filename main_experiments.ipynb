{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "import utils\n",
    "from tsfresh import extract_features\n",
    "# from torch_geometric_temporal import  DynamicGraphTemporalSignal,StaticGraphTemporalSignal, temporal_signal_split, DynamicGraphTemporalSignalBatch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import scipy\n",
    "import sklearn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "# from torch_geometric_temporal.nn.recurrent import DCRNN,  GConvGRU, A3TGCN, TGCN2, TGCN, A3TGCN2\n",
    "# from torch_geometric_temporal.nn.attention import STConv\n",
    "from torchmetrics.classification import BinaryRecall, BinarySpecificity, AUROC, ROC\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv, BatchNorm, GATv2Conv\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from mne_features.univariate import (\n",
    "    compute_variance,\n",
    "    compute_hjorth_complexity,\n",
    "    compute_hjorth_mobility,\n",
    "    compute_line_length,\n",
    "    compute_higuchi_fd,\n",
    "    compute_katz_fd,\n",
    ")\n",
    "import mne_features\n",
    "import torch_geometric\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from librosa import zero_crossings\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "from statistics import mean\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think about using kwargs argument here to specify args for dataloader\n",
    "@dataclass\n",
    "class SeizureDataLoader:\n",
    "    npy_dataset_path: Path\n",
    "    event_tables_path: Path\n",
    "    plv_values_path: Path\n",
    "    loso_patient: str = None\n",
    "    sampling_f: int = 256\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    preictal_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    self_loops: bool = True\n",
    "    balance: bool = True\n",
    "    train_test_split: float = None\n",
    "    fft: bool = False\n",
    "    hjorth: bool = False\n",
    "    downsample: int = None\n",
    "    buffer_time: int = 15\n",
    "    batch_size: int = 32\n",
    "    smote: bool = False\n",
    "    tsfresh: bool = False\n",
    "    rescale: bool = False\n",
    "    used_classes_dict: dict[str] = field(default_factory=lambda: {\"interictal\": True, \"preictal\": True, \"ictal\": True})\n",
    "    \"\"\"Class to prepare dataloaders for eeg seizure perdiction from stored files.\n",
    "\n",
    "    Attributes:\n",
    "        npy_dataset_path: (Path) Path to directory with .npy files\n",
    "        event_tables_path: (Path) Path to directory with .csv files\n",
    "        plv_values_path: (Path) Path to directory with .npy files\n",
    "        loso_patient: (str) Patient name to be left out of training set.\n",
    "    If None, no patient is left out for testing. (default: None)\n",
    "        sampling_f: (int) Sampling frequency of the recordings. (default: 256)\n",
    "        seizure_lookback: (int) Time in seconds to look back from seizure onset. (default: 600)\n",
    "        sample_timestep: (int) Time in seconds between samples. (default: 5)\n",
    "        inter_overlap: (int) Time in seconds to overlap between interictal samples. (default: 0)\n",
    "        preictal_overlap: (int) Time in seconds to overlap between preictal samples. (default: 0)\n",
    "        ictal_overlap: (int) Time in seconds to overlap between ictal samples. (default: 0)\n",
    "        self_loops: (bool) Whether to add self loops to the graph. (default: True)\n",
    "        balance: (bool) Whether to balance the classes. (default: True)\n",
    "        train_test_split: (float) Percentage of data to be used for testing. (default: None)\n",
    "        fft: (bool) Whether to use fft features. (default: False)\n",
    "        hjorth: (bool) Whether to use hjorth features. (default: False)\n",
    "        downsample: (int) Factor by which to downsample the data. (default: None)\n",
    "        buffer_time: (int) Time in seconds to skip before and after every sample from seizure period. \n",
    "    (default: 15)\n",
    "        batch_size: (int) Batch size for dataloaders. (default: 32)\n",
    "        smote: (bool) Whether to use smote to balance the classes. (default: False)\n",
    "        used_classes_ditct: (dict) Dictionary with classes to be used. \n",
    "    (default: {'interictal': True, 'preictal': True, 'ictal': True})\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # if used_classes_dict is None:\n",
    "    #     used_classes_dict = {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    assert (fft and hjorth) == False, \"When fft is True, hjorth should be False\"\n",
    "    assert (downsample is None) or (\n",
    "        downsample > 0\n",
    "    ), \"Downsample should be None or positive integer\"\n",
    "    assert (train_test_split is None) or (\n",
    "        train_test_split > 0 and train_test_split < 1\n",
    "    ), \"Train test split should be None or float between 0 and 1\"\n",
    "    \n",
    "    assert not (smote and balance), \"Cannot use smote and balance at the same time\"\n",
    "    assert not ((fft or hjorth) and tsfresh), \"Cannot use fft or hjorth and tsfresh at the same time\"\n",
    "\n",
    "    def _get_event_tables(self, patient_name: str) -> tuple[dict, dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\n",
    "        Args:\n",
    "            patient_name: (str) Name of the patient to get events for.\n",
    "        Returns:\n",
    "            start_events_dict: (dict) Dictionary with start events for given patient.\n",
    "            stop_events_dict: (dict) Dictionary with stop events for given patient.\n",
    "        \"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [\n",
    "            os.path.join(self.event_tables_path, ev_table)\n",
    "            for ev_table in event_table_list\n",
    "            if patient_name in ev_table\n",
    "        ]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[\n",
    "            0\n",
    "        ]  ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict(\"index\")\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict(\"index\")\n",
    "        return start_events_dict, stop_events_dict\n",
    "\n",
    "    def _get_recording_events(self, events_dict, recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict.\n",
    "        Args:\n",
    "            events_dict: (dict) Dictionary with events for given patient.\n",
    "            recording: (str) Name of the recording to get events for.\n",
    "        Returns:\n",
    "            recording_events: (list) List of seizure event start and stop time for given recording.\n",
    "        \"\"\"\n",
    "        recording_list = list(events_dict[recording + \".edf\"].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "    def _get_graph(self, n_nodes: int) -> nx.Graph:\n",
    "        \"\"\"Creates Networx complete graph with self loops\n",
    "        for given number of nodes.\n",
    "        Args:\n",
    "            n_nodes: (int) Number of nodes in the graph.\n",
    "        Returns:\n",
    "            graph: (nx.Graph) Fully connected graph with self loops.\n",
    "        \"\"\"\n",
    "        graph = nx.complete_graph(n_nodes)\n",
    "        self_loops = [[node, node] for node in graph.nodes()]\n",
    "        graph.add_edges_from(self_loops)\n",
    "        return graph\n",
    "\n",
    "    def _get_edge_weights_recording(self, plv_values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Method for extracting PLV values associated with given edges for a recording.\n",
    "        The PLV was computed for the entire recroding for all channels when the recording was\n",
    "        processed.\n",
    "        Args:\n",
    "            plv_values: (np.ndarray) Array with PLV values for given recording.\n",
    "        Returns:\n",
    "            edge_weights: (np.ndarray) Array with PLV values for given edges.\n",
    "        \"\"\"\n",
    "        graph = self._get_graph(plv_values.shape[0])\n",
    "        garph_dict = {}\n",
    "        for edge in graph.edges():\n",
    "            e_start, e_end = edge\n",
    "            garph_dict[edge] = {\"plv\": plv_values[e_start, e_end]}\n",
    "        nx.set_edge_attributes(graph, garph_dict)\n",
    "        edge_weights = from_networkx(graph).plv.numpy()\n",
    "        return edge_weights\n",
    "\n",
    "    def _get_edges(self):\n",
    "        \"\"\"Method to assign edge attributes. Has to be called AFTER get_dataset() method.\"\"\"\n",
    "        graph = self._get_graph(self._features.shape[1])\n",
    "        edges = np.expand_dims(from_networkx(graph).edge_index.numpy(), axis=0)\n",
    "        edges_per_sample_train = np.repeat(\n",
    "            edges, repeats=self._features.shape[0], axis=0\n",
    "        )\n",
    "        self._edges = torch.tensor(edges_per_sample_train)\n",
    "        if self.loso_patient is not None:\n",
    "            edges_per_sample_val = np.repeat(\n",
    "                edges, repeats=self._val_features.shape[0], axis=0\n",
    "            )\n",
    "            self._val_edges = torch.tensor(edges_per_sample_val)\n",
    "\n",
    "    def _array_to_tensor(self):\n",
    "        \"\"\"Method converting features, edges and weights to torch.tensors\"\"\"\n",
    "\n",
    "        self._features = torch.from_numpy(self._features)\n",
    "        self._labels = torch.from_numpy(self._labels)\n",
    "        # self._time_labels = torch.from_numpy(self._time_labels)\n",
    "        # self._edge_weights = torch.from_numpy(self._edge_weights)\n",
    "        if self.loso_patient is not None:\n",
    "            self._val_features = torch.from_numpy(self._val_features)\n",
    "            self._val_labels = torch.from_numpy(self._val_labels)\n",
    "            # self._val_time_labels = torch.from_numpy(self._val_time_labels)\n",
    "            # self._val_edge_weights = torch.from_numpy(self._val_edge_weights)\n",
    "\n",
    "    def _get_labels_count(self):\n",
    "        \"\"\"Convenience method to get counts of labels in the dataset.\"\"\"\n",
    "        labels, counts = np.unique(self._labels, return_counts=True)\n",
    "        self._label_counts = {}\n",
    "        for n, label in enumerate(labels):\n",
    "            self._label_counts[int(label)] = counts[n]\n",
    "        if self.loso_patient is not None:\n",
    "            labels, counts = np.unique(self._val_labels, return_counts=True)\n",
    "            self._val_label_counts = {}\n",
    "            for n, label in enumerate(labels):\n",
    "                self._val_label_counts[int(label)] = counts[n]\n",
    "\n",
    "    def _calculate_hjorth_features(self, features):\n",
    "        \"\"\"Converting features to Hjorth features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features to be converted.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with Hjorth features.\n",
    "        \"\"\"\n",
    "        new_features = np.array(\n",
    "            [\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        np.expand_dims(compute_variance(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_mobility(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_complexity(feature), 1),\n",
    "                        np.expand_dims(compute_line_length(feature), 1),\n",
    "                        np.expand_dims(compute_katz_fd(feature), 1),\n",
    "                        np.expand_dims(compute_higuchi_fd(feature), 1),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for feature in features\n",
    "            ]\n",
    "        )\n",
    "        # new_mean = new_features.mean(axis=0)\n",
    "        # new_std = new_features.std(axis=0)\n",
    "        # new_features = (new_features - new_mean) / new_std\n",
    "        return new_features\n",
    "\n",
    "    def _features_to_data_list(self, features, edges, labels):\n",
    "        \"\"\"Converts features, edges and labels to list of torch_geometric.data.Data objects.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            edges: (np.ndarray) Array with edges.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        \"\"\"\n",
    "        data_list = [\n",
    "            Data(\n",
    "                x=features[i],\n",
    "                edge_index=edges[i],\n",
    "                # edge_attr=edge_weights[i],\n",
    "                y=labels[i],\n",
    "                # time=time_label[i],\n",
    "            )\n",
    "            for i in range(len(features))\n",
    "        ]\n",
    "        return data_list\n",
    "\n",
    "    def _split_data_list(self, data_list):\n",
    "        \"\"\"Methods for splitting list of torch_geometric.data.Data objects into train and validation sets.\n",
    "        Uses StratifiedShuffleSplit to ensure that the classes are balanced in both sets.\n",
    "        Args:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        Returns:\n",
    "            data_list_train: (list) List of torch_geometric.data.Data objects for training.\n",
    "            dataset_list_val: (list) List of torch_geometric.data.Data objects for validation.\n",
    "        \"\"\"\n",
    "        class_labels = torch.tensor(\n",
    "            [data.y.item() for data in data_list], dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "        patient_labels = torch.tensor(\n",
    "            np.expand_dims(self._patient_number, 1), dtype=torch.float32\n",
    "        )\n",
    "        class_labels_patient_labels = torch.cat([class_labels, patient_labels], dim=1)\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=self.train_test_split, random_state=42\n",
    "        )\n",
    "        train_indices, val_indices = next(\n",
    "            splitter.split(data_list, class_labels_patient_labels)\n",
    "        )\n",
    "        self._indexes_to_later_delete = {\"train\": train_indices, \"val\": val_indices}\n",
    "        data_list_train = [data_list[i] for i in train_indices]\n",
    "        dataset_list_val = [data_list[i] for i in val_indices]\n",
    "        return data_list_train, dataset_list_val\n",
    "\n",
    "    def _initialize_dicts(self):\n",
    "        \"\"\"Temporary method to initialize dictionaries for storing features, labels, etc.\n",
    "        Looks terrible, but convenient so far.\n",
    "        \"\"\"\n",
    "        self._features_dict = {}\n",
    "        self._labels_dict = {}\n",
    "        self._time_labels_dict = {}\n",
    "        self._edge_weights_dict = {}\n",
    "        self._patient_number_dict = {}\n",
    "        if self.loso_patient:\n",
    "            self._val_features_dict = {}\n",
    "            self._val_labels_dict = {}\n",
    "            self._val_time_labels_dict = {}\n",
    "            self._val_edge_weights_dict = {}\n",
    "            self._val_patient_number_dict = {}\n",
    "\n",
    "    def _convert_dict_to_array(self):\n",
    "        \"\"\"A method to convert dictionaries to numpy arrays. This approach with dicts is redundant,\n",
    "        but allows for joblib parallelization for data loading by not using concatenation in the loading loop.\n",
    "        \"\"\"\n",
    "        self._features = np.concatenate(\n",
    "            [self._features_dict[key] for key in self._features_dict.keys()]\n",
    "        )\n",
    "        del self._features_dict\n",
    "        self._labels = np.concatenate(\n",
    "            [self._labels_dict[key] for key in self._labels_dict.keys()]\n",
    "        )\n",
    "        del self._labels_dict\n",
    "        # self._time_labels = np.concatenate(\n",
    "        #     [self._time_labels_dict[key] for key in self._time_labels_dict.keys()]\n",
    "        # )\n",
    "        # del self._time_labels_dict\n",
    "        # self._edge_weights = np.concatenate(\n",
    "        #     [self._edge_weights_dict[key] for key in self._edge_weights_dict.keys()]\n",
    "        # )\n",
    "        # del self._edge_weights_dict\n",
    "        self._patient_number = np.concatenate(\n",
    "            [self._patient_number_dict[key] for key in self._patient_number_dict.keys()]\n",
    "        )\n",
    "        del self._patient_number_dict\n",
    "        if self.loso_patient:\n",
    "            self._val_features = np.concatenate(\n",
    "                [self._val_features_dict[key] for key in self._val_features_dict.keys()]\n",
    "            )\n",
    "            del self._val_features_dict\n",
    "            self._val_labels = np.concatenate(\n",
    "                [self._val_labels_dict[key] for key in self._val_labels_dict.keys()]\n",
    "            )\n",
    "            del self._val_labels_dict\n",
    "            # self._val_time_labels = np.concatenate(\n",
    "            #     [\n",
    "            #         self._val_time_labels_dict[key]\n",
    "            #         for key in self._val_time_labels_dict.keys()\n",
    "            #     ]\n",
    "            # )\n",
    "            # del self._val_time_labels_dict\n",
    "            # self._val_edge_weights = np.concatenate(\n",
    "            #     [\n",
    "            #         self._val_edge_weights_dict[key]\n",
    "            #         for key in self._val_edge_weights_dict.keys()\n",
    "            #     ]\n",
    "            # )\n",
    "            # del self._val_edge_weights_dict\n",
    "            self._val_patient_number = np.concatenate(\n",
    "                [\n",
    "                    self._val_patient_number_dict[key]\n",
    "                    for key in self._val_patient_number_dict.keys()\n",
    "                ]\n",
    "            )\n",
    "            del self._val_patient_number_dict\n",
    "\n",
    "    def _balance_classes(self):\n",
    "        \"\"\"Method to balance classes in the dataset by removing samples from the majority class.\n",
    "        Currently works only for interictal and ictal classes.\"\"\"\n",
    "        negative_label = self._label_counts[0]\n",
    "        positive_label = self._label_counts[1]\n",
    "\n",
    "        print(f\"Number of negative samples pre removal {negative_label}\")\n",
    "        print(f\"Number of positive samples pre removal {positive_label}\")\n",
    "        imbalance = negative_label - positive_label\n",
    "        print(f\"imbalance {imbalance}\")\n",
    "        negative_indices = np.where(self._labels == 0)[0]\n",
    "        indices_to_discard = np.random.choice(\n",
    "            negative_indices, size=imbalance, replace=False\n",
    "        )\n",
    "\n",
    "        self._features = np.delete(self._features, obj=indices_to_discard, axis=0)\n",
    "        self._labels = np.delete(self._labels, obj=indices_to_discard, axis=0)\n",
    "        self._time_labels = np.delete(self._time_labels, obj=indices_to_discard, axis=0)\n",
    "        self._edge_weights = np.delete(\n",
    "            self._edge_weights, obj=indices_to_discard, axis=0\n",
    "        )\n",
    "        self._patient_number = np.delete(\n",
    "            self._patient_number, obj=indices_to_discard, axis=0\n",
    "        )\n",
    "\n",
    "    def _standardize_data(self, features, labels, loso_features=None):\n",
    "        \"\"\"Standardize features by subtracting mean and dividing by standard deviation.\n",
    "        The mean and std are computed from the interictal class. The same values are used for loso_features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "            loso_features (optional): (np.ndarray) Array with features for LOSO patient.\n",
    "        \"\"\"\n",
    "        indexes = np.where(labels == 0)[0]\n",
    "        features_negative = features[indexes]\n",
    "        channel_mean = features_negative.mean()\n",
    "        channel_std = features_negative.std()\n",
    "        for i in range(features.shape[0]):\n",
    "            for n in range(features.shape[1]):\n",
    "                features[i, n, :] = (features[i, n, :] - channel_mean) / channel_std\n",
    "        if (\n",
    "            loso_features is not None\n",
    "        ):  ## standardize loso features with the same values as for training data\n",
    "            \n",
    "            for i in range(loso_features.shape[0]):\n",
    "                for n in range(loso_features.shape[1]):\n",
    "                    loso_features[i, n, :] = (\n",
    "                        loso_features[i, n, :] - channel_mean\n",
    "                    ) / channel_std\n",
    "\n",
    "    def _min_max_scale(self, features, labels, loso_features=None):\n",
    "        \"\"\"Min max scale features to range [0,1]. The min and max values are computed from the interictal class.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        \"\"\"\n",
    "        indexes = np.where(labels == 0)[0]\n",
    "        features_negative = features[indexes]\n",
    "\n",
    "        channel_min = features_negative.min()\n",
    "        channel_max = features_negative.max()\n",
    "        for i in range(features.shape[0]):\n",
    "            for n in range(features.shape[1]):\n",
    "                features[i, n, :] = (features[i, n, :] - channel_min) / (\n",
    "                    channel_max - channel_min\n",
    "                )\n",
    "        if loso_features is not None:\n",
    "            for i in range(loso_features.shape[0]):\n",
    "                for n in range(loso_features.shape[1]):\n",
    "                    loso_features[i, n, :] = (loso_features[i, n, :] - channel_min) / (\n",
    "                        channel_max - channel_min\n",
    "                    )\n",
    "\n",
    "    def _apply_smote(self, features, labels):\n",
    "        \"\"\"Performs SMOTE oversampling on the dataset. Implemented for preictal vs ictal scenarion only.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            x_train_smote: (np.ndarray) Array with SMOTE oversampled features.\n",
    "            y_train_smote: (np.ndarray) Array with SMOTE oversampled labels.\n",
    "        \"\"\"\n",
    "        dim_1, dim_2, dim_3 = features.shape\n",
    "\n",
    "        new_dim = dim_1 * dim_2\n",
    "        new_x_train = features.reshape(new_dim, dim_3)\n",
    "        new_y_train = []\n",
    "        for i in range(len(labels)):\n",
    "            new_y_train.extend([labels[i]] * dim_2)\n",
    "\n",
    "        new_y_train = np.array(new_y_train)\n",
    "\n",
    "        # transform the dataset\n",
    "        oversample = SMOTE(random_state=42)\n",
    "        x_train, y_train = oversample.fit_resample(new_x_train, new_y_train)\n",
    "        x_train_smote = x_train.reshape(int(x_train.shape[0] / dim_2), dim_2, dim_3)\n",
    "        y_train_smote = []\n",
    "        for i in range(int(x_train.shape[0] / dim_2)):\n",
    "            # print(i)\n",
    "            value_list = list(y_train.reshape(int(x_train.shape[0] / dim_2), dim_2)[i])\n",
    "            # print(list(set(value_list)))\n",
    "            y_train_smote.extend(list(set(value_list)))\n",
    "            ## Check: if there is any different value in a list\n",
    "            if len(set(value_list)) != 1:\n",
    "                print(\n",
    "                    \"\\n\\n********* STOP: THERE IS SOMETHING WRONG IN TRAIN ******\\n\\n\"\n",
    "                )\n",
    "        y_train_smote = np.array(y_train_smote)\n",
    "        # print(np.unique(y_train_smote,return_counts=True))\n",
    "        return x_train_smote, y_train_smote\n",
    "\n",
    "    def _compute_tsfresh_features(self, features):\n",
    "        \"\"\"Compute tsfresh features for given features. The features are computed for each channel in every sample.\n",
    "        Currently the extracted features are hardcoded into the method.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with tsfresh features.\n",
    "        \"\"\"\n",
    "        fc_parameters = {\n",
    "            \"abs_energy\" : None,\n",
    "            \"absolute_sum_of_changes\" : None,\n",
    "            \"agg_autocorrelation\" : [{\"f_agg\": \"mean\", \"maxlag\": 10}],\n",
    "            \"autocorrelation\" : [{\"lag\": 10}],\n",
    "            \"number_peaks\" : [{\"n\":5}],\n",
    "            \"c3\" : [{\"lag\": 10}],\n",
    "            \"cid_ce\" : [{\"normalize\": True}],\n",
    "            \"longest_strike_below_mean\" : None,\n",
    "            \"longest_strike_above_mean\" : None,\n",
    "            \"fourier_entropy\" : [{\"bins\": 10}],\n",
    "            \"mean_change\" : None,\n",
    "            \"number_crossing_m\" : [{\"m\": 0}],\n",
    "            \"sample_entropy\" : None,\n",
    "            \"variance\" : None,\n",
    "            \"variation_coefficient\" : None,\n",
    "        }\n",
    "        n_variables = len(fc_parameters.keys())\n",
    "        n_nodes = features[0].shape[0]\n",
    "        for n,case in enumerate(features):\n",
    "            new_dataframe = pd.DataFrame(columns=['id','time','channel','value'])\n",
    "            new_dataframe[\"id\"] = np.repeat(n,case.shape[0]*case.shape[1])\n",
    "            new_dataframe[\"time\"] = np.tile(np.arange(case.shape[1]),case.shape[0])\n",
    "            new_dataframe[\"channel\"] = np.stack([torch.full([case.shape[1]],i) for i in range(case.shape[0])]).flatten()\n",
    "            new_dataframe['value'] = abs(case.flatten())\n",
    "            extracted_features = extract_features(\n",
    "            new_dataframe,\n",
    "            column_id=\"id\",\n",
    "            column_sort=\"time\",\n",
    "            column_kind=\"channel\",\n",
    "            column_value=\"value\",\n",
    "            default_fc_parameters=fc_parameters,\n",
    "            n_jobs=6\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                final_df = pd.concat([final_df,extracted_features],axis=0)\n",
    "            except:\n",
    "                final_df = extracted_features\n",
    "        new_features = final_df.to_numpy().reshape(-1,n_nodes,n_variables)\n",
    "        return new_features\n",
    "    def _extend_data(\n",
    "        self,\n",
    "        patient,\n",
    "        patient_number,\n",
    "        features,\n",
    "        labels,\n",
    "        time_labels=None,\n",
    "        plv_edge_weights=None,\n",
    "    ):\n",
    "        \"\"\"Convenience method to extend the dictionaries with features, labels, time labels and edge weights.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to extend the dictionaries for.\n",
    "            patient_number: (int) Patient number to extend the dictionaries for.\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "            time_labels (optional): (np.ndarray) Array with time labels.\n",
    "            plv_edge_weights (optional): (np.ndarray) Array with edge weights.\n",
    "        \"\"\"\n",
    "        if patient == self.loso_patient:\n",
    "            # logging.info(f\"Adding recording {record} of patient {patient}\")\n",
    "            try:\n",
    "                self._val_features_dict[patient] = np.concatenate(\n",
    "                    (self._val_features_dict[patient], features), axis=0\n",
    "                )\n",
    "                self._val_labels_dict[patient] = np.concatenate(\n",
    "                    (self._val_labels_dict[patient], labels), axis=0\n",
    "                )\n",
    "                # self._val_time_labels_dict[patient] = np.concatenate(\n",
    "                #     (self._val_time_labels_dict[patient], time_labels), axis=0\n",
    "                # )\n",
    "                # self._val_edge_weights_dict[patient] = np.concatenate(\n",
    "                #     (\n",
    "                #         self._val_edge_weights_dict[patient],\n",
    "                #         np.repeat(plv_edge_weights, features.shape[0], axis=0),\n",
    "                #     )\n",
    "                # )\n",
    "\n",
    "                self._val_patient_number_dict[patient] = np.concatenate(\n",
    "                    (self._val_patient_number_dict[patient], patient_number)\n",
    "                )\n",
    "            except:\n",
    "                self._val_features_dict[patient] = features\n",
    "                self._val_labels_dict[patient] = labels\n",
    "                # self._val_time_labels_dict[patient] = time_labels\n",
    "                # self._val_edge_weights_dict[patient] = np.repeat(\n",
    "                #     plv_edge_weights, features.shape[0], axis=0\n",
    "                # )\n",
    "                self._val_patient_number_dict[patient] = patient_number\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                self._features_dict[patient] = np.concatenate(\n",
    "                    (self._features_dict[patient], features), axis=0\n",
    "                )\n",
    "                self._labels_dict[patient] = np.concatenate(\n",
    "                    (self._labels_dict[patient], labels), axis=0\n",
    "                )\n",
    "                # self._time_labels_dict[patient] = np.concatenate(\n",
    "                #     (self._time_labels_dict[patient], time_labels), axis=0\n",
    "                # )\n",
    "                # self._edge_weights_dict[patient] = np.concatenate(\n",
    "                #     (\n",
    "                #         self._edge_weights_dict[patient],\n",
    "                #         np.repeat(plv_edge_weights, features.shape[0], axis=0),\n",
    "                #     )\n",
    "                # )\n",
    "\n",
    "                self._patient_number_dict[patient] = np.concatenate(\n",
    "                    (self._patient_number_dict[patient], patient_number)\n",
    "                )\n",
    "            except:\n",
    "                self._features_dict[patient] = features\n",
    "                self._labels_dict[patient] = labels\n",
    "                # self._time_labels_dict[patient] = time_labels\n",
    "                # self._edge_weights_dict[patient] = np.repeat(\n",
    "                #     plv_edge_weights, features.shape[0], axis=0\n",
    "                # )\n",
    "                self._patient_number_dict[patient] = patient_number\n",
    "\n",
    "    def _get_labels_features_edge_weights_seizure(self, patient):\n",
    "        \"\"\"Method to extract features, labels and edge weights for seizure and interictal samples.\"\"\"\n",
    "\n",
    "        event_tables = self._get_event_tables(\n",
    "            patient\n",
    "        )  # extract start and stop of seizure for patient\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if \"seizures\" in recording\n",
    "        ]\n",
    "        for record in recording_list:  # iterate over recordings for a patient\n",
    "            recording_path = os.path.join(patient_path, record)\n",
    "            record = record.replace(\n",
    "                \"seizures_\", \"\"\n",
    "            )  ## some magic to get it properly working with event tables\n",
    "            record_id = record.split(\".npy\")[0]  #  get record id\n",
    "            start_event_tables = self._get_recording_events(\n",
    "                event_tables[0], record_id\n",
    "            )  # get start events\n",
    "            stop_event_tables = self._get_recording_events(\n",
    "                event_tables[1], record_id\n",
    "            )  # get stop events\n",
    "            data_array = np.load(recording_path)  # load the recording\n",
    "\n",
    "            # plv_edge_weights = np.expand_dims(\n",
    "            #     self._get_edge_weights_recording(\n",
    "            #         np.load(os.path.join(self.plv_values_path, patient, record))\n",
    "            #     ),\n",
    "            #     axis=0,\n",
    "            # )\n",
    "\n",
    "            features, labels, time_labels = utils.extract_training_data_and_labels(\n",
    "                data_array,\n",
    "                start_event_tables,\n",
    "                stop_event_tables,\n",
    "                fs=self.sampling_f,\n",
    "                seizure_lookback=self.seizure_lookback,\n",
    "                sample_timestep=self.sample_timestep,\n",
    "                preictal_overlap=self.preictal_overlap,\n",
    "                ictal_overlap=self.ictal_overlap,\n",
    "                buffer_time=self.buffer_time,\n",
    "            )\n",
    "\n",
    "            if features is None:\n",
    "                print(\n",
    "                    f\"Skipping the recording {record} patients {patient} cuz features are none\"\n",
    "                )\n",
    "                continue\n",
    "            # if len(np.unique(labels)) != 2:\n",
    "            #     print(\n",
    "            #         f\"Skipping the recording {record} patients {patient} cuz no seizure samples\"\n",
    "            #     )\n",
    "            #     continue\n",
    "\n",
    "            features = features.squeeze(2)\n",
    "\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = scipy.signal.resample(features, new_sample_count, axis=2)\n",
    "            if self.fft:\n",
    "                features = np.fft.rfft(features, axis=2)\n",
    "            if self.smote:\n",
    "                features, labels = self._apply_smote(features, labels)\n",
    "            time_labels = np.expand_dims(time_labels.astype(np.int32), 1)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "            patient_number = torch.full(\n",
    "                [labels.shape[0]],\n",
    "                int(\"\".join(x for x in patient if x.isdigit())),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "            self._extend_data(patient, patient_number, features, labels)\n",
    "\n",
    "    def _get_labels_features_edge_weights_interictal(\n",
    "        self, patient, samples_patient: int = None\n",
    "    ):\n",
    "        \"\"\"Method to extract features, labels and edge weights for interictal samples.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to extract the data for.\n",
    "            samples_patient (optional): (int) Number of samples to extract for a patient.\n",
    "        Samples are extracted from non-seizure recordings for a patient, starting from random time point.\n",
    "        If not specified, the number of samples is calculated as the number of interictal samples for a patient\n",
    "        divided by the number of recordings for a patient.\n",
    "\n",
    "        \"\"\"\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        ## get all non-seizure recordings for a patient\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if not \"seizures_\" in recording\n",
    "        ]\n",
    "        if samples_patient is None:\n",
    "            patient_num = int(\"\".join(filter(str.isdigit, patient)))\n",
    "            if patient == self.loso_patient:\n",
    "                patient_negatives = np.unique(\n",
    "                    self._val_labels_dict[patient], return_counts=True\n",
    "                )[1][0]\n",
    "                samples_per_recording = int(patient_negatives / len(recording_list))\n",
    "            else:\n",
    "                patient_negatives = np.unique(\n",
    "                    self._labels_dict[patient], return_counts=True\n",
    "                )[1][0]\n",
    "                samples_per_recording = int(patient_negatives / len(recording_list))\n",
    "        else:\n",
    "            samples_per_recording = int(samples_patient / len(recording_list))\n",
    "        for recording in recording_list:\n",
    "            recording_path = os.path.join(patient_path, recording)\n",
    "            data_array = np.expand_dims(np.load(recording_path), 1)\n",
    "            try:\n",
    "                features, labels = utils.extract_training_data_and_labels_interictal(\n",
    "                    input_array=data_array,\n",
    "                    samples_per_recording=samples_per_recording,\n",
    "                    fs=self.sampling_f,\n",
    "                    timestep=self.sample_timestep,\n",
    "                    overlap=self.preictal_overlap,\n",
    "                )\n",
    "            except:\n",
    "                print(f\"Skipping recording {recording} for patient due to the error\")\n",
    "                continue\n",
    "            idx_to_delete = np.where(np.array([np.diff(feature,axis=-1).mean() for feature in features])==0)[0]\n",
    "            if len(idx_to_delete) >0:\n",
    "                features = np.delete(features,obj=idx_to_delete,axis=0)\n",
    "                labels = np.delete(labels,obj=idx_to_delete,axis=0)\n",
    "                print(f\"Deleted {len(idx_to_delete)} samples due to zero variance\")\n",
    "            patient_number = torch.full(\n",
    "                [labels.shape[0]],\n",
    "                patient_num,\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            features = features.squeeze(2)\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = scipy.signal.resample(features, new_sample_count, axis=2)\n",
    "            if self.fft:\n",
    "                features = np.fft.rfft(features, axis=2)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "            self._extend_data(patient, patient_number, features, labels)\n",
    "\n",
    "    def _update_classes(self):\n",
    "        \"\"\"Method to remove samples of period that we do not want to load, as specified in used_classes_dict.\n",
    "        If it is possible, the method aims set the interictal period as class 0 to be used for extracting normalization parameters.\n",
    "        If it is not possible, preictal period remains chosen as class 0.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            not self.used_classes_dict[\"ictal\"]\n",
    "            or not self.used_classes_dict[\"preictal\"]\n",
    "        ):\n",
    "            label_to_delete = 0 if self.used_classes_dict[\"ictal\"] else 1\n",
    "            idx_to_delete = np.where(self._labels == label_to_delete)[0]\n",
    "            self._features = np.delete(self._features, obj=idx_to_delete, axis=0)\n",
    "            self._labels = np.delete(self._labels, obj=idx_to_delete, axis=0)\n",
    "            self._patient_number = np.delete(\n",
    "                self._patient_number, obj=idx_to_delete, axis=0\n",
    "            )\n",
    "            ## change labels of remaining classes\n",
    "            if label_to_delete == 0:\n",
    "                self._labels[self._labels == 2] = 0\n",
    "                print(\n",
    "                    \"Deleted preictal samples, changed interictal label to 0,  ictal remains 1 \"\n",
    "                )\n",
    "            else:\n",
    "                self._labels[self._labels == 0] = 1\n",
    "                self._labels[self._labels == 2] = 0\n",
    "                print(\n",
    "                    \"Deleted ictal samples, changed interictal label to 0, preictal to 1\"\n",
    "                )\n",
    "            if self.loso_patient is not None:\n",
    "                idx_to_delete = np.where(self._val_labels == label_to_delete)[0]\n",
    "                self._val_features = np.delete(\n",
    "                    self._val_features, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                self._val_labels = np.delete(\n",
    "                    self._val_labels, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                self._val_patient_number = np.delete(\n",
    "                    self._val_patient_number, obj=idx_to_delete, axis=0\n",
    "                )\n",
    "                if label_to_delete == 0:\n",
    "                    self._val_labels[self._val_labels == 2] = 0\n",
    "                    print(\n",
    "                        \"Deleted preictal samples from LOSO patient, changed interictal label to 0, ictal remains 1 \"\n",
    "                    )\n",
    "                else:\n",
    "                    self._val_labels[self._val_labels == 0] = 1\n",
    "                    self._val_labels[self._val_labels == 2] = 0\n",
    "                    print(\n",
    "                        \"Deleted ictal from LOSO patient, changed interictal label to 0, preictal to 1\"\n",
    "                    )\n",
    "        elif (sum(self.used_classes_dict.values())== 3): ## case when all three classes are used - just flipping labels\n",
    "            self._labels[self._labels == 2] = 4 ## change interictal to 4 from 2 temporarily\n",
    "            self._labels[self._labels == 0] = 2 ## change preictal to 2 from 0\n",
    "            self._labels[self._labels == 4] = 0  ## change interictal to 0 from 4\n",
    "            if self.loso_patient is not None:\n",
    "                self._val_labels[self._val_labels == 2] = 4\n",
    "                self._val_labels[self._val_labels == 0] = 2\n",
    "                self._val_labels[self._val_labels == 4] = 0\n",
    "    \n",
    "    # TODO define a method to create edges and calculate plv to get weights\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Creating graph data iterators. The iterator yelds dynamic, weighted and undirected graphs\n",
    "        containing self loops. Every node represents one electrode in EEG. The graph is fully connected,\n",
    "        edge weights are calculated for every EEG recording as PLV between channels (edge weight describes\n",
    "        the \"strength\" of connectivity between two channels in a recording). Node features are values of\n",
    "        channel voltages in time. Features are of shape [nodes,features,timesteps].\n",
    "\n",
    "        Returns:\n",
    "            train_dataset {DynamicGraphTemporalSignal} -- Training data iterator.\n",
    "            valid_dataset {DynamicGraphTemporalSignal} -- Validation data iterator (only if loso_patient is\n",
    "            specified in class constructor).\n",
    "        \"\"\"\n",
    "        ### TODO rozkminić o co chodzi z tym całym time labels - na razie wartość liczbowa która tam wchodzi\n",
    "        ### to shape atrybutu time_labels\n",
    "        assert (\n",
    "            \"interictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for interictal class in used_classes_dict\"\n",
    "        assert (\n",
    "            \"preictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for preictal class in used_classes_dict\"\n",
    "        assert (\n",
    "            \"ictal\" in self.used_classes_dict.keys()\n",
    "        ), \"Please define the behavior for ictal class in used_classes_dict\"\n",
    "        \n",
    "        assert (\n",
    "            sum(self.used_classes_dict.values()) > 1\n",
    "        ), \"Please define at least two classes to use in used_classes_dict\"\n",
    "        \n",
    "        self._initialize_dicts()\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "        start_time = time.time()\n",
    "        if self.smote:\n",
    "            for patient in patient_list:\n",
    "                self._get_labels_features_edge_weights_seizure(patient)\n",
    "        else:\n",
    "            Parallel(n_jobs=6, require=\"sharedmem\")(\n",
    "                delayed(self._get_labels_features_edge_weights_seizure)(patient)\n",
    "                for patient in patient_list\n",
    "            )\n",
    "        print(\n",
    "            f\"Finished reading in {time.time() - start_time} seconds for seizure data\"\n",
    "        )\n",
    "        if self.used_classes_dict[\"interictal\"]:\n",
    "            Parallel(n_jobs=6, require=\"sharedmem\")(\n",
    "                delayed(self._get_labels_features_edge_weights_interictal)(patient)\n",
    "                for patient in patient_list\n",
    "            )\n",
    "\n",
    "        self._convert_dict_to_array()\n",
    "        self._update_classes()\n",
    "\n",
    "        self._get_labels_count()\n",
    "        \n",
    "        if self.balance:\n",
    "            self._balance_classes()\n",
    "\n",
    "        print(\n",
    "            f\"Finished reading in {time.time() - start_time} seconds for non seizure data\"\n",
    "        )\n",
    "        start_time_preprocessing = time.time()\n",
    "        if self.rescale:\n",
    "            self._features *= self._features*1e6 ## rescale to back to volts\n",
    "            if self.loso_patient is not None:\n",
    "                self._val_features *= self._val_features*1e6\n",
    "        self._standardize_data(self._features, self._labels, self._val_features)\n",
    "        if self.tsfresh:\n",
    "            self._features = self._compute_tsfresh_features(self._features)\n",
    "            if self.loso_patient is not None:\n",
    "                self._val_features = self._compute_tsfresh_features(self._val_features)\n",
    "        \n",
    "        # self._get_labels_count()\n",
    "        if self.hjorth:\n",
    "            self._features = self._calculate_hjorth_features(self._features)\n",
    "            if self.loso_patient is not None:\n",
    "                self._val_features = self._calculate_hjorth_features(self._val_features)\n",
    "        self._get_edges()\n",
    "        self._array_to_tensor()\n",
    "\n",
    "        if self.train_test_split is not None:\n",
    "            if self.fft or self.hjorth:\n",
    "                data_list = self._features_to_data_list(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    # self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                train_data_list, val_data_list = self._split_data_list(data_list)\n",
    "                label_count = np.unique(\n",
    "                    [data.y.item() for data in train_data_list], return_counts=True\n",
    "                )[1]\n",
    "                self.alpha = label_count[0] / label_count[1]\n",
    "                loaders = [\n",
    "                    DataLoader(\n",
    "                        train_data_list,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                    DataLoader(\n",
    "                        val_data_list,\n",
    "                        batch_size=len(val_data_list),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                train_dataset = torch.utils.data.TensorDataset(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    # self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "\n",
    "                train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                    train_dataset,\n",
    "                    [1 - self.train_test_split, self.train_test_split],\n",
    "                    generator=torch.Generator().manual_seed(42),\n",
    "                )\n",
    "\n",
    "                train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    # num_workers=2,\n",
    "                    # pin_memory=True,\n",
    "                    # prefetch_factor=4,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "\n",
    "                val_dataloader = torch.utils.data.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=False,\n",
    "                    # num_workers=2,\n",
    "                    # pin_memory=True,\n",
    "                    # prefetch_factor=4,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "                loaders = [train_dataloader, val_dataloader]\n",
    "        else:\n",
    "            if self.fft or self.hjorth:\n",
    "                train_data_list = self._features_to_data_list(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    # self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                loaders = [\n",
    "                    DataLoader(\n",
    "                        train_data_list,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False,\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                train_dataset = torch.utils.data.TensorDataset(\n",
    "                    self._features,\n",
    "                    self._edges,\n",
    "                    # self._edge_weights,\n",
    "                    self._labels,\n",
    "                    # self._time_labels,\n",
    "                )\n",
    "                train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=True,\n",
    "                    # num_workers=2,\n",
    "                    # pin_memory=True,\n",
    "                    # prefetch_factor=4,\n",
    "                    drop_last=False,\n",
    "                )\n",
    "                loaders = [train_dataloader]\n",
    "        if self.loso_patient:\n",
    "            # if self.hjorth:\n",
    "            #     self._val_features = self._calculate_hjorth_features(self._val_features)\n",
    "\n",
    "            if self.fft or self.hjorth:\n",
    "                loso_data_list = self._features_to_data_list(\n",
    "                    self._val_features,\n",
    "                    self._val_edges,\n",
    "                    # self._val_edge_weights,\n",
    "                    self._val_labels,\n",
    "                    # self._val_time_labels,\n",
    "                )\n",
    "                print(\"Preprocessing time: \", time.time() - start_time_preprocessing)\n",
    "                return (\n",
    "                    *loaders,\n",
    "                    DataLoader(\n",
    "                        loso_data_list,\n",
    "                        batch_size=len(loso_data_list),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                    ),\n",
    "                )\n",
    "            loso_dataset = torch.utils.data.TensorDataset(\n",
    "                self._val_features,\n",
    "                self._val_edges,\n",
    "                # self._val_edge_weights,\n",
    "                self._val_labels,\n",
    "                #  self._val_time_labels,\n",
    "            )\n",
    "            loso_dataloader = torch.utils.data.DataLoader(\n",
    "                loso_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                # pin_memory=True,\n",
    "                # num_workers=2,\n",
    "                # prefetch_factor=4,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            return (*loaders, loso_dataloader)\n",
    "\n",
    "        return (*loaders,)\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO ekstrachować do osobnego loadera, oddzielić już ten jeden track całkowicie\n",
    "## czy nie zapisać tego po prostu jako jeden duży HDF5, a potem wczytywać?\n",
    "## pomyśleć o strukturze hdfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the recording chb20_12.npy patients chb20 cuz features are none\n",
      "Finished reading in 16.540140628814697 seconds for seizure data\n",
      "Finished reading in 16.72674322128296 seconds for non seizure data\n",
      "Preprocessing time:  16.326669454574585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.6"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "TIMESTEP = 6\n",
    "PREICTAL_OVERLAP = 0\n",
    "ICTAL_OVERLAP = 0\n",
    "INTER_OVERLAP = 0\n",
    "SFREQ = 256\n",
    "torch_geometric.seed_everything(42)\n",
    "dataloader = SeizureDataLoader(\n",
    "    npy_dataset_path=Path('data/npy_data_full'),\n",
    "    event_tables_path=Path('data/event_tables'),\n",
    "    plv_values_path=Path('data/plv_arrays'),\n",
    "    loso_patient='chb06',\n",
    "    sampling_f=SFREQ,\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep= TIMESTEP,\n",
    "    inter_overlap=INTER_OVERLAP,\n",
    "    preictal_overlap=PREICTAL_OVERLAP,\n",
    "    ictal_overlap=ICTAL_OVERLAP,\n",
    "    self_loops=False,\n",
    "    balance=False,\n",
    "    train_test_split=0.05,\n",
    "    fft=False,\n",
    "    hjorth=True,\n",
    "    downsample=60,\n",
    "    batch_size=64,\n",
    "    buffer_time=60,\n",
    "    smote=False,\n",
    "    tsfresh=False,\n",
    "    rescale=False,\n",
    "    used_classes_dict={\"ictal\": True, \"interictal\": False, \"preictal\": True}\n",
    "    )\n",
    "train_loader,valid_loader, loso_loader =dataloader.get_dataset() \n",
    "alpha = list(dataloader._label_counts.values())[0]/list(dataloader._label_counts.values())[1]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataloader._features\n",
    "labels = dataloader._labels\n",
    "indexes_interictal = np.where(dataloader._labels == 0)[0]\n",
    "features_interictal = features[indexes_interictal]\n",
    "indexes_ictal = np.where(dataloader._labels == 1)[0]\n",
    "features_ictal = features[indexes_ictal]\n",
    "indexes_preictal = np.where(dataloader._labels == 2)[0]\n",
    "features_preictal = features[indexes_preictal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare samples 14323 negative and 2543 positive - for the standarization they were the same amplitudes\n",
    "## how do I choose criteria for rejecting a sample when it is flat most of the time?\n",
    "## wavelet coef energy is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "hjorth_features_negative = mne_features.univariate.compute_variance(features_negative[865].numpy())\n",
    "hjorth_features_positive = mne_features.univariate.compute_variance(features_positive[865].numpy())\n",
    "hjoth_features_healthy = mne_features.univariate.compute_variance(features_healthy[234].numpy())\n",
    "plt.plot(hjorth_features_negative)\n",
    "plt.plot(hjorth_features_positive)\n",
    "plt.plot(hjoth_features_healthy)\n",
    "plt.legend(['negative','positive','healthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig,ax = plt.subplots(18,1,figsize=(10,10))\n",
    "\n",
    "for i in range(18):\n",
    "    \n",
    "    ax[i].plot(features_interictal[12,i,:])\n",
    "    ax[i].plot(features_ictal[15,i,:])\n",
    "    #ax[i].plot(features_preictal[5,i,:])\n",
    "    ax[i].set_title(f'Channel {i+1}')\n",
    "plt.legend(['interictal', 'preictal'])\n",
    "plt.show()\n",
    "\n",
    "# fig,ax = plt.subplots(18,1,figsize=(10,10))\n",
    "# for i in range(18):\n",
    "#     ax [i].plot(features_positive[865,i,:])\n",
    "#     ax[i].set_title(f'Channel {i+1}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loso = dataloader._val_features\n",
    "labels_loso = dataloader._val_labels\n",
    "indexes_interictal_loso = np.where(dataloader._val_labels == 0)[0]\n",
    "features_interictal_loso = features_loso[indexes_interictal_loso]\n",
    "indexes_ictal_loso = np.where(dataloader._val_labels == 1)[0]\n",
    "features_ictal_loso = features_loso[indexes_ictal_loso]\n",
    "indexes_preictal_loso = np.where(dataloader._val_labels == 2)[0]\n",
    "features_preictal_loso = features_loso[indexes_preictal_loso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig,ax = plt.subplots(18,1,figsize=(10,10))\n",
    "for i in range(18):\n",
    "    \n",
    "    ax[i].plot(features_interictal_loso[20,i,:])\n",
    "    #ax[i].plot(features_ictal_loso[2,i,:])\n",
    "    ax[i].plot(features_preictal_loso[15,i,:])\n",
    "    ax[i].set_title(f'Channel {i+1}')\n",
    "plt.legend(['interictal', 'preictal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loso = dataloader._val_features.numpy()\n",
    "indexes_loso = np.where(dataloader._val_labels == 0)[0]\n",
    "features_negative_loso = features_loso[indexes_loso]\n",
    "indexes_positive_loso = np.where(dataloader._val_labels == 1)[0]\n",
    "features_positive_loso = features_loso[indexes_positive_loso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, AttentionExplainer, ExplainerConfig\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=AttentionExplainer(),\n",
    "    explanation_type='model',\n",
    "   # node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='binary_classification',\n",
    "        task_level='graph',\n",
    "        return_type='raw',  # Model returns log probabilities.\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, timestep,sfreq, n_nodes=18,batch_size=32):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.out_features = 128\n",
    "        self.recurrent_1 = GCNConv(6,32, add_self_loops=True,improved=False)\n",
    "        self.recurrent_2 = GCNConv(32,64,add_self_loops=True,improved=False)\n",
    "        self.recurrent_3 = GCNConv(64,128,add_self_loops=True,improved=False)\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 16)\n",
    "        self.fc4 = torch.nn.Linear(16, 1)\n",
    "        self.batch_norm_1 = torch.nn.BatchNorm1d(32)\n",
    "        self.batch_norm_2 = torch.nn.BatchNorm1d(64)\n",
    "        self.batch_norm_3 = torch.nn.BatchNorm1d(128)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "    def forward(self, x, edge_index,edge_weight,batch):\n",
    "        x = torch.squeeze(x)\n",
    "        h = self.recurrent_1(x, edge_index, edge_weight)\n",
    "        h = self.batch_norm_1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_2(h, edge_index,edge_weight)\n",
    "        h = self.batch_norm_2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.recurrent_3(h, edge_index,edge_weight)\n",
    "        h = self.batch_norm_3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = global_mean_pool(h,batch)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc4(h)\n",
    "        return h.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2Lightning(pl.LightningModule):\n",
    "    def __init__(self, timestep,sfreq,alpha,threshold=0.5, hidden_channels=32,heads=8,negative_slope = 0.01, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.recurrent_1 = GATv2Conv(\n",
    "            sfreq*timestep,hidden_channels,heads=heads,negative_slope=negative_slope,dropout=dropout, add_self_loops=True,improved=True,edge_dim=1)\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels*heads, 64)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight,a=negative_slope)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight,a=negative_slope)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight,a=negative_slope)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation_recurrent = nn.Sequential(\n",
    "            self.batch_norm,\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.dropout,\n",
    "            self.fc1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.dropout,\n",
    "            self.fc2,\n",
    "            nn.LeakyReLU(),\n",
    "            self.dropout,\n",
    "            self.fc3 \n",
    "        )\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=torch.full([1], alpha))\n",
    "        self.sensitivity = BinaryRecall(threshold=threshold)\n",
    "        self.specificity = BinarySpecificity(threshold=threshold)\n",
    "        self.auroc = AUROC(task=\"binary\")\n",
    "    def forward(self, x, edge_index, edge_attr,batch):\n",
    "        h = self.recurrent_1(x, edge_index, edge_attr)\n",
    "        h = self.activation_recurrent(h)\n",
    "        h = global_mean_pool(h,batch)\n",
    "        h = self.classifier(h)\n",
    "        return h.squeeze()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch.x\n",
    "        x = x.squeeze()\n",
    "        signal_samples = x.shape[1]\n",
    "        x = 2 / signal_samples * torch.abs(x)\n",
    "        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "        edge_index = batch.edge_index\n",
    "        edge_attr = batch.edge_attr.float()\n",
    "        y = batch.y\n",
    "        batches = batch.batch\n",
    "        y_hat = self(x, edge_index, edge_attr,batches)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True,prog_bar=True)\n",
    "        self.log('train_sensitivity', self.sensitivity(y_hat, y), on_step=False, on_epoch=True,prog_bar=True)\n",
    "        self.log('train_specificity', self.specificity(y_hat, y), on_step=False, on_epoch=True,prog_bar=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch.x\n",
    "        x = x.squeeze()\n",
    "        signal_samples = x.shape[1]\n",
    "        x = 2 / signal_samples * torch.abs(x)\n",
    "        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "        edge_index = batch.edge_index\n",
    "        edge_attr = batch.edge_attr.float()\n",
    "        y = batch.y\n",
    "        batches = batch.batch\n",
    "        y_hat = self(x, edge_index, edge_attr,batches)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True,prog_bar=True)\n",
    "        self.log('valid_sensitivity', self.sensitivity(y_hat, y), on_step=False, on_epoch=True,prog_bar=True)\n",
    "        self.log('valid_specificity', self.specificity(y_hat, y), on_step=False, on_epoch=True,prog_bar=True)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        return optimizer\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, timestep, sfreq, n_nodes=18, batch_size=32,n_classes = 1):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        hidden_dim = 32\n",
    "        out_dim = 64\n",
    "        n_heads = 4\n",
    "        self.recurrent_1 = GATv2Conv(\n",
    "            6,\n",
    "            hidden_dim,\n",
    "            heads=n_heads,\n",
    "            negative_slope=0.01,\n",
    "            dropout=0.4,\n",
    "            add_self_loops=True,\n",
    "            improved=True,\n",
    "            edge_dim=1,\n",
    "        )\n",
    "        self.recurrent_2 = GATv2Conv(\n",
    "            hidden_dim * n_heads,\n",
    "            out_dim,\n",
    "            heads=n_heads,\n",
    "            negative_slope=0.01,\n",
    "            dropout=0.4,\n",
    "            add_self_loops=True,\n",
    "            improved=True,\n",
    "            edge_dim=1,\n",
    "        )\n",
    "    \n",
    "        self.fc1 = torch.nn.Linear(out_dim*n_heads, 512)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, a=0.01)\n",
    "        self.fc2 = torch.nn.Linear(512, 128)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, a=0.01)\n",
    "        self.fc3 = torch.nn.Linear(128, n_classes)\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight, a=0.01)\n",
    "\n",
    "        self.connectivity = torch.nn.Linear(sfreq * timestep * n_nodes, 324)\n",
    "        self.connectivity_2 = torch.nn.Linear(sfreq * timestep, 324)\n",
    "        self.batch_norm_1 = torch.nn.BatchNorm1d(hidden_dim * n_heads)\n",
    "        self.batch_norm_2 = torch.nn.BatchNorm1d(out_dim* n_heads)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.recurrent_1(x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        h = self.batch_norm_1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        # h = global_mean_pool(h,batch)\n",
    "\n",
    "        h = self.recurrent_2(h, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        h = self.batch_norm_2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = global_mean_pool(h, batch)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc1(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc3(h)\n",
    "        # h = F.leaky_relu(h)\n",
    "        # h = self.dropout(h)\n",
    "        # h = self.fc4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "\n",
    "    def __init__(self, sfreq, timestep, dim_h=128):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(int((sfreq * timestep / 2) + 1), dim_h),\n",
    "                BatchNorm1d(dim_h),\n",
    "                ReLU(),\n",
    "                Linear(dim_h, dim_h),\n",
    "                ReLU(),\n",
    "            ),\n",
    "           # edge_dim=1,\n",
    "        )\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h),\n",
    "                BatchNorm1d(dim_h),\n",
    "                ReLU(),\n",
    "                Linear(dim_h, dim_h),\n",
    "                ReLU(),\n",
    "            ),\n",
    "           # edge_dim=1,\n",
    "        )\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h),\n",
    "                BatchNorm1d(dim_h),\n",
    "                ReLU(),\n",
    "                Linear(dim_h, dim_h),\n",
    "                ReLU(),\n",
    "            ),\n",
    "           # edge_dim=1,\n",
    "        )\n",
    "\n",
    "        self.att_1 = GATv2Conv(\n",
    "            int((sfreq * timestep / 2) + 1),\n",
    "            dim_h,\n",
    "            heads=1,\n",
    "            negative_slope=0.01,\n",
    "            dropout=0.4,\n",
    "            add_self_loops=True,\n",
    "            improved=True,\n",
    "            edge_dim=1,\n",
    "        )\n",
    "        self.att_2 = GATv2Conv(\n",
    "            dim_h,\n",
    "            dim_h,\n",
    "            heads=1,\n",
    "            negative_slope=0.01,\n",
    "            dropout=0.4,\n",
    "            add_self_loops=True,\n",
    "            improved=True,\n",
    "            edge_dim=1,\n",
    "        )\n",
    "        self.att_3 = GATv2Conv(\n",
    "            dim_h,\n",
    "            dim_h,\n",
    "            heads=1,\n",
    "            negative_slope=0.01,\n",
    "            dropout=0.4,\n",
    "            add_self_loops=True,\n",
    "            improved=True,\n",
    "            edge_dim=1,\n",
    "        )\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Node embeddings\n",
    "       # _, edge_scores_1 = self.att_1(x, edge_index, return_attention_weights=True)\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "      #  _, edge_scores_2 = self.att_2(h1, edge_index, return_attention_weights=True)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "      #  _, edge_scores_3 = self.att_3(h1, edge_index, return_attention_weights=True)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_mean_pool(h1, batch)\n",
    "        h2 = global_mean_pool(h2, batch)\n",
    "        h3 = global_mean_pool(h3, batch)\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.unique(dataloader._labels.int().squeeze()),\n",
    "    y=dataloader._labels.int().squeeze().numpy(),\n",
    ")\n",
    "class_weights = torch.from_numpy(class_weights).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "Epoch: 0 Epoch loss: 0.627361714598621\n",
      "Epoch sensitivity: 0.15834347903728485\n",
      "Epoch specificity: 0.8991864323616028\n",
      "Epoch AUROC: 0.4710063338279724 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/35 [00:18<10:20, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.311484).  Saving model ...\n",
      "Epoch val_loss: 0.3114840090274811\n",
      "Epoch val_sensitivity: 0.09090909361839294\n",
      "Epoch val specificity: 0.9973081946372986\n",
      "Epoch val AUROC: 0.7131103277206421 \n",
      "1e-05\n",
      "Epoch: 1 Epoch loss: 0.5065747465681933\n",
      "Epoch sensitivity: 0.18879415094852448\n",
      "Epoch specificity: 0.9419172406196594\n",
      "Epoch AUROC: 0.5654115676879883 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/35 [00:32<08:36, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.311484 --> 0.284010).  Saving model ...\n",
      "Epoch val_loss: 0.28401029109954834\n",
      "Epoch val_sensitivity: 0.27272728085517883\n",
      "Epoch val specificity: 0.9878869652748108\n",
      "Epoch val AUROC: 0.7911568880081177 \n",
      "1e-05\n",
      "Epoch: 2 Epoch loss: 0.4387376298788588\n",
      "Epoch sensitivity: 0.2460414171218872\n",
      "Epoch specificity: 0.9581181406974792\n",
      "Epoch AUROC: 0.624851644039154 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 3/35 [00:47<08:13, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n",
      "Epoch val_loss: 0.294310063123703\n",
      "Epoch val_sensitivity: 0.34090909361839294\n",
      "Epoch val specificity: 0.9798115491867065\n",
      "Epoch val AUROC: 0.8167135715484619 \n",
      "1e-05\n",
      "Epoch: 3 Epoch loss: 0.4160065129700943\n",
      "Epoch sensitivity: 0.23751522600650787\n",
      "Epoch specificity: 0.9574814438819885\n",
      "Epoch AUROC: 0.6399974226951599 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 4/35 [01:04<08:22, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 4\n",
      "Epoch val_loss: 0.29675355553627014\n",
      "Epoch val_sensitivity: 0.3181818127632141\n",
      "Epoch val specificity: 0.983849287033081\n",
      "Epoch val AUROC: 0.8253471255302429 \n",
      "1e-05\n",
      "Epoch: 4 Epoch loss: 0.3951584047634109\n",
      "Epoch sensitivity: 0.25943970680236816\n",
      "Epoch specificity: 0.9628581404685974\n",
      "Epoch AUROC: 0.6751475930213928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/35 [01:19<07:55, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.284010 --> 0.283744).  Saving model ...\n",
      "Epoch val_loss: 0.2837444841861725\n",
      "Epoch val_sensitivity: 0.2613636255264282\n",
      "Epoch val specificity: 0.9905787110328674\n",
      "Epoch val AUROC: 0.8248271942138672 \n",
      "1e-05\n",
      "Epoch: 5 Epoch loss: 0.3856273141467137\n",
      "Epoch sensitivity: 0.2655298411846161\n",
      "Epoch specificity: 0.9661832451820374\n",
      "Epoch AUROC: 0.6857556700706482 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/35 [01:35<07:38, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n",
      "Epoch val_loss: 0.3059767186641693\n",
      "Epoch val_sensitivity: 0.3181818127632141\n",
      "Epoch val specificity: 0.983849287033081\n",
      "Epoch val AUROC: 0.8381332159042358 \n",
      "1e-05\n",
      "Epoch: 6 Epoch loss: 0.36533519420546556\n",
      "Epoch sensitivity: 0.2771010994911194\n",
      "Epoch specificity: 0.9670321941375732\n",
      "Epoch AUROC: 0.7146421670913696 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 7/35 [01:51<07:23, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 4\n",
      "Epoch val_loss: 0.2971000373363495\n",
      "Epoch val_sensitivity: 0.3181818127632141\n",
      "Epoch val specificity: 0.9878869652748108\n",
      "Epoch val AUROC: 0.842308521270752 \n",
      "1e-05\n",
      "Epoch: 7 Epoch loss: 0.37717045463531124\n",
      "Epoch sensitivity: 0.26735687255859375\n",
      "Epoch specificity: 0.9670321941375732\n",
      "Epoch AUROC: 0.7053065299987793 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 8/35 [02:06<07:03, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 4\n",
      "Epoch val_loss: 0.30636781454086304\n",
      "Epoch val_sensitivity: 0.3181818127632141\n",
      "Epoch val specificity: 0.9865410327911377\n",
      "Epoch val AUROC: 0.832290768623352 \n",
      "1e-05\n",
      "Epoch: 8 Epoch loss: 0.35662097390364056\n",
      "Epoch sensitivity: 0.28258222341537476\n",
      "Epoch specificity: 0.9684470891952515\n",
      "Epoch AUROC: 0.7181870937347412 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 8/35 [02:23<08:03, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 4\n",
      "Epoch val_loss: 0.2910214364528656\n",
      "Epoch val_sensitivity: 0.28409090638160706\n",
      "Epoch val specificity: 0.9892328381538391\n",
      "Epoch val AUROC: 0.833544909954071 \n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## normal loop\n",
    "from torchmetrics import Specificity, Recall, F1Score\n",
    "torch_geometric.seed_everything(42)\n",
    "early_stopping = utils.EarlyStopping(patience=4, verbose=True)\n",
    "device = torch.device(\"cpu\")\n",
    "model = GATv2(TIMESTEP,60,batch_size=32).to(device) #Gatv2\n",
    "#model = RecurrentGCN(TIMESTEP,60,batch_size=32).to(device) \n",
    "#model = GIN(60, 6).to(device)\n",
    "#loss_fn =  nn.CrossEntropyLoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.00001,weight_decay=0.0001)\n",
    "\n",
    "# recall = Recall(task=\"multiclass\", average='macro', num_classes=3)\n",
    "# specificity = Specificity(task=\"multiclass\", average='macro', num_classes=3)\n",
    "# auroc = AUROC(task=\"multiclass\", num_classes=3)\n",
    "# roc = ROC('multiclass', num_classes=3)\n",
    "# f1_score = F1Score(\"multiclass\",num_classes=3,average='macro')\n",
    "recall = BinaryRecall(threshold=0.5).to(device)\n",
    "specificity = BinarySpecificity(threshold=0.5).to(device)\n",
    "auroc = AUROC(task=\"binary\").to(device)\n",
    "\n",
    "model.train()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=2)\n",
    "\n",
    "for epoch in tqdm(range(35)):\n",
    "        try:\n",
    "                del preds, ground_truth\n",
    "        except:\n",
    "                pass\n",
    "        epoch_loss = 0.0\n",
    "        epoch_loss_valid = 0.0\n",
    "        model.train()\n",
    "        sample_counter = 0\n",
    "        batch_counter = 0\n",
    "        print(get_lr(optimizer))\n",
    "        for time_train, batch in enumerate(train_loader): ## TODO - this thing is still operating with no edge weights!!!\n",
    "                ## find a way to compute plv per batch fast (is it even possible?)\n",
    "            \n",
    "                x = batch.x.float()          \n",
    "                edge_index = batch.edge_index\n",
    "                y = batch.y\n",
    "                batch_idx = batch.batch\n",
    "            #    x = torch.square(torch.abs(x)).float()\n",
    "                y_hat = model(x, edge_index,None,batch_idx).squeeze()\n",
    "               # y_hat = model(x, edge_index,None, batch_idx).squeeze()\n",
    "               # y_hat = model(x, edge_index, batch_idx).squeeze()\n",
    "                # print(y_hat.shape)\n",
    "                # print(y.shape)\n",
    "                loss = loss_fn(y_hat,y)\n",
    "                epoch_loss += loss\n",
    "                try:\n",
    "                 preds = torch.cat([preds,y_hat.detach()],dim=0)\n",
    "                 ground_truth = torch.cat([ground_truth,y],dim=0)\n",
    "            \n",
    "                except:\n",
    "                 preds= y_hat.detach()\n",
    "                 ground_truth = y\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        ## calculate acc\n",
    "        \n",
    "        train_auroc = auroc(preds,ground_truth)\n",
    "        train_sensitivity = recall(preds,ground_truth)\n",
    "        train_specificity = specificity(preds,ground_truth)\n",
    "        #train_f1 = f1_score(preds,ground_truth)\n",
    "        del preds, ground_truth\n",
    "        print(f'Epoch: {epoch}', f'Epoch loss: {epoch_loss.detach().numpy()/(time_train+1)}')\n",
    "        print(f'Epoch sensitivity: {train_sensitivity}')\n",
    "        print(f'Epoch specificity: {train_specificity}')\n",
    "        print(f'Epoch AUROC: {train_auroc} ')\n",
    "       # print(f'Epoch F1: {train_f1} ')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                try:\n",
    "                        del preds_valid, ground_truth_valid\n",
    "                except:\n",
    "                        pass\n",
    "                for time_valid, batch_valid in enumerate(valid_loader):\n",
    "                        x = batch_valid.x.float()\n",
    "                        edge_index = batch_valid.edge_index\n",
    "                        y_val = batch_valid.y\n",
    "                        batch_idx = batch_valid.batch\n",
    "                        #x = torch.square(torch.abs(x)).float()\n",
    "                        y_hat_val = model(x, edge_index,None,batch_idx).squeeze()\n",
    "                        #y_hat_val = model(x, edge_index,None,batch_idx)\n",
    "                        #y_hat_val = model(x, edge_index,batch_idx)\n",
    "                        loss_valid = loss_fn(y_hat_val,y_val)\n",
    "                        epoch_loss_valid += loss_valid\n",
    "                        try:\n",
    "                         preds_valid = torch.cat([preds_valid,y_hat_val],dim=0)\n",
    "                         ground_truth_valid = torch.cat([ground_truth_valid,y_val],dim=0)\n",
    "                        except:\n",
    "                         preds_valid= y_hat_val\n",
    "                         ground_truth_valid = y_val\n",
    "        early_stopping(epoch_loss_valid.numpy()/(time_valid+1), model)\n",
    "        val_auroc = auroc(preds_valid,ground_truth_valid)\n",
    "        val_sensitivity = recall(preds_valid,ground_truth_valid)\n",
    "        val_specificity = specificity(preds_valid,ground_truth_valid)\n",
    "       # val_f1 = f1_score(preds_valid,ground_truth_valid)\n",
    "        del preds_valid, ground_truth_valid\n",
    "        print(f'Epoch val_loss: {epoch_loss_valid.detach().numpy()/(time_valid+1)}')\n",
    "        print(f'Epoch val_sensitivity: {val_sensitivity}')\n",
    "        print(f'Epoch val specificity: {val_specificity}')\n",
    "        print(f'Epoch val AUROC: {val_auroc} ')\n",
    "        #print(f'Epoch val F1: {val_f1} ')\n",
    "        #scheduler.step(epoch_loss_valid.detach().numpy()/(time_valid+1))\n",
    "        if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loso_loss: 0.321968674659729\n",
      "Loso_sensitivity: 0.0\n",
      "Loso_specificity: 0.9654788374900818\n",
      "Loso_AUROC: 0.5247519612312317 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        del preds_valid, ground_truth_valid\n",
    "    except:\n",
    "        pass\n",
    "    epoch_loss_loso = 0.0\n",
    "    for time_loso, batch_loso in enumerate(loso_loader):\n",
    "            x = batch_loso.x.to(device).float()\n",
    "            edge_index = batch_loso.edge_index.to(device)\n",
    "            y_loso = batch_loso.y.to(device)\n",
    "            batch_idx = batch_loso.batch.to(device)\n",
    "   \n",
    "\n",
    "            \n",
    "            \n",
    "            #y_hat_loso = model(x, edge_index,None,batch_idx)\n",
    "            y_hat_loso = model(x, edge_index,None,batch_idx).squeeze()\n",
    "            loss_loso = loss_fn(y_hat_loso,y_loso)\n",
    "            #loss_loso = torchvision.ops.sigmoid_focal_loss(y_hat,y,alpha=0.65,gamma=4,reduction='mean')\n",
    "            epoch_loss_loso += loss_loso\n",
    "            try:\n",
    "                preds_loso = torch.cat([preds_loso,y_hat_loso],dim=0)\n",
    "                ground_truth_loso= torch.cat([ground_truth_loso,y_loso],dim=0)\n",
    "            except:\n",
    "                preds_loso= y_hat_loso\n",
    "                ground_truth_loso = y_loso\n",
    "    loso_auroc = auroc(preds_loso,ground_truth_loso)\n",
    "    loso_sensitivity = recall(preds_loso,ground_truth_loso)\n",
    "    loso_specificity = specificity(preds_loso,ground_truth_loso)\n",
    "    #loso_f1 = f1_score(preds_loso,ground_truth_loso)\n",
    "    del preds_loso, ground_truth_loso\n",
    "\n",
    "    print(f'Loso_loss: {epoch_loss_loso.cpu().numpy()/(time_loso+1)}')\n",
    "    print(f'Loso_sensitivity: {loso_sensitivity}')\n",
    "    print(f'Loso_specificity: {loso_specificity}')\n",
    "   # print(f'Loso_F1: {loso_f1} ')\n",
    "    print(f'Loso_AUROC: {loso_auroc} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loso_preds_cm = np.argmax(torch.nn.functional.softmax(preds_loso,dim=1).cpu().numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loso_preds_cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "classes = ('Preictal', 'Ictal', 'Healthy')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(ground_truth_loso, loso_preds_cm)\n",
    "df_cm = pd.DataFrame(cf_matrix , index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "cm_loso = ConfusionMatrix('multiclass',num_classes=3)\n",
    "cm_loso(torch.tensor(loso_preds_cm),ground_truth_loso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "cm_loso = ConfusionMatrix('multiclass',num_classes=3)\n",
    "cm_loso(preds_loso,ground_truth_loso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88cc438b9c90976695678f0d6c20e4c06983b5710e6855b5b4390f60ecf93fe8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
