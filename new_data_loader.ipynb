{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "from mne_features.univariate import (\n",
    "    compute_variance,\n",
    "    compute_hjorth_complexity,\n",
    "    compute_hjorth_mobility,\n",
    "    compute_line_length,\n",
    "    compute_higuchi_fd,\n",
    "    compute_katz_fd,\n",
    "    compute_energy_freq_bands\n",
    ")\n",
    "import traceback\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HDFDataset_Writer:\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    preictal_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    downsample: int = None\n",
    "    sampling_f: int = 256\n",
    "    smote: bool = False\n",
    "    buffer_time: int = 15\n",
    "    connectivity_metric : str = \"plv\"\n",
    "    npy_dataset_path: str = \"npy_dataset\"\n",
    "    event_tables_path: str = \"event_tables\"\n",
    "    cache_folder: str = \"cache\"\n",
    "    \n",
    "    \"\"\"Class for creating hdf5 dataset from npy files.\n",
    "    Args:\n",
    "        seizure_lookback: (int) Time in seconds to look back from seizure onset. Default: 600.\n",
    "        sample_timestep: (int) Time in seconds to sample the data. Default: 5.\n",
    "        inter_overlap: (int) Time in seconds to overlap between interictal samples. Default: 0.\n",
    "        preictal_overlap: (int) Time in seconds to overlap between preictal samples. Default: 0.\n",
    "        icatal_overlap: (int) Time in seconds to overlap between ictal samples. Default: 0.\n",
    "        downsample: (int) Downsampling factor. Default: None.\n",
    "        sampling_f: (int) Sampling frequency of the input data (before downsampling). Default: 256.\n",
    "        smote: (bool) Whether to use SMOTE oversampling. Default: False.\n",
    "        buffer_time: (int) Time in seconds to add to the beggining and the end of the seizure. Default: 15.\n",
    "        connectivity_metric: (str) Connectivity metric to use. Either 'plv' or 'spectral_corr'. Default: 'plv'.\n",
    "        npy_dataset_path: (str) Path to the folder with npy recording files. Default: 'npy_dataset'.\n",
    "        event_tables_path: (str) Path to the folder with event tables. Default: 'event_tables'.\n",
    "        cache_folder: (str) Path to the folder to store the dataset. Default: 'cache'.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self._initialize_logger()\n",
    "        assert self.connectivity_metric in [\"plv\", \"spectral_corr\"], \"Connectivity metric must be either 'plv' or 'spectral_corr'\"\n",
    "        assert self.downsample is None or self.downsample > 0, \"Downsample must be either None or positive integer\"\n",
    "        assert self.sampling_f > 0, \"Sampling frequency must be positive integer\"\n",
    "        assert self.sample_timestep > 0, \"Sample timestep must be positive integer\"\n",
    "        assert self.seizure_lookback > 0, \"Seizure lookback must be positive integer\"\n",
    "        assert self.inter_overlap >= 0, \"Inter overlap must be positive integer\"\n",
    "        assert self.preictal_overlap >= 0, \"Preictal overlap must be positive integer\"\n",
    "        assert self.ictal_overlap >= 0, \"Ictal overlap must be positive integer\"\n",
    "        assert self.buffer_time >= 0, \"Buffer time must be positive integer\"\n",
    "        assert self.inter_overlap < self.sample_timestep, \"Inter overlap must be smaller than sample timestep\"\n",
    "        assert self.preictal_overlap < self.sample_timestep, \"Preictal overlap must be smaller than sample timestep\"\n",
    "        assert self.ictal_overlap < self.sample_timestep, \"Ictal overlap must be smaller than sample timestep\"\n",
    "        \n",
    "    \n",
    "    def _initialize_logger(self):\n",
    "        \"\"\"Initializing logger.\"\"\"\n",
    "        if not os.path.exists(\"logs/\"):\n",
    "            os.makedirs(\"logs/\")\n",
    "        start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        logging.basicConfig(\n",
    "            filename=f\"logs/hdf_dataset_writer_{start_time}.log\",\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            force=True,\n",
    "        )\n",
    "        self.logger = logging.getLogger(\"hdf_dataset_writer\")\n",
    "    def _find_matching_configs(self, current_config):\n",
    "        \n",
    "        def find_yaml_files(directory):\n",
    "            yaml_files = []\n",
    "            for root, dirs, files in os.walk(directory):\n",
    "                for file in files:\n",
    "                    if fnmatch.fnmatch(file, \"*.yaml\"):\n",
    "                        yaml_files.append(os.path.join(root, file))\n",
    "            return yaml_files\n",
    "        \n",
    "        config_files = find_yaml_files(self.cache_folder)\n",
    "        for config_file in config_files:\n",
    "            with open(config_file) as f:\n",
    "                config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                if config_dict == current_config:\n",
    "                    self.logger.info(f\"Found matching config file {config_file}\")\n",
    "                    print(f\"Found matching config file {config_file}\")\n",
    "                    self.found_dataset_path = os.path.dirname(config_file)\n",
    "                    return True\n",
    "        return False\n",
    "        \n",
    "    \n",
    "    def _create_config_dict(self):\n",
    "        dataclass_keys = list(self.__dataclass_fields__.keys())\n",
    "        dict_values = [self.__getattribute__(key) for key in dataclass_keys]\n",
    "        initial_config_dict = dict(zip(dataclass_keys, dict_values))\n",
    "        return initial_config_dict\n",
    "    \n",
    "    def _create_config_file(self,config_dict, dataset_folder_path):\n",
    "        with open(os.path.join(dataset_folder_path, \"config.yaml\"), \"w\") as f:\n",
    "            yaml.dump(config_dict, f)\n",
    "    \n",
    "    def _get_event_tables(self, patient_name: str) -> tuple[dict, dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\n",
    "        Args:\n",
    "            patient_name: (str) Name of the patient to get events for.\n",
    "        Returns:\n",
    "            start_events_dict: (dict) Dictionary with start events for given patient.\n",
    "            stop_events_dict: (dict) Dictionary with stop events for given patient.\n",
    "        \"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [\n",
    "            os.path.join(self.event_tables_path, ev_table)\n",
    "            for ev_table in event_table_list\n",
    "            if patient_name in ev_table\n",
    "        ]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[\n",
    "            0\n",
    "        ]  ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict(\"index\")\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict(\"index\")\n",
    "        return start_events_dict, stop_events_dict\n",
    "\n",
    "    def _get_recording_events(self, events_dict, recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict.\n",
    "        Args:\n",
    "            events_dict: (dict) Dictionary with events for given patient.\n",
    "            recording: (str) Name of the recording to get events for.\n",
    "        Returns:\n",
    "            recording_events: (list) List of seizure event start and stop time for given recording.\n",
    "        \"\"\"\n",
    "        recording_list = list(events_dict[recording + \".edf\"].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "\n",
    "    def _create_edge_idx_and_attributes(\n",
    "        self, connectivity_matrix: np.ndarray, threshold: int = 0.0\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create adjacency matrix from connectivity matrix. Edges are created for values above threshold.\n",
    "        If the edge is created, it has an attribute \"weight\" with the value of the connectivity measure associated.\n",
    "        Args:\n",
    "            connectivity_matrix: (np.ndarray) Array with connectivity values.\n",
    "            threshold: (float) Threshold for creating edges. (default: 0.0)\n",
    "        Returns:\n",
    "            edge_index: (np.ndarray) Array with edge indices.\n",
    "            edge_weights: (np.ndarray) Array with edge weights.\n",
    "        \"\"\"\n",
    "        result_graph = nx.graph.Graph()\n",
    "        n_nodes = connectivity_matrix.shape[0]\n",
    "        result_graph.add_nodes_from(range(n_nodes))\n",
    "        edge_tuples = [\n",
    "            (i, j)\n",
    "            for i in range(n_nodes)\n",
    "            for j in range(n_nodes)\n",
    "            if connectivity_matrix[i, j] > threshold\n",
    "        ]\n",
    "        result_graph.add_edges_from(edge_tuples)\n",
    "        edge_index = nx.convert_matrix.to_numpy_array(result_graph)\n",
    "        # connection_indices = np.where(edge_index==1)\n",
    "        # edge_weights = connectivity_matrix[connection_indices] ## ??\n",
    "\n",
    "        return edge_index\n",
    "\n",
    "\n",
    "    def _apply_smote(self, features, labels):\n",
    "        \"\"\"Performs SMOTE oversampling on the dataset. Implemented for preictal vs ictal scenarion only.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            x_train_smote: (np.ndarray) Array with SMOTE oversampled features.\n",
    "            y_train_smote: (np.ndarray) Array with SMOTE oversampled labels.\n",
    "        \"\"\"\n",
    "        dim_1, dim_2, dim_3 = features.shape\n",
    "\n",
    "        new_dim = dim_1 * dim_2\n",
    "        new_x_train = features.reshape(new_dim, dim_3)\n",
    "        new_y_train = []\n",
    "        for i in range(len(labels)):\n",
    "            new_y_train.extend([labels[i]] * dim_2)\n",
    "\n",
    "        new_y_train = np.array(new_y_train)\n",
    "\n",
    "        # transform the dataset\n",
    "        oversample = SMOTE(random_state=42)\n",
    "        x_train, y_train = oversample.fit_resample(new_x_train, new_y_train)\n",
    "        x_train_smote = x_train.reshape(int(x_train.shape[0] / dim_2), dim_2, dim_3)\n",
    "        y_train_smote = []\n",
    "        for i in range(int(x_train.shape[0] / dim_2)):\n",
    "            # print(i)\n",
    "            value_list = list(y_train.reshape(int(x_train.shape[0] / dim_2), dim_2)[i])\n",
    "            # print(list(set(value_list)))\n",
    "            y_train_smote.extend(list(set(value_list)))\n",
    "            ## Check: if there is any different value in a list\n",
    "            if len(set(value_list)) != 1:\n",
    "                print(\n",
    "                    \"\\n\\n********* STOP: THERE IS SOMETHING WRONG IN TRAIN ******\\n\\n\"\n",
    "                )\n",
    "        y_train_smote = np.array(y_train_smote)\n",
    "        return x_train_smote, y_train_smote\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    def _get_labels_features_edge_weights_seizure(self, patient):\n",
    "        \"\"\"Method to extract features, labels and edge weights for seizure and interictal samples.\"\"\"\n",
    "\n",
    "        event_tables = self._get_event_tables(\n",
    "            patient\n",
    "        )  # extract start and stop of seizure for patient\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if \"seizures\" in recording\n",
    "        ]\n",
    "        self.logger.info(f\"Starting seizure period data loading for patient {patient}\")\n",
    "        for n, record in enumerate(\n",
    "            recording_list\n",
    "        ):  # iterate over recordings for a patient\n",
    "            recording_path = os.path.join(patient_path, record)\n",
    "            record = record.replace(\n",
    "                \"seizures_\", \"\"\n",
    "            )  ## some magic to get it properly working with event tables\n",
    "            record_id = record.split(\".npy\")[0]  #  get record id\n",
    "            start_event_tables = self._get_recording_events(\n",
    "                event_tables[0], record_id\n",
    "            )  # get start events\n",
    "            stop_event_tables = self._get_recording_events(\n",
    "                event_tables[1], record_id\n",
    "            )  # get stop events\n",
    "            data_array = np.load(recording_path)  # load the recording\n",
    "\n",
    "            features, labels, time_labels = utils.extract_training_data_and_labels(\n",
    "                data_array,\n",
    "                start_event_tables,\n",
    "                stop_event_tables,\n",
    "                fs=self.sampling_f,\n",
    "                seizure_lookback=self.seizure_lookback,\n",
    "                sample_timestep=self.sample_timestep,\n",
    "                preictal_overlap=self.preictal_overlap,\n",
    "                ictal_overlap=self.ictal_overlap,\n",
    "                buffer_time=self.buffer_time,\n",
    "            )\n",
    "\n",
    "            if features is None or features.shape[0] == 0:\n",
    "                self.logger.info(\n",
    "                    f\"Skipping the recording {record} patients {patient} - features are none\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            features = features.squeeze(2)\n",
    "            sampling_f = self.sampling_f if self.downsample is None else self.downsample\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = resample(features, new_sample_count, axis=2)\n",
    "                \n",
    "            conn_matrix_list = [\n",
    "                self.connectivity_function(feature, sampling_f) for feature in features\n",
    "            ]\n",
    "      \n",
    "            edge_idx = np.stack(\n",
    "                [\n",
    "                    self._create_edge_idx_and_attributes(\n",
    "                        conn_matrix, threshold=np.mean(conn_matrix)\n",
    "                    )\n",
    "                    for conn_matrix in conn_matrix_list\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            \n",
    "            # if self.smote:\n",
    "            #     features, labels = self._apply_smote(features, labels)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "\n",
    "            features_patient = (\n",
    "                features if n == 0 else np.concatenate([features_patient, features])\n",
    "            )\n",
    "            labels_patient = (\n",
    "                labels if n == 0 else np.concatenate([labels_patient, labels])\n",
    "            )\n",
    "            edge_idx_patient = (\n",
    "                edge_idx if n == 0 else np.concatenate([edge_idx_patient, edge_idx])\n",
    "            )\n",
    "        #     edge_weights_patient = edge_weights if n == 0 else np.concatenate([edge_weights_patient, edge_weights])\n",
    "        try:\n",
    "            \n",
    "\n",
    "            sample_count = features_patient.shape[0]\n",
    "            n_channels = features_patient.shape[1]\n",
    "            n_features = features_patient.shape[2]\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    with h5py.File(self.dataset_path, \"a\") as hdf5_file:\n",
    "                \n",
    "                        hdf5_file[patient].create_dataset(\"features\", data=features_patient,maxshape=(None,n_channels,n_features))\n",
    "                        hdf5_file[patient].create_dataset(\"labels\", data=labels_patient,maxshape=(None,1))\n",
    "                        hdf5_file[patient].create_dataset(\"edge_idx\", data=edge_idx_patient,maxshape=(None,n_channels,n_channels))\n",
    "                    \n",
    "                        self.logger.info(f\"Dataset for patient {patient} created successfully!.\")\n",
    "                    break\n",
    "                except BlockingIOError:\n",
    "                    self.logger.warning(f\"Waiting for dataset {patient} to be created.\")\n",
    "                    continue\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(e)\n",
    "            traceback_str = traceback.format_exc()\n",
    "            self.logger.error(traceback_str)\n",
    "            self.logger.error(\"##############################################\")\n",
    "            self.logger.error(f\"Cannot create dataset for patient {patient}!\")\n",
    "            self.logger.error(\"##############################################\")\n",
    "        preictal_samples = np.unique(labels_patient, return_counts=True)[1][0] \n",
    "        return (patient,preictal_samples,sample_count)\n",
    "\n",
    "    def _get_labels_features_edge_weights_interictal(\n",
    "        self, patient, samples_patient: int = None\n",
    "    ):\n",
    "        \"\"\"Method to extract features, labels and edge weights for interictal samples.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to extract the data for.\n",
    "            samples_patient (optional): (int) Number of samples to extract for a patient.\n",
    "        Samples are extracted from non-seizure recordings for a patient, starting from random time point.\n",
    "        If not specified, the number of samples is calculated as the number of interictal samples for a patient\n",
    "        divided by the number of recordings for a patient.\n",
    "\n",
    "        \"\"\"\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        ## get all non-seizure recordings for a patient\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if not \"seizures_\" in recording\n",
    "        ]\n",
    "        if samples_patient is None:\n",
    "            ## if not specified use the same number of samples for each recording as for preictal samples\n",
    "            samples_per_recording = int(self.preictal_samples_dict[patient] / len(recording_list))\n",
    "        else:\n",
    "            samples_per_recording = int(samples_patient / len(recording_list))\n",
    "            \n",
    "        for n, record in enumerate(recording_list):\n",
    "            recording_path = os.path.join(patient_path, record)\n",
    "            data_array = np.expand_dims(np.load(recording_path), 1)\n",
    "            try:\n",
    "                features, labels = utils.extract_training_data_and_labels_interictal(\n",
    "                    input_array=data_array,\n",
    "                    samples_per_recording=samples_per_recording,\n",
    "                    fs=self.sampling_f,\n",
    "                    timestep=self.sample_timestep,\n",
    "                    overlap=self.inter_overlap,\n",
    "                )\n",
    "            except ValueError:\n",
    "                self.logger.error(f\"Cannot extract demanded amount of interictal samples from recording {record} for patient {patient}\")\n",
    "                continue\n",
    "            \n",
    "            if features is None:\n",
    "                self.logger.info(\n",
    "                    f\"Skipping the recording {record} patients {patient} - features are none\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            idx_to_delete = np.where(\n",
    "                np.array([np.diff(feature, axis=-1).mean() for feature in features])\n",
    "                == 0\n",
    "            )[0]\n",
    "            if len(idx_to_delete) > 0:\n",
    "                features = np.delete(features, obj=idx_to_delete, axis=0)\n",
    "                labels = np.delete(labels, obj=idx_to_delete, axis=0)\n",
    "            if features.shape[0] == 0:\n",
    "                self.logger.info(f\"No samples left after removing bad ones for patient {patient} - recording {record}\")\n",
    "                continue\n",
    "            features = features.squeeze(2)\n",
    "            sampling_f = self.sampling_f if self.downsample is None else self.downsample\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = resample(features, new_sample_count, axis=2)\n",
    "            conn_matrix_list = [\n",
    "                self.connectivity_function(feature, sampling_f) for feature in features\n",
    "            ]\n",
    "            try:\n",
    "                edge_idx = np.stack(\n",
    "                    [\n",
    "                        self._create_edge_idx_and_attributes(\n",
    "                            conn_matrix, threshold=np.mean(conn_matrix)\n",
    "                        )\n",
    "                        for conn_matrix in conn_matrix_list\n",
    "                    ]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.logger.error(e)\n",
    "                self.logger.error(f\"Cannot create edge index for patient {patient} \\n recording {record}\")\n",
    "                continue\n",
    "                \n",
    "     \n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "\n",
    "            features_patient = (\n",
    "                features if n == 0 else np.concatenate([features_patient, features])\n",
    "            )\n",
    "            labels_patient = (\n",
    "                labels if n == 0 else np.concatenate([labels_patient, labels])\n",
    "            )\n",
    "            edge_idx_patient = (\n",
    "                edge_idx if n == 0 else np.concatenate([edge_idx_patient, edge_idx])\n",
    "            )\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                with h5py.File(self.dataset_path, \"a\") as hdf5_file:\n",
    "                    current_patient_features = hdf5_file[patient][\"features\"].shape[0]\n",
    "                    current_patient_labels = hdf5_file[patient][\"labels\"].shape[0]\n",
    "                    current_patient_edge_idx = hdf5_file[patient][\"edge_idx\"].shape[0]\n",
    "                    hdf5_file[patient][\"features\"].resize(\n",
    "                        (current_patient_features + features_patient.shape[0]),axis=0\n",
    "                    )\n",
    "                    hdf5_file[patient][\"features\"][-features_patient.shape[0]:] = features_patient\n",
    "                    hdf5_file[patient][\"labels\"].resize(\n",
    "                        (current_patient_labels + labels_patient.shape[0]),axis=0\n",
    "                    )\n",
    "                    hdf5_file[patient][\"labels\"][-labels_patient.shape[0]:] = labels_patient\n",
    "                    hdf5_file[patient][\"edge_idx\"].resize(\n",
    "                        (current_patient_edge_idx + edge_idx_patient.shape[0]),axis=0\n",
    "                    )\n",
    "                    hdf5_file[patient][\"edge_idx\"][-edge_idx_patient.shape[0]:] = edge_idx_patient\n",
    "                    self.logger.info(f\"Dataset for patient {patient} appended successfully!.\")\n",
    "                break\n",
    "            except BlockingIOError:\n",
    "                self.logger.warning(f\"Waiting for appending of {patient}.\")\n",
    "                continue\n",
    "        return features_patient.shape[0]\n",
    "    def _multiprocess_seizure_period_data_loading(self):\n",
    "        num_processes = mp.cpu_count()\n",
    "        pool = mp.Pool(processes=num_processes)\n",
    "        result = pool.map(\n",
    "            self._get_labels_features_edge_weights_seizure, self.patient_list\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        self.sample_count = 0\n",
    "        for patient, preictal_samples, sample_count in result:\n",
    "            self.preictal_samples_dict[patient] = preictal_samples\n",
    "            self.sample_count += sample_count\n",
    "    \n",
    "    def _multiprocess_interictal_data_loading(self):\n",
    "        num_processes = mp.cpu_count()\n",
    "        pool = mp.Pool(processes=num_processes)\n",
    "        result = pool.map(\n",
    "            self._get_labels_features_edge_weights_interictal, self.patient_list\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for sample_count in result:\n",
    "            self.sample_count += sample_count\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        \n",
    "        folder_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        dataset_folder = os.path.join(self.cache_folder, folder_name)\n",
    "        self.dataset_path = os.path.join(dataset_folder, \"dataset.hdf5\")\n",
    "        current_config = self._create_config_dict()\n",
    "        if self._find_matching_configs(current_config):\n",
    "            print(\n",
    "                f\"Dataset already exists. Dataset not created.\"\n",
    "            )\n",
    "            return self.found_dataset_path\n",
    "        \n",
    "        os.makedirs(dataset_folder, exist_ok=True)\n",
    "        self._create_config_file(current_config, dataset_folder)\n",
    "        self.patient_list = os.listdir(self.npy_dataset_path)\n",
    "        with h5py.File(self.dataset_path, \"w\") as hdf5_file:\n",
    "            for patient in self.patient_list:\n",
    "                hdf5_file.create_group(patient)\n",
    "        self.connectivity_function = utils.compute_plv_matrix if self.connectivity_metric == \"plv\" else utils.compute_spect_corr_matrix\n",
    "        self.preictal_samples_dict = {}\n",
    "        try:\n",
    "            if self.smote:\n",
    "                for patient in self.patient_list:\n",
    "                    self._get_labels_features_edge_weights_seizure(patient)\n",
    "            else:\n",
    "                self._multiprocess_seizure_period_data_loading()\n",
    "                self.logger.info(\"Seizure period data loaded.\")\n",
    "                self._multiprocess_interictal_data_loading()\n",
    "                self.logger.info(\"Interictal data loaded.\")\n",
    "\n",
    "            print(f\"Dataset created in folder {dataset_folder}.\")\n",
    "            print(f\"Dataset contains {self.sample_count} samples.\")\n",
    "        except:\n",
    "            if os.path.exists(self.dataset_path):\n",
    "                os.remove(self.dataset_path)\n",
    "            if os.path.exists(os.path.join(dataset_folder, \"config.yaml\")):\n",
    "             os.remove(os.path.join(dataset_folder, \"config.yaml\"))\n",
    "            if os.path.exists(dataset_folder):\n",
    "                os.rmdir(dataset_folder)\n",
    "            self.logger.error(\"Dataset creation failed. Dataset deleted.\")\n",
    "            raise Exception(\"Dataset creation failed. Dataset deleted.\")\n",
    "        \n",
    "        return dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "hdf_writer = HDFDataset_Writer(\n",
    "    npy_dataset_path=\"data/npy_data_full\",\n",
    "    event_tables_path=\"data/event_tables\",\n",
    "    cache_folder=\"cache\",\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep=9,\n",
    "    downsample=60,\n",
    "    connectivity_metric=\"plv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matching config file cache/2023-07-11_09-31-05/config.yaml\n",
      "Dataset already exists. Dataset not created.\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "ds_path = hdf_writer.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HDFDatasetLoader:\n",
    "    root: str\n",
    "    train_val_split_ratio: float = 0.0\n",
    "    loso_subject: str = None\n",
    "    sampling_f: int = 60\n",
    "    extract_features: bool = False\n",
    "    fft: bool = False\n",
    "    seed: int = 42\n",
    "    used_classes_dict: dict[str] = field(\n",
    "        default_factory=lambda: {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    )\n",
    "    \"\"\"\n",
    "    Class to load graph data from HDF5 file as lists of torch.geomtric.data.Data objects.\n",
    "        Args:\n",
    "            root: (str) Path to the HDF5 file.\n",
    "            train_val_split_ratio: (float) Ratio of samples to be used for validation. Default: 0.0\n",
    "            loso_subject: (str) Name of the patient to be used for leave-one-subject-out cross-validation. Default: None\n",
    "            sampling_f: (int) Sampling frequency of the data. Default: 60\n",
    "            extract_features: (bool) If True, the timeseries are transformed into a set of chosen EEG features. Default: False\n",
    "            fft: (bool) If True, FFT is calculated on the timeseries. Default: False\n",
    "            seed: (int) Random seed. Default: 42\n",
    "            default_factory: (dict) Dictionary with periods to be used for training. Default: {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._check_arguments()\n",
    "        self._logger_init()\n",
    "        self._determine_dataset_characteristics()\n",
    "        self._show_used_classes()\n",
    "\n",
    "    def _check_arguments(self):\n",
    "        \"\"\"Method to check if the arguments are valid.\"\"\"\n",
    "        assert self.sampling_f > 0, \"Sampling frequency must be positive integer\"\n",
    "        assert not (\n",
    "            self.extract_features and self.fft\n",
    "        ), \"Cannot extract both features and FFT\"\n",
    "        assert (\n",
    "            self.train_val_split_ratio >= 0.0 and self.train_val_split_ratio <= 1.0\n",
    "        ), \"Train val split ratio must be between 0.0 and 1.0\"\n",
    "        assert (\n",
    "            sum(self.used_classes_dict.values()) > 1\n",
    "        ), \"At least two classes must be used for training!\"\n",
    "\n",
    "    def _show_used_classes(self):\n",
    "        \"\"\"Method to show which classes are used for training.\"\"\"\n",
    "        self.logger.info(f\"Used classes: {self.used_classes_dict}\")\n",
    "        used_periods = [key for key, value in self.used_classes_dict.items() if value]\n",
    "        for period in used_periods:\n",
    "            print(f\"USING CLASS: {period}\")\n",
    "\n",
    "    def _determine_dataset_characteristics(self):\n",
    "        \"\"\"Method to determine dataset characteristics.\"\"\"\n",
    "        self.hdf_data_path = f\"{self.root}/dataset.hdf5\"\n",
    "        with h5py.File(self.hdf_data_path, \"r\") as hdf5_file:\n",
    "            self.patient_list = list(hdf5_file.keys())\n",
    "            self.n_channels, self.n_features = hdf5_file[self.patient_list[0]][\n",
    "                \"features\"\n",
    "            ].shape[1:]\n",
    "        if self.loso_subject is not None:\n",
    "            self.patient_list.remove(self.loso_subject)\n",
    "        self._determine_sample_count()\n",
    "        self._get_mean_std()\n",
    "\n",
    "    def _logger_init(self):\n",
    "        \"\"\"Initializing logger.\"\"\"\n",
    "        if not os.path.exists(\"logs/\"):\n",
    "            os.makedirs(\"logs/\")\n",
    "        start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        logging.basicConfig(\n",
    "            filename=f\"logs/hdf_dataloader_{start_time}.log\",\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            force=True,\n",
    "        )\n",
    "        self.logger = logging.getLogger(\"hdf_dataloader\")\n",
    "        print(f\"Logger initialized. Logs saved to logs/hdf_dataloader_{start_time}.log\")\n",
    "\n",
    "    def _determine_sample_count(self):\n",
    "        \"\"\"Method to determine number of samples in the dataset. the values are later used to\n",
    "        create placeholder arrays during mean and standard deviation calculation.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf_data_path, \"r\") as hdf5_file:\n",
    "            total_samples = 0\n",
    "            for patient in self.patient_list:\n",
    "                total_samples += hdf5_file[patient][\"features\"].shape[0]\n",
    "            if self.train_val_split_ratio > 0:\n",
    "                self.train_samples = int(\n",
    "                    total_samples * (1 - self.train_val_split_ratio)\n",
    "                )\n",
    "                self.val_samples = total_samples - self.train_samples\n",
    "            else:\n",
    "                self.train_samples = total_samples\n",
    "                self.val_samples = 0\n",
    "            if self.loso_subject is not None:\n",
    "                self.loso_samples = hdf5_file[self.loso_subject][\"features\"].shape[0]\n",
    "                total_samples += self.loso_samples\n",
    "            else:\n",
    "                self.loso_samples = 0\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Counting samples successful. Dataset contains {total_samples} samples.\"\n",
    "        )\n",
    "        self.logger.info(f\"Dataset contains {self.train_samples} training samples.\")\n",
    "        self.logger.info(f\"Dataset contains {self.val_samples} validation samples.\")\n",
    "        self.logger.info(f\"Dataset contains {self.loso_samples} loso samples.\")\n",
    "\n",
    "    def _get_mean_std(self):\n",
    "        \"\"\"Method to determine mean and standard deviation of interictal samples. Those values are used late to normalize all data.\"\"\"\n",
    "\n",
    "        current_sample = 0\n",
    "        features_all = np.empty(\n",
    "            (self.train_samples + self.val_samples, self.n_channels, self.n_features)\n",
    "        )  ## for standarization only\n",
    "        labels_all = np.empty((self.train_samples + self.val_samples, 1))\n",
    "        with h5py.File(self.hdf_data_path, \"r\") as hdf5_file:\n",
    "            for patient in self.patient_list:\n",
    "                features_all[\n",
    "                    current_sample : current_sample\n",
    "                    + hdf5_file[patient][\"features\"].shape[0]\n",
    "                ] = hdf5_file[patient][\"features\"]\n",
    "                labels_all[\n",
    "                    current_sample : current_sample\n",
    "                    + hdf5_file[patient][\"features\"].shape[0]\n",
    "                ] = hdf5_file[patient][\"labels\"]\n",
    "                current_sample += hdf5_file[patient][\"features\"].shape[0]\n",
    "        idx = np.where(labels_all == 0)[0]\n",
    "        self.data_mean = np.mean(features_all[idx])\n",
    "        self.data_std = np.std(features_all[idx])\n",
    "        self.logger.info(\n",
    "            f\"Mean and standard deviation calculated for interictal samples.\"\n",
    "        )\n",
    "\n",
    "    def _normalize_data(self, features: np.ndarray):\n",
    "        \"\"\"Method to normalize input features data using mean and standard deviation extracted previously.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features to be normalized.\n",
    "        Returns:\n",
    "            normalized_features: (np.ndarray) Array with normalized features.\n",
    "        \"\"\"\n",
    "        normalized_features = features.copy()\n",
    "        for i in range(features.shape[0]):\n",
    "            for n in range(features.shape[1]):\n",
    "                normalized_features[i, n, :] = (features[i, n, :] - self.data_mean) / self.data_std\n",
    "        return normalized_features\n",
    "    \n",
    "    def update_classes(\n",
    "        self, features: np.ndarray, labels: np.ndarray, edge_idx: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Method to properly relabel classes for training accordint to used_classes_dict argument.\"\"\"\n",
    "        \"\"\"Convention: \n",
    "            Base case: 0 - preictal, 1 - ictal, 2 - interictal\n",
    "            Case 1: 0 - preictal, 1 - ictal\n",
    "            Case 2: 0 - interictal, 1 - ictal\n",
    "            Case 3: 0 - preictal, 1 - interictal\n",
    "        If present, ictal period always as class 1.\n",
    "        Args:\n",
    "            labels: (np.ndarray) Array with labels to be relabeled.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with removed examples from unused classes.\n",
    "            new_labels: (np.ndarray) Array with relabeled labels.\n",
    "            new_edge_idx: (np.ndarray) Array with removed examples from unused classes.\n",
    "        \"\"\"\n",
    "        if self.used_classes_dict[\"preictal\"] and self.used_classes_dict[\"ictal\"]:\n",
    "            label_to_remove = 2\n",
    "            new_labels = np.delete(\n",
    "                labels, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_features = np.delete(\n",
    "                features, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_idx = np.delete(\n",
    "                edge_idx, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "        elif self.used_classes_dict[\"ictal\"] and self.used_classes_dict[\"interictal\"]:\n",
    "            label_to_remove = 0\n",
    "            new_labels = np.delete(\n",
    "                labels, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_features = np.delete(\n",
    "                features, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_idx = np.delete(\n",
    "                edge_idx, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_labels = np.where(new_labels == 2, 0, new_labels)\n",
    "        elif (\n",
    "            self.used_classes_dict[\"preictal\"] and self.used_classes_dict[\"interictal\"]\n",
    "        ):\n",
    "            label_to_remove = 1\n",
    "            new_labels = np.delete(\n",
    "                labels, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_features = np.delete(\n",
    "                features, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_idx = np.delete(\n",
    "                edge_idx, np.where(labels == label_to_remove)[0], axis=0\n",
    "            )\n",
    "            new_labels = np.where(new_labels == 2, 1, new_labels)\n",
    "\n",
    "        return new_features, new_labels, new_idx\n",
    "\n",
    "    def _calculate_timeseries_features(self, features: np.ndarray):\n",
    "        \"\"\"Converting features to EEG timeseries features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features to be converted.\n",
    "        Returns:\n",
    "            new_features: (np.ndarray) Array with Hjorth features.\n",
    "        \"\"\"\n",
    "        FREQ_BANDS = [0.5, 4, 8, 13, 29]  ## hardcoded for now\n",
    "        band_count = len(FREQ_BANDS) - 1\n",
    "\n",
    "        new_features = np.array(\n",
    "            [\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        np.expand_dims(compute_variance(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_mobility(feature), 1),\n",
    "                        np.expand_dims(compute_hjorth_complexity(feature), 1),\n",
    "                        np.expand_dims(compute_line_length(feature), 1),\n",
    "                        np.expand_dims(compute_katz_fd(feature), 1),\n",
    "                        np.expand_dims(compute_higuchi_fd(feature), 1),\n",
    "                        compute_energy_freq_bands(\n",
    "                            self.sampling_f, feature, FREQ_BANDS\n",
    "                        ).reshape(self.n_channels, band_count),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for feature in features\n",
    "            ]\n",
    "        )\n",
    "        return new_features\n",
    "\n",
    "    def _get_train_val_indices(self, n_samples: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Perfom train/validation split on provided indices with labels.\n",
    "        Args:\n",
    "            n_samples: (np.ndarray) Array with indices.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            train_indices: (np.ndarray) Array with indices for training.\n",
    "            val_indices: (np.ndarray) Array with indices for validation.\n",
    "        \"\"\"\n",
    "\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            n_samples,\n",
    "            test_size=self.train_val_split_ratio,\n",
    "            shuffle=True,\n",
    "            stratify=labels,\n",
    "            random_state=self.seed,\n",
    "        )\n",
    "        train_indices = np.sort(train_indices)\n",
    "        val_indices = np.sort(val_indices)\n",
    "        return train_indices, val_indices\n",
    "\n",
    "    def _transform_features(self, features: np.ndarray):\n",
    "        \"\"\"Performs normalization and  optionally transformation of features.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features to be transformed.\n",
    "        Returns:\n",
    "            processed_features: (np.ndarray) Array with transformed features.\n",
    "        \"\"\"\n",
    "        #processed_features = self._normalize_data(features)\n",
    "        processed_features = (features - self.data_mean) / self.data_std\n",
    "        if self.extract_features:\n",
    "            processed_features = self._calculate_timeseries_features(processed_features)\n",
    "        elif self.fft:\n",
    "            processed_features = np.fft.rfft(processed_features)\n",
    "        processed_features = torch.from_numpy(processed_features).float()\n",
    "        return processed_features\n",
    "\n",
    "    def _transform_edges(self, edge_idx : np.ndarray):\n",
    "        \"\"\"Converts the edges from numpy format to torch_geometric format.\n",
    "        Args:\n",
    "            edge_idx: (np.ndarray) Array with edges.\n",
    "        Returns:\n",
    "            edge_index: (torch.tensor) Tensor with edges.\n",
    "        \"\"\"\n",
    "        edge_index = nx.convert_matrix.from_numpy_array(edge_idx)\n",
    "        data_object = torch_geometric.utils.from_networkx(edge_index)\n",
    "        edge_index = data_object.edge_index\n",
    "        # edge_weight = data_object.weight\n",
    "        return edge_index\n",
    "\n",
    "    def _transform_labels(self, labels : np.ndarray):\n",
    "        \"\"\"Convert labels to torch tensor.\n",
    "        Args:\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            labels_transformed: (torch.tensor) Tensor with labels.\n",
    "        \"\"\"\n",
    "        labels_transformed = torch.from_numpy(labels).float()\n",
    "        return labels_transformed\n",
    "\n",
    "    def _features_to_data_list(\n",
    "        self, features: np.ndarray, edge_idx: np.ndarray, labels: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Converts features, edges and labels to list of torch_geometric.data.Data objects.\n",
    "        Before the conversion, features are normalized using mean and standard deviation of interictal samples\n",
    "        and transformed if specified in the config.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            edges: (np.ndarray) Array with edges.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        \"\"\"\n",
    "        processed_features = self._transform_features(features)\n",
    "\n",
    "        processed_labels = self._transform_labels(labels)\n",
    "        data_list = [\n",
    "            Data(\n",
    "                x=processed_features[i],\n",
    "                edge_index=self._transform_edges(edge_idx[i]),\n",
    "                y=processed_labels[i],\n",
    "            )\n",
    "            for i in range(len(processed_features))\n",
    "        ]\n",
    "        return data_list\n",
    "\n",
    "    def _get_single_patient_data_train_val(self, patient: str):\n",
    "        \"\"\"Get patient data for train and validation sets.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to get the data for.\n",
    "        Returns:\n",
    "            train_data_list: (list) List of torch_geometric.data.Data objects for training.\n",
    "            val_data_list: (list) List of torch_geometric.data.Data objects for validation.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf_data_path, \"r\") as hdf5_file:\n",
    "            n_samples = np.arange(hdf5_file[patient][\"features\"].shape[0])\n",
    "            labels = np.squeeze(hdf5_file[patient][\"labels\"])\n",
    "            train_indices, val_indices = self._get_train_val_indices(n_samples, labels)\n",
    "\n",
    "            features_train = hdf5_file[patient][\"features\"][train_indices]\n",
    "            labels_train = hdf5_file[patient][\"labels\"][train_indices]\n",
    "            edge_idx_train = hdf5_file[patient][\"edge_idx\"][train_indices]\n",
    "\n",
    "            features_val = hdf5_file[patient][\"features\"][val_indices]\n",
    "            labels_val = hdf5_file[patient][\"labels\"][val_indices]\n",
    "            edge_idx_val = hdf5_file[patient][\"edge_idx\"][val_indices]\n",
    "            if sum(self.used_classes_dict.values()) < 3:\n",
    "                features_train, labels_train, edge_idx_train = self.update_classes(\n",
    "                    features_train, labels_train, edge_idx_train\n",
    "                )\n",
    "                features_val, labels_val, edge_idx_val = self.update_classes(\n",
    "                    features_val, labels_val, edge_idx_val\n",
    "                )\n",
    "\n",
    "            data_list_train = self._features_to_data_list(\n",
    "                features_train, edge_idx_train, labels_train\n",
    "            )\n",
    "            data_list_val = self._features_to_data_list(\n",
    "                features_val, edge_idx_val, labels_val\n",
    "            )\n",
    "        self.logger.info(f\"Processed patient {patient} for train and validation sets.\")\n",
    "        return (data_list_train, data_list_val)\n",
    "\n",
    "    def _get_single_patient_data(self, patient: str):\n",
    "        \"\"\"Get patient data for training set.\n",
    "        Args:\n",
    "            patient: (str) Name of the patient to get the data for.\n",
    "        Returns:\n",
    "            train_data_list: (list) List of torch_geometric.data.Data objects for training.\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf_data_path, \"r\") as hdf5_file:\n",
    "            features = hdf5_file[patient][\"features\"][:]\n",
    "            labels = hdf5_file[patient][\"labels\"][:]\n",
    "            edge_idx = hdf5_file[patient][\"edge_idx\"][:]\n",
    "            if sum(self.used_classes_dict.values()) < 3:\n",
    "                features, labels, edge_idx = self.update_classes(\n",
    "                    features, labels, edge_idx\n",
    "                )\n",
    "            data_list = self._features_to_data_list(features, edge_idx, labels)\n",
    "        self.logger.info(f\"Processed patient {patient} set.\")\n",
    "        return data_list\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Get data for training, validation and leave-one-subject-out cross-validation.\n",
    "        Returns:\n",
    "            data_lists: (list) List of lists of torch_geometric.data.Data objects loaded.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        train_data_list = []\n",
    "        if self.train_val_split_ratio > 0:\n",
    "            val_data_list = []\n",
    "        num_processes = mp.cpu_count()\n",
    "        pool = mp.Pool(processes=num_processes)\n",
    "        try:\n",
    "            if self.train_val_split_ratio > 0:\n",
    "                result_list = pool.map(\n",
    "                    self._get_single_patient_data_train_val, self.patient_list\n",
    "                )\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                for train_data, val_data in result_list:\n",
    "                    train_data_list = train_data_list + train_data\n",
    "                    val_data_list = val_data_list + val_data\n",
    "            else:\n",
    "                result_list = pool.map(self._get_single_patient_data, self.patient_list)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                for train_data in result_list:\n",
    "                    train_data_list = train_data_list + train_data\n",
    "        except KeyboardInterrupt:\n",
    "            pool.terminate()\n",
    "            print(\"Keyboard interrupt detected, terminating worker pool.\")\n",
    "            raise KeyboardInterrupt\n",
    "        data_lists = (\n",
    "            [train_data_list]\n",
    "            if self.train_val_split_ratio == 0\n",
    "            else [train_data_list, val_data_list]\n",
    "        )\n",
    "        if self.loso_subject is not None:\n",
    "            loso_data_list = self._get_single_patient_data(self.loso_subject)\n",
    "            data_lists.append(loso_data_list)\n",
    "        self.logger.info(f\"Data loading took {time.time() - start} seconds.\")\n",
    "        return data_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger initialized. Logs saved to logs/hdf_dataloader_2023-07-11_10-26-22.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CLASS: preictal\n",
      "USING CLASS: ictal\n"
     ]
    }
   ],
   "source": [
    "#root = \"cache/2023-07-03_15-23-17\"\n",
    "dataset = HDFDatasetLoader(\n",
    "    ds_path,\n",
    "    extract_features=True,\n",
    "    train_val_split_ratio=0.1,\n",
    "    loso_subject=\"chb20\",\n",
    "    fft=False,\n",
    "    used_classes_dict={\"interictal\": False, \"preictal\": True, \"ictal\": True},\n",
    ")\n",
    "data_list_v_2 = dataset.get_data()\n",
    "features_shape = data_list_v_2[0][0].x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from models import GINLightning, GATv2Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_list_v_2[0], batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(data_list_v_2[1], batch_size=64, shuffle=False)\n",
    "loso_loader = DataLoader(data_list_v_2[2], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_device = torch.device(device)\n",
    "precision = \"bf16-mixed\" if device == \"cpu\" else \"16-mixed\"\n",
    "strategy = pl.strategies.SingleDeviceStrategy(device=torch_device)\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=6, verbose=False, mode=\"min\"\n",
    ")\n",
    "best_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    verbose=False,\n",
    ")\n",
    "epochs = 30\n",
    "callbacks = [early_stopping,best_checkpoint_callback]\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    precision=precision,\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    enable_progress_bar=True,\n",
    "    strategy=strategy,\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=1,\n",
    "    enable_model_summary=False,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "model = GATv2Lightning(features_shape,n_classes=2,lr=0.0001,weight_decay=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52baad7fde84baebb499b24cd8be50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcda43ea170a4d089e123a5500eb55e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3262eb771949d5b42928b797b09667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a068506c8ec04145815221192d80802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680b5c4693f417489428a2ae50262e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d88fdad5541d19823663316cbb505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0506e8caeab4ce8a94f955aac1478a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017201bb742c445c9c45d35e3de2cfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5f62173f034946ba79743aab14c525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa96b41cd084838bef23699d463f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2760035ba6fe42e78b47d0a6f529d545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eee849cf0c44f4ad57f2c204da0bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3846f43b0b422591e630090df2b968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c95b347b1b4a22a393b0bebc879b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc41eff29084996be225b6d2d0fe227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe833af288143bfb63b39b507efa0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b0781095444de8832ba3c7a1c85a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70a5e9e21a458db2e2c60e6f655a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af22314a9b3a4b3c9a5606e081445e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1325151ac10f49db9c669470a1244255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cdb3c34355453ebcbfe5c12545d94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170f0f696e06475689554c90ed9b8845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc01c466bc6405d92255aa5fa002bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f5a28d6fb34b2c8968346bb5bc5b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73754ed19f3540a9b6fed6b991122f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06c2326aed84f328f6c45f71ec18151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ec24ac25d34d9d832f317e5054c5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9689f81468d424daff4c5f1955c27eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f34e872d4fb4708811ebd7a5006a7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9587a229e834bbd8fcf16ca13c725ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Restoring states from the checkpoint path at /home/szymon/code/sano/sano_eeg/lightning_logs/version_41/checkpoints/epoch=21-step=3432.ckpt\n",
      "INFO: Loaded model weights from the checkpoint at /home/szymon/code/sano/sano_eeg/lightning_logs/version_41/checkpoints/epoch=21-step=3432.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9691495241ba45f387fa156bfb59a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_auroc         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9285825490951538     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1859501153230667     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_specificity      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_auroc        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9285825490951538    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1859501153230667    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_specificity     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.1859501153230667,\n",
       "  'test_recall': 0.0,\n",
       "  'test_specificity': 1.0,\n",
       "  'test_auroc': 0.9285825490951538}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model,loso_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
