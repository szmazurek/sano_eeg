{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from dataclasses import dataclass\n",
    "import utils\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch_geometric.utils import from_networkx\n",
    "from scipy.signal import resample\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HDFDataset_Writer:\n",
    "    npy_dataset_path: str\n",
    "    event_tables_path: str\n",
    "    cache_folder: str\n",
    "    seizure_lookback: int = 600\n",
    "    sample_timestep: int = 5\n",
    "    inter_overlap: int = 0\n",
    "    preictal_overlap: int = 0\n",
    "    ictal_overlap: int = 0\n",
    "    downsample: int = None\n",
    "    sampling_f: int = 256\n",
    "    self_loops: bool = False\n",
    "    balance: bool = False\n",
    "    smote: bool = False\n",
    "    buffer_time: int = 15\n",
    "    used_classes_dict: dict[str] = field(\n",
    "        default_factory=lambda: {\"interictal\": True, \"preictal\": True, \"ictal\": True}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def _get_event_tables(self, patient_name: str) -> tuple[dict, dict]:\n",
    "        \"\"\"Read events for given patient into start and stop times lists from .csv extracted files.\n",
    "        Args:\n",
    "            patient_name: (str) Name of the patient to get events for.\n",
    "        Returns:\n",
    "            start_events_dict: (dict) Dictionary with start events for given patient.\n",
    "            stop_events_dict: (dict) Dictionary with stop events for given patient.\n",
    "        \"\"\"\n",
    "\n",
    "        event_table_list = os.listdir(self.event_tables_path)\n",
    "        patient_event_tables = [\n",
    "            os.path.join(self.event_tables_path, ev_table)\n",
    "            for ev_table in event_table_list\n",
    "            if patient_name in ev_table\n",
    "        ]\n",
    "        patient_event_tables = sorted(patient_event_tables)\n",
    "        patient_start_table = patient_event_tables[\n",
    "            0\n",
    "        ]  ## done terribly, but it has to be so for win/linux compat\n",
    "        patient_stop_table = patient_event_tables[1]\n",
    "        start_events_dict = pd.read_csv(patient_start_table).to_dict(\"index\")\n",
    "        stop_events_dict = pd.read_csv(patient_stop_table).to_dict(\"index\")\n",
    "        return start_events_dict, stop_events_dict\n",
    "\n",
    "    def _get_recording_events(self, events_dict, recording) -> list[int]:\n",
    "        \"\"\"Read seizure times into list from event_dict.\n",
    "        Args:\n",
    "            events_dict: (dict) Dictionary with events for given patient.\n",
    "            recording: (str) Name of the recording to get events for.\n",
    "        Returns:\n",
    "            recording_events: (list) List of seizure event start and stop time for given recording.\n",
    "        \"\"\"\n",
    "        recording_list = list(events_dict[recording + \".edf\"].values())\n",
    "        recording_events = [int(x) for x in recording_list if not np.isnan(x)]\n",
    "        return recording_events\n",
    "    \n",
    "    def _create_edge_idx_and_attributes(self,connectivity_matrix : np.ndarray , threshold : int =0.0) -> tuple[np.ndarray,np.ndarray]:\n",
    "        \"\"\"Create adjacency matrix from connectivity matrix. Edges are created for values above threshold.\n",
    "        If the edge is created, it has an attribute \"weight\" with the value of the connectivity measure associated.\n",
    "        Args:\n",
    "            connectivity_matrix: (np.ndarray) Array with connectivity values.\n",
    "            threshold: (float) Threshold for creating edges. (default: 0.0)\n",
    "        Returns:\n",
    "            edge_index: (np.ndarray) Array with edge indices.\n",
    "            edge_weights: (np.ndarray) Array with edge weights.\n",
    "        \"\"\"\n",
    "        result_graph = nx.graph.Graph()\n",
    "        n_nodes = connectivity_matrix.shape[0]\n",
    "        result_graph.add_nodes_from(range(n_nodes))\n",
    "        edge_tuples = [(i,j) for i in range(n_nodes) for j in range(n_nodes) if connectivity_matrix[i,j]>threshold]\n",
    "        result_graph.add_edges_from(edge_tuples)\n",
    "        edge_index = nx.convert_matrix.to_numpy_array(result_graph)\n",
    "        # connection_indices = np.where(edge_index==1)\n",
    "        # edge_weights = connectivity_matrix[connection_indices] ## ??\n",
    "    \n",
    "        return edge_index\n",
    "    \n",
    "    def _features_to_data_list(self, features, edges, labels, edge_weights=None):\n",
    "        \"\"\"Converts features, edges and labels to list of torch_geometric.data.Data objects.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            edges: (np.ndarray) Array with edges.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            data_list: (list) List of torch_geometric.data.Data objects.\n",
    "        \"\"\"\n",
    "        data_list = [\n",
    "            Data(\n",
    "                x=features[i],\n",
    "                edge_index=edges[i],\n",
    "                edge_attr=edge_weights[i],\n",
    "                y=labels[i],\n",
    "                # time=time_label[i],\n",
    "            )\n",
    "            for i in range(len(features))\n",
    "        ]\n",
    "        return data_list\n",
    "    def _apply_smote(self, features, labels):\n",
    "        \"\"\"Performs SMOTE oversampling on the dataset. Implemented for preictal vs ictal scenarion only.\n",
    "        Args:\n",
    "            features: (np.ndarray) Array with features.\n",
    "            labels: (np.ndarray) Array with labels.\n",
    "        Returns:\n",
    "            x_train_smote: (np.ndarray) Array with SMOTE oversampled features.\n",
    "            y_train_smote: (np.ndarray) Array with SMOTE oversampled labels.\n",
    "        \"\"\"\n",
    "        dim_1, dim_2, dim_3 = features.shape\n",
    "\n",
    "        new_dim = dim_1 * dim_2\n",
    "        new_x_train = features.reshape(new_dim, dim_3)\n",
    "        new_y_train = []\n",
    "        for i in range(len(labels)):\n",
    "            new_y_train.extend([labels[i]] * dim_2)\n",
    "\n",
    "        new_y_train = np.array(new_y_train)\n",
    "\n",
    "        # transform the dataset\n",
    "        oversample = SMOTE(random_state=42)\n",
    "        x_train, y_train = oversample.fit_resample(new_x_train, new_y_train)\n",
    "        x_train_smote = x_train.reshape(int(x_train.shape[0] / dim_2), dim_2, dim_3)\n",
    "        y_train_smote = []\n",
    "        for i in range(int(x_train.shape[0] / dim_2)):\n",
    "            # print(i)\n",
    "            value_list = list(y_train.reshape(int(x_train.shape[0] / dim_2), dim_2)[i])\n",
    "            # print(list(set(value_list)))\n",
    "            y_train_smote.extend(list(set(value_list)))\n",
    "            ## Check: if there is any different value in a list\n",
    "            if len(set(value_list)) != 1:\n",
    "                print(\n",
    "                    \"\\n\\n********* STOP: THERE IS SOMETHING WRONG IN TRAIN ******\\n\\n\"\n",
    "                )\n",
    "        y_train_smote = np.array(y_train_smote)\n",
    "        # print(np.unique(y_train_smote,return_counts=True))\n",
    "        return x_train_smote, y_train_smote\n",
    "    \n",
    "    def _get_labels_features_edge_weights_seizure(self, patient):\n",
    "        \"\"\"Method to extract features, labels and edge weights for seizure and interictal samples.\"\"\"\n",
    "\n",
    "        event_tables = self._get_event_tables(\n",
    "            patient\n",
    "        )  # extract start and stop of seizure for patient\n",
    "        patient_path = os.path.join(self.npy_dataset_path, patient)\n",
    "        recording_list = [\n",
    "            recording\n",
    "            for recording in os.listdir(patient_path)\n",
    "            if \"seizures\" in recording\n",
    "        ]\n",
    "        self.hdf5_file.create_group(patient)\n",
    "        for n, record in enumerate(recording_list):  # iterate over recordings for a patient\n",
    "            recording_path = os.path.join(patient_path, record)\n",
    "            record = record.replace(\n",
    "                \"seizures_\", \"\"\n",
    "            )  ## some magic to get it properly working with event tables\n",
    "            record_id = record.split(\".npy\")[0]  #  get record id\n",
    "            start_event_tables = self._get_recording_events(\n",
    "                event_tables[0], record_id\n",
    "            )  # get start events\n",
    "            stop_event_tables = self._get_recording_events(\n",
    "                event_tables[1], record_id\n",
    "            )  # get stop events\n",
    "            data_array = np.load(recording_path)  # load the recording\n",
    "\n",
    "            # plv_edge_weights = np.expand_dims(\n",
    "            #     self._get_edge_weights_recording(\n",
    "            #         np.load(os.path.join(self.plv_values_path, patient, record))\n",
    "            #     ),\n",
    "            #     axis=0,\n",
    "            # )\n",
    "\n",
    "            features, labels, time_labels = utils.extract_training_data_and_labels(\n",
    "                data_array,\n",
    "                start_event_tables,\n",
    "                stop_event_tables,\n",
    "                fs=self.sampling_f,\n",
    "                seizure_lookback=self.seizure_lookback,\n",
    "                sample_timestep=self.sample_timestep,\n",
    "                preictal_overlap=self.preictal_overlap,\n",
    "                ictal_overlap=self.ictal_overlap,\n",
    "                buffer_time=self.buffer_time,\n",
    "            )\n",
    "\n",
    "            if features is None:\n",
    "                print(\n",
    "                    f\"Skipping the recording {record} patients {patient} cuz features are none\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            features = features.squeeze(2)\n",
    "            conn_matrix_list = [utils.compute_spect_corr_matrix(feature,256) for feature in features]\n",
    "            edge_idx = np.stack([self._create_edge_idx_and_attributes(conn_matrix, threshold=np.mean(conn_matrix)) for conn_matrix in conn_matrix_list])\n",
    "            # edge_idx, edge_weights = zip(*edges_and_weights)\n",
    "            edge_idx = np.stack(edge_idx)\n",
    "           # edge_weights = np.stack(edge_weights)\n",
    "            if self.downsample:\n",
    "                new_sample_count = int(self.downsample * self.sample_timestep)\n",
    "                features = resample(features, new_sample_count, axis=2)\n",
    "            if self.smote:\n",
    "                features, labels = self._apply_smote(features, labels)\n",
    "            labels = labels.reshape((labels.shape[0], 1)).astype(np.float32)\n",
    "            \n",
    "            features_patient = features if n == 0 else np.concatenate([features_patient, features])\n",
    "            labels_patient = labels if n == 0 else np.concatenate([labels_patient, labels])\n",
    "            edge_idx_patient = edge_idx if n == 0 else np.concatenate([edge_idx_patient, edge_idx])\n",
    "       #     edge_weights_patient = edge_weights if n == 0 else np.concatenate([edge_weights_patient, edge_weights])\n",
    "   \n",
    "        self.hdf5_file[patient].create_dataset(\"features\", data=features_patient)\n",
    "        self.hdf5_file[patient].create_dataset(\"labels\", data=labels_patient)\n",
    "        self.hdf5_file[patient].create_dataset(\"edge_idx\", data=edge_idx_patient)\n",
    "            \n",
    "    def get_dataset(self):\n",
    "        config_name = f\"lookback_{self.seizure_lookback}_timestep_{self.sample_timestep}_overlap_{self.inter_overlap}_{self.preictal_overlap}_{self.ictal_overlap}_downsample_{self.downsample}_smote_{self.smote}_self_loops_{self.self_loops}_balance_{self.balance}\"\n",
    "        dataset_folder = os.path.join(self.cache_folder, config_name)\n",
    "        dataset_path = os.path.join(dataset_folder, \"dataset.hdf5\")\n",
    "        if os.path.exists(dataset_path):\n",
    "            print(f\"Folder {config_name} already exists in folder {dataset_folder}. Dataset not created.\")\n",
    "            return None\n",
    "        os.makedirs(dataset_folder,exist_ok=True)\n",
    "        self.hdf5_file = h5py.File(dataset_path, \"w\")\n",
    "        patient_list = os.listdir(self.npy_dataset_path)\n",
    "     \n",
    "        try:\n",
    "            if self.smote:\n",
    "                for patient in patient_list:\n",
    "                    self._get_labels_features_edge_weights_seizure(patient)\n",
    "            else:\n",
    "                Parallel(n_jobs=6, require=\"shardedmem\")(\n",
    "                    delayed(self._get_labels_features_edge_weights_seizure)(patient)\n",
    "                    for patient in patient_list\n",
    "                )\n",
    "                \n",
    "            self.hdf5_file.close()\n",
    "            print(f\"Dataset created in folder {dataset_folder}.\")\n",
    "        except:\n",
    "            self.hdf5_file.close()\n",
    "            os.remove(dataset_path)\n",
    "            raise Exception(\"Dataset creation failed. Dataset deleted.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "hdf_writer = HDFDataset_Writer(\n",
    "    npy_dataset_path=\"data/npy_data_full\",\n",
    "    event_tables_path=\"data/event_tables\",\n",
    "    cache_folder=\"cache1\",\n",
    "    seizure_lookback=600,\n",
    "    sample_timestep=9,\n",
    "    downsample=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the recording chb24_07.npy patients chb24 cuz features are none\n",
      "Skipping the recording chb16_16.npy patients chb16 cuz features are none\n",
      "Dataset created in folder cache1/lookback_600_timestep_9_overlap_0_0_0_downsample_60_smote_False_self_loops_False_balance_False.\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(42)\n",
    "hdf_writer.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both files are same\n",
      "Hash: ea97fb188f2d96fc40a116e195cbd1147fe559e6a494f34af4ad20cc700df583\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "def hashfile(file):\n",
    "  \n",
    "    # A arbitrary (but fixed) buffer\n",
    "    # size (change accordingly)\n",
    "    # 65536 = 65536 bytes = 64 kilobytes\n",
    "    BUF_SIZE = 65536\n",
    "  \n",
    "    # Initializing the sha256() method\n",
    "    sha256 = hashlib.sha256()\n",
    "  \n",
    "    # Opening the file provided as\n",
    "    # the first commandline argument\n",
    "    with open(file, 'rb') as f:\n",
    "         \n",
    "        while True:\n",
    "             \n",
    "            # reading data = BUF_SIZE from\n",
    "            # the file and saving it in a\n",
    "            # variable\n",
    "            data = f.read(BUF_SIZE)\n",
    "  \n",
    "            # True if eof = 1\n",
    "            if not data:\n",
    "                break\n",
    "      \n",
    "            # Passing that data to that sh256 hash\n",
    "            # function (updating the function with\n",
    "            # that data)\n",
    "            sha256.update(data)\n",
    "  \n",
    "      \n",
    "    # sha256.hexdigest() hashes all the input\n",
    "    # data passed to the sha256() via sha256.update()\n",
    "    # Acts as a finalize method, after which\n",
    "    # all the input data gets hashed hexdigest()\n",
    "    # hashes the data, and returns the output\n",
    "    # in hexadecimal format\n",
    "    return sha256.hexdigest()\n",
    "f1_hash = hashfile(\"cache/lookback_600_timestep_9_overlap_0_0_0_downsample_60_smote_False_self_loops_False_balance_False/dataset.hdf5\")\n",
    "f2_hash = hashfile(\"cache1/lookback_600_timestep_9_overlap_0_0_0_downsample_60_smote_False_self_loops_False_balance_False/dataset.hdf5\")\n",
    "  \n",
    "# Doing primitive string comparison to\n",
    "# check whether the two hashes match or not\n",
    "if f1_hash == f2_hash:\n",
    "    print(\"Both files are same\")\n",
    "    print(f\"Hash: {f1_hash}\")\n",
    " \n",
    "else:\n",
    "    print(\"Files are different!\")\n",
    "    print(f\"Hash of File 1: {f1_hash}\")\n",
    "    print(f\"Hash of File 2: {f2_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_hdf = h5py.File(\"cache1/lookback_600_timestep_9_overlap_0_0_0_downsample_60_smote_False_self_loops_False_balance_False/dataset.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float32), array([143,  19]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(created_hdf[\"chb02\"][\"labels\"],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_graph = nx.Graph()\n",
    "example_graph.add_nodes_from(range(10))\n",
    "example_graph.add_edges_from([(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9)])\n",
    "nx.draw(example_graph,with_labels=True)\n",
    "numpy_adj = nx.convert_matrix.to_numpy_array(example_graph)\n",
    "torch_geometric.utils.dense_to_sparse(torch.from_numpy(numpy_adj))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hdf = h5py.File(\"dummy.hdf5\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hdf.create_group(\"patient_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.random.rand(10, 10, 10)\n",
    "labels = np.random.randint(0, 2, (10, 1))\n",
    "edge_idx = np.random.randint(0, 10, (10, 2))\n",
    "edge_weights = np.random.rand(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hdf['patient_1'].create_dataset(\"features\", data=features, maxshape=(None, 10, 10))\n",
    "dummy_hdf['patient_1'].create_dataset(\"labels\", data=labels, maxshape=(None, 1))\n",
    "dummy_hdf['patient_1'].create_dataset(\"edge_idx\", data=edge_idx, maxshape=(None, 2))\n",
    "dummy_hdf['patient_1'].create_dataset(\"edge_weights\", data=edge_weights, maxshape=(None, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = Data(x=torch.tensor(features[0]), edge_index=torch.tensor(edge_idx[0]).T, edge_attr=torch.tensor(edge_weights[0]), y=torch.tensor(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to dataset\n",
    "# dummy_hdf['patient_1']['features'].resize((dummy_hdf['patient_1']['features'].shape[0] + features.shape[0]), axis=0)z\n",
    "# dummy_hdf['patient_1']['features'][-features.shape[0]:] = features\n",
    "dummy_hdf['patient_1']['labels'].resize((dummy_hdf['patient_1']['labels'].shape[0] + labels.shape[0]), axis=0)\n",
    "dummy_hdf['patient_1']['labels'][-labels.shape[0]:] = labels\n",
    "dummy_hdf['patient_1']['edge_idx'].resize((dummy_hdf['patient_1']['edge_idx'].shape[0] + edge_idx.shape[0]), axis=0)\n",
    "dummy_hdf['patient_1']['edge_idx'][-edge_idx.shape[0]:] = edge_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_hdf = h5py.File(\"dummy.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = dummy_hdf[\"patient_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(grp['labels'][:],np.concatenate([labels,labels],axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
